{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (0.9.26)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (0.6.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (2023.12.2)\n",
      "Requirement already satisfied: httpx in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (0.26.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (1.5.8)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (1.26.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (1.6.1)\n",
      "Requirement already satisfied: pandas in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (0.5.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from llama-index) (0.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index) (1.16.0)\n",
      "Requirement already satisfied: click in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.1.0->llama-index) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.1.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.1.0->llama-index) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.1.0->llama-index) (1.3.0)\n",
      "Requirement already satisfied: certifi in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx->llama-index) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx->llama-index) (1.0.2)\n",
      "Requirement already satisfied: idna in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx->llama-index) (3.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests>=2.31.0->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests>=2.31.0->llama-index) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from dataclasses-json->llama-index) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pandas->llama-index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pandas->llama-index) (2023.4)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index) (2.14.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpllmws23/.pyenv/versions/3.11.7/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! return_full_text is not default parameter.\n",
      "                return_full_text was transferred to model_kwargs.\n",
      "                Please confirm that return_full_text is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/tpllmws23/llms/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.42 MiB\n",
      "llm_load_tensors: VRAM used           = 4095.05 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 291.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 288.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 4383.06 MiB (model: 4095.05 MiB, context: 288.00 MiB)\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/tpllmws23/llms/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.42 MiB\n",
      "llm_load_tensors: VRAM used           = 4095.05 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 291.19 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 288.00 MiB\n",
      "llama_new_context_with_model: total VRAM used: 4383.06 MiB (model: 4095.05 MiB, context: 288.00 MiB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, LLMPredictor, PromptHelper, ServiceContext, StorageContext, load_index_from_storage\n",
    "\n",
    "from langchain.llms.llamacpp import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import chromadb\n",
    "from llama_index import SimpleDirectoryReader, StorageContext, VectorStoreIndex, ServiceContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "\n",
    "database_path = \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/Chatbot-Jan/databases/sentence-transformers/all-mpnet-base-v2/context_1024_chunk_128_overlap_0.db\"\n",
    "database_collection = \"Pruefungsamt\"\n",
    "# model_path = \"/home/tpllmws23/llms/llama-2-13b-chat.Q4_K_M.gguf\"\n",
    "model_path = \"/home/tpllmws23/llms/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "\n",
    "n_ctx = 4096\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(model_path=model_path,\n",
    "        n_gpu_layers=-1,\n",
    "        n_batch=512,\n",
    "        n_ctx=n_ctx,\n",
    "        f16_kv=True,\n",
    "        verbose=False,\n",
    "        temperature=0.0,\n",
    "        top_p=1,\n",
    "        callback_manager=callback_manager,\n",
    "        return_full_text=False\n",
    ")\n",
    "\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "embedding = LlamaCppEmbeddings(model_path=model_path,\n",
    "        n_gpu_layers=-1,\n",
    "        n_batch=512,\n",
    "        n_ctx=n_ctx,\n",
    "        f16_kv=True,\n",
    "        verbose=False\n",
    "        )\n",
    "\n",
    "embed_model = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "\n",
    "# check if storage already exists\n",
    "if not os.path.exists(database_path):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"./main_data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    # store it for later\n",
    "    index.storage_context.persist()\n",
    "else:\n",
    "    # load the existing index\n",
    "    db2 = chromadb.PersistentClient(path=database_path)\n",
    "    chroma_collection = db2.get_or_create_collection(\"Pruefungsamt\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import BaseRetriever as LIBaseRetriever\n",
    "from langchain_core.retrievers import BaseRetriever as LCBaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from langchain_community.retrievers.llama_index import LlamaIndexRetriever\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.globals import set_debug\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "\n",
    "class CustomRetriever(LCBaseRetriever):\n",
    "    li_retriever: LIBaseRetriever | None = None\n",
    "\n",
    "    def set_retriever(self, index: LIBaseRetriever):\n",
    "        self.li_retriever = index\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        if self.li_retriever is None:\n",
    "            raise Exception(\"No retriever is set\")\n",
    "        \n",
    "        docs : List[Document] = []\n",
    "        \n",
    "        llama_index_nodes = self.li_retriever.retrieve(query)\n",
    "        for node in llama_index_nodes:\n",
    "            docs.append(Document(page_content = node.get_text(), metadata = node.metadata))\n",
    "\n",
    "        return docs\n",
    "\n",
    "custom_retriever = CustomRetriever()\n",
    "custom_retriever.set_retriever(index.as_retriever())\n",
    "\n",
    "template = \"\"\"Answer the following question based only on the provided context. Also return the source in APA style:\n",
    "{context}\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "chain = (\n",
    "    {\"context\": custom_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3)  Lehrveranstaltungen sind aus dem Wahlpflichtangeb ot aller drei Vertiefungsrichtungen und dem \\nveröffentlichten Wahlpflichtkatalog für den Studiengan g MSI und anderer Masterprogramme der Hochschule \\nKonstanz zu wählen.', metadata={'page_label': '4', 'file_name': 'SPO_MSI_SPONr5_Senat_10122019.pdf', 'file_path': '/home/tpllmws23/llms/main_data/SPO_MSI_SPONr5_Senat_10122019.pdf', 'file_type': 'application/pdf', 'file_size': 170767, 'creation_date': '2023-11-20', 'last_modified_date': '2023-11-20', 'last_accessed_date': '2024-03-09'}),\n",
       " Document(page_content='3)  Lehrveranstaltungen sind aus dem Wahlpflichtangeb ot aller drei Vertiefungsrichtungen und dem \\nveröffentlichten Wahlpflichtkatalog für den Studiengan g MSI und anderer Masterprogramme der Hochschule \\nKonstanz zu wählen.', metadata={'page_label': '2', 'file_name': 'SPO_MSI_SPONr5_Senat_10122019.pdf', 'file_path': '/home/tpllmws23/llms/main_data/SPO_MSI_SPONr5_Senat_10122019.pdf', 'file_type': 'application/pdf', 'file_size': 170767, 'creation_date': '2023-11-20', 'last_modified_date': '2023-11-20', 'last_accessed_date': '2024-03-09'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_retriever.get_relevant_documents(\"What are the requirements for studying MSI in masters degree at the HTWG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the requirements for studying MSI in masters degree at the HTWG?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,input>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the requirements for studying MSI in masters degree at the HTWG?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,input> > 3:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the requirements for studying MSI in masters degree at the HTWG?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,input> > 4:chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the requirements for studying MSI in masters degree at the HTWG?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,input> > 4:chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What are the requirements for studying MSI in masters degree at the HTWG?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,input> > 3:chain:RunnableSequence > 5:chain:format_docs] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,input> > 3:chain:RunnableSequence > 5:chain:format_docs] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Hochschule Konstanz 7Aufbau des Studiums MSI\\n▪Insgesamt 90 ECTS in 3 Semestern, davon 30 ECTS für die Masterarbeit\\n▪3 Studienrichtungen\\n•Autonome Systeme (MSI -AS) \\n•IT-Management (MSI -ITM) \\n•Software -Engineering (MSI -SE) \\n▪Studienrichtung muss bereits bei der Bewerbung angegeben werden.\\n•Die Vergabe der Plätze erfolgt nach Studienrichtung.Informatik (M. Sc.)\\n\\nHochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg -konstanz.de\\nHerzlich willkommen bei der \\nInformationsveranstaltung \\nMaster Informatik (MSI) und \\nBusiness Information Technology (BIT)\\nan der HTWG Konstanz\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,input> > 3:chain:RunnableSequence] [23ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Hochschule Konstanz 7Aufbau des Studiums MSI\\n▪Insgesamt 90 ECTS in 3 Semestern, davon 30 ECTS für die Masterarbeit\\n▪3 Studienrichtungen\\n•Autonome Systeme (MSI -AS) \\n•IT-Management (MSI -ITM) \\n•Software -Engineering (MSI -SE) \\n▪Studienrichtung muss bereits bei der Bewerbung angegeben werden.\\n•Die Vergabe der Plätze erfolgt nach Studienrichtung.Informatik (M. Sc.)\\n\\nHochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg -konstanz.de\\nHerzlich willkommen bei der \\nInformationsveranstaltung \\nMaster Informatik (MSI) und \\nBusiness Information Technology (BIT)\\nan der HTWG Konstanz\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,input>] [26ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"Hochschule Konstanz 7Aufbau des Studiums MSI\\n▪Insgesamt 90 ECTS in 3 Semestern, davon 30 ECTS für die Masterarbeit\\n▪3 Studienrichtungen\\n•Autonome Systeme (MSI -AS) \\n•IT-Management (MSI -ITM) \\n•Software -Engineering (MSI -SE) \\n▪Studienrichtung muss bereits bei der Bewerbung angegeben werden.\\n•Die Vergabe der Plätze erfolgt nach Studienrichtung.Informatik (M. Sc.)\\n\\nHochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg -konstanz.de\\nHerzlich willkommen bei der \\nInformationsveranstaltung \\nMaster Informatik (MSI) und \\nBusiness Information Technology (BIT)\\nan der HTWG Konstanz\",\n",
      "  \"input\": \"What are the requirements for studying MSI in masters degree at the HTWG?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"Hochschule Konstanz 7Aufbau des Studiums MSI\\n▪Insgesamt 90 ECTS in 3 Semestern, davon 30 ECTS für die Masterarbeit\\n▪3 Studienrichtungen\\n•Autonome Systeme (MSI -AS) \\n•IT-Management (MSI -ITM) \\n•Software -Engineering (MSI -SE) \\n▪Studienrichtung muss bereits bei der Bewerbung angegeben werden.\\n•Die Vergabe der Plätze erfolgt nach Studienrichtung.Informatik (M. Sc.)\\n\\nHochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg -konstanz.de\\nHerzlich willkommen bei der \\nInformationsveranstaltung \\nMaster Informatik (MSI) und \\nBusiness Information Technology (BIT)\\nan der HTWG Konstanz\",\n",
      "  \"input\": \"What are the requirements for studying MSI in masters degree at the HTWG?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Answer the following question based only on the provided context. Also return the source in APA style:\\nHochschule Konstanz 7Aufbau des Studiums MSI\\n▪Insgesamt 90 ECTS in 3 Semestern, davon 30 ECTS für die Masterarbeit\\n▪3 Studienrichtungen\\n•Autonome Systeme (MSI -AS) \\n•IT-Management (MSI -ITM) \\n•Software -Engineering (MSI -SE) \\n▪Studienrichtung muss bereits bei der Bewerbung angegeben werden.\\n•Die Vergabe der Plätze erfolgt nach Studienrichtung.Informatik (M. Sc.)\\n\\nHochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg -konstanz.de\\nHerzlich willkommen bei der \\nInformationsveranstaltung \\nMaster Informatik (MSI) und \\nBusiness Information Technology (BIT)\\nan der HTWG Konstanz\\n\\nQuestion: What are the requirements for studying MSI in masters degree at the HTWG?\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following question based only on the provided context. Also return the source in APA style:\\nHochschule Konstanz 7Aufbau des Studiums MSI\\n▪Insgesamt 90 ECTS in 3 Semestern, davon 30 ECTS für die Masterarbeit\\n▪3 Studienrichtungen\\n•Autonome Systeme (MSI -AS) \\n•IT-Management (MSI -ITM) \\n•Software -Engineering (MSI -SE) \\n▪Studienrichtung muss bereits bei der Bewerbung angegeben werden.\\n•Die Vergabe der Plätze erfolgt nach Studienrichtung.Informatik (M. Sc.)\\n\\nHochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg -konstanz.de\\nHerzlich willkommen bei der \\nInformationsveranstaltung \\nMaster Informatik (MSI) und \\nBusiness Information Technology (BIT)\\nan der HTWG Konstanz\\n\\nQuestion: What are the requirements for studying MSI in masters degree at the HTWG?\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: To study MSI (Master Informatik) in a master's degree at the HTWG, you must have already chosen your preferred field of study during the application process. The allocation of places is based on the chosen field of study. For more information, please visit the official website of the HTWG Konstanz.\n",
      "\n",
      "Source: Hochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg-konstanz.de\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:llm:LlamaCpp] [5.39s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\n\\nAnswer: To study MSI (Master Informatik) in a master's degree at the HTWG, you must have already chosen your preferred field of study during the application process. The allocation of places is based on the chosen field of study. For more information, please visit the official website of the HTWG Konstanz.\\n\\nSource: Hochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg-konstanz.de\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\\n\\nAnswer: To study MSI (Master Informatik) in a master's degree at the HTWG, you must have already chosen your preferred field of study during the application process. The allocation of places is based on the chosen field of study. For more information, please visit the official website of the HTWG Konstanz.\\n\\nSource: Hochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg-konstanz.de\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\\n\\nAnswer: To study MSI (Master Informatik) in a master's degree at the HTWG, you must have already chosen your preferred field of study during the application process. The allocation of places is based on the chosen field of study. For more information, please visit the official website of the HTWG Konstanz.\\n\\nSource: Hochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg-konstanz.de\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [5.42s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\\n\\nAnswer: To study MSI (Master Informatik) in a master's degree at the HTWG, you must have already chosen your preferred field of study during the application process. The allocation of places is based on the chosen field of study. For more information, please visit the official website of the HTWG Konstanz.\\n\\nSource: Hochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg-konstanz.de\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAnswer: To study MSI (Master Informatik) in a master's degree at the HTWG, you must have already chosen your preferred field of study during the application process. The allocation of places is based on the chosen field of study. For more information, please visit the official website of the HTWG Konstanz.\\n\\nSource: Hochschule Konstanz | Brauneggerstr . 55 | 78462 Konstanz | www.htwg-konstanz.de\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chain.invoke(\"What are the requirements for studying MSI in masters degree at the HTWG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
