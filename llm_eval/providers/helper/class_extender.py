from langchain.llms.llamacpp import LlamaCpp
from typing import Dict, Type
from inspect import isclass

class ClassExtender:
    '''Class to temporarily extend a class with methods or overwrite its methods

    
    Example Usage:

        >>> class C:
                pass

        >>> def get_class_name(self):
            ... return self.__class__.__name__

        >>> with ClassExtender(C, get_class_name):
            ...    c = C()
            ...    print(c.get_class_name()) # prints 'C'


    '''
    def __init__(self, obj, method):
        method_name = method.__name__
        setattr(obj, method_name, method)
        self.obj = obj
        self.method_name = method_name

    def __enter__(self):
        return self.obj

    def __exit__(self, type, value, traceback):
        # removes extension method after context exit (after with block)
        delattr(self.obj, self.method_name)

class LlamaCppOutputExtender(ClassExtender):
    '''Convenience Class for ClassExtender to extend output of an LLM provided by Langchain

    Langchain only outputs the generated answer (a string) when invoking a chain (e.g. prompting the LLM),
    this usually being result.generation[0]['text'].
    Sometimes it is of importance to retrieve the metadata associated with a generation e.g. logprobs or token usage
    This class can be used to override the invoke (by default, or any other) function of Langchain's LlamaCpp wrapper to directly access
    all of the llm output.
    It will temporarily (this being inside the with block) overwrite (or extend the class with) a method.

    Example Usage (default overwrite of invoke):

        >>> from langchain.llms.llamacpp import LlamaCpp

        >>> llm = LlamaCpp(...) # instantiate LlamaCpp with some args, in most cases its already available somehow, otherwise instantiate it inside with block

        >>> with ClassExtender(LlamaCpp):
            ...    print(llm.invoke("Wie gehts?")) # prints full LLM output as json object

    Example Usage (custom method):

        >>> from langchain.llms.llamacpp import LlamaCpp

        >>> llm = LlamaCpp(...) # instantiate LlamaCpp with some args

        >>> def get_class_name(self):
            ... return self.__class__.__name__

        >>> with ClassExtender(LLamaCpp, get_class_name):
            ...    print(llm.get_class_name()) # prints 'LlamaCpp'

    '''
    def __init__(self, obj: Type[LlamaCpp], method = None):
        if obj is not LlamaCpp:
            raise ValueError(
                "Class to extend is expected to be of type LlamaCpp (from langchain.llms.llamacpp import LlamaCpp), received"
                f" Class or object of type {str(obj) if isclass(obj) else type(obj)}."
            )
        print('HI')
        if method is None:
            # use custom invoke method by default
            method = invoke
        super().__init__(obj, method)

def invoke(self, prompt: str):
    return self.client(prompt=prompt, **self._get_parameters())




def main():
    import torch
    from langchain import PromptTemplate
    from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline 
    import os 
    from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline

    os.environ["HF_TOKEN"]='your_huggingface_API_key'

    # /home/tpllmws23/Chatbot-LLama-Pruefungsamt/Chatbot-Benni/finetune/convert/model/raftv2

    BASE_MODEL_ID =  "/home/tpllmws23/llms/raftv2" # "mistralai/Mistral-7B-v0.3"

    #"../../../llms/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    # Initialize tokenizer
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True)
    #tokenizer.pad_token = tokenizer.eos_token
    #tokenizer.pad_token = tokenizer.unk_token

    # Initialize language model
    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, torch_dtype=torch.bfloat16,
    trust_remote_code=True, device_map="auto",
    quantization_config=bnb_config)

    generation_config = GenerationConfig.from_pretrained(BASE_MODEL_ID)
    generation_config.max_new_tokens = 1024 # maximum number of new tokens that can be generated by the model
    generation_config.temperature = 0.001 # randomness of the generated tex
    generation_config.top_p = 0 # diversity of the generated text
    generation_config.do_sample = True 
    generation_config.repetition_penalty = 1.2

    #generation_config.use_cache=True,
    #generation_config.num_return_sequences=1,
    
    generation_config.output_logits=True,
    generation_config.output_scores=True,
    generation_config.output_hidden_states=True,
    generation_config.return_dict_in_generate=True,
    print(generation_config)

    '''
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        return_full_text=True,
        generation_config=generation_config,
    )
    '''
    

    from transformers import TextClassificationPipeline
    from transformers import Pipeline
    '''
    class TextClassificationFullOutputsPipeline(TextClassificationPipeline):
        def postprocess(self, model_outputs):
            return model_outputs
    '''

    
    
    def postprocess(self, model_outputs, **postprocess_parameters: Dict):
        clean_up_tokenization_spaces=True
        input_ids = model_outputs["input_ids"]
        prompt_text = model_outputs["prompt_text"]
        generated_sequence = model_outputs["generated_sequence"][0]
        records = []
        for sequence in generated_sequence:
            # Decode text
            text = self.tokenizer.decode(
                sequence,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            )

            # Remove PADDING prompt of the sequence if XLNet or Transfo-XL model is used
            if input_ids is None:
                prompt_length = 0
            else:
                prompt_length = len(
                    self.tokenizer.decode(
                        input_ids[0],
                        skip_special_tokens=True,
                        clean_up_tokenization_spaces=clean_up_tokenization_spaces,
                    )
                )

            all_text = text[prompt_length:]
            if isinstance(prompt_text, str):
                all_text = prompt_text + all_text
                

            record = {"generated_text": all_text, "model_outputs": model_outputs["output"], "input_ids": input_ids}
        records.append(record)
        return records
    
    pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            return_full_text=True,
            generation_config=generation_config,
        )
    
    with ClassExtender(type(pipe), postprocess), ClassExtender(TextClassificationPipeline, postprocess):

        

        #pipe.postprocess = postprocess_new


        llm = HuggingFacePipeline(pipeline=pipe)

        def create_eval_prompt(query: str, context: str):
            system_prompt = "You are a smart helpful assistant for the HTWG Konstanz. Answer the following question based only on the provided context. It is mandatory to answer in GERMAN:\n\n"
            return f"[INST]{system_prompt}Context: {context}\n\nQuestion: {query}[/INST]"
        
        data = [{"page_content": "\n\u00a7 4 Sprachkenntnisse\n(1) 1Neben den allgemeinen Zugangsvoraussetzungen (\u00a7 59 LHG) sind f\u00fcr die in \u00a7 1 Abs. 1\nS. 1 genannten Studieng\u00e4nge deutsche Sprach kenntnisse nachzuweisen. 2Diese k\u00f6nnen\ndurch eine deutsche Hochschulzugangsberechtigung (u. a. erfolgreich abgeschlossenes\ngrundst\u00e4ndiges Hochschulstudium) nachgewiesen werden. 3Ferner kann der\nSprachnachweis durch die Vorlage eines der folgenden Dokumente erbracht werden:\n1. Feststellungspr\u00fcfung f\u00fcr ein Bachelorstudium durch Vorlage der Zugangsberechtigung\ndes Studienkollegs an der Hochschule Konstanz,\n2. Test Deutsch als Fremdsprache (TestDaF), sofern im Durchschnitt mindestens die\nStufe TDN 4 erreicht wurde,   Seite 5 von 43 3. Deutsche Sprachpr\u00fcfung f\u00fcr den Hochschulzugang (DSH), sofern die DSH mit\nmindestens der Stufe DSH -2 abgeschlossen wurde,\n4. \u201eTelc Deutsch C1 Hochschule\u201c\noder eine \u00e4quivalente Sprachpr\u00fcfung gem\u00e4\u00df der Rahmenordnung \u00fcber Deutsche\nSprachpr\u00fcfungen f\u00fcr das Studium an deutschen Hochschulen (RO -DT). 4Auf den Nachweis\neiner deutschen Sprachpr\u00fcfung kann bei Bewerber innen und Bewerbern im besonders\nbegr\u00fcndeten Einzelfall verzichtet werden, insbesondere wenn sie die deutsche\nStaatsangeh\u00f6rigkeit besitzen.\n(2) 1Sprachnachweise f\u00fcr den gew\u00e4hl ten Studiengang, die durch die Bewerberin oder den\nBewerber bis zum Bewerbungsschluss nicht vorgelegt werden k\u00f6nnen, k\u00f6nnen bis zum\nVorlesungsbeginn des Semesters gem\u00e4\u00df Terminplan der Hochschule Konstanz, f\u00fcr das  der\nAntrag auf Zulassung gestellt wurde, nachgereicht werden. 2Die Zulassung erfolgt in diesem\nFall gem \u00e4\u00df \u00a7 6  Abs. 5  unter Vorbehalt .\n(3) 1F\u00fcr Zeitstudierende gelten die Regelungen in \u00a7 10 Zulassungs - und\nImmatrikulationsordnung (ZIO) der Hochschule Konstanz.", "metadata": {"file_path": "/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf"}, "type": "Document"}, {"page_content": "\n\u00a7 21b Mechatronik (MME) Berufsbegleitendes Studium\n(1) Studiengangspezifische Zugangsvoraussetzungen gem\u00e4\u00df \u00a7 5  Abs. 1\nZugangsvoraussetzungen f\u00fcr den Masterstudiengang Mechatronik sind:\n1. Ein mit der Note 2,9 oder besser abgeschlossenes grundst\u00e4ndiges Hochschulstudium\ngem\u00e4\u00df \u00a7 5 Abs. 1 Nr. 1 in einem Studiengang der Fachrichtungen Systemtechnik,\nMaschinenbau, Elektrotechnik, Fahrzeugtechnik, Mechatronik, Feinwerktechnik oder einer\nverwandten Fachrichtung.\n2. Englischkenntnisse, \u00e4quivalent z u Niveau- Stufe B1 des Europ\u00e4ischen Referenzrahmens\nf\u00fcr das Lernen und Lehren von Fremdsprachen. Als \u00e4quivalent zu einem Zertifikat \u00fcber die\nNiveau -Stufe B1 gelten insbesondere folgende Nachweise:\nI. das Schulabschlusszeugnis, aus dem der Besuch des Englischunterrichts bis zum\nErreichen des mittleren Bildungsabschlusses (10. Klasse) bzw. bis zum Erreichen\nder Fachhochschulreife hervorgeht oder\nII. ein Notenspiegel, aus dem die bestandene Pr\u00fcfungsleistung \u00fcber eine\nLehrveranstaltung im Rahmen des grundst\u00e4ndigen Studiums hervorgeht, die die\nenglische Sprache zum Inhalt hatte oder\nIII. eine Bescheinigung \u00fcber den mindestens sechsmonatigen Aufenthalt an einer Schule, Hochschule oder anderen Bildungsinstitution mit Englisch als\nUnterrichtssprache oder\nIV. eine Bescheinigung \u00fcber den Aufenthalt im englischsprachigen Ausland, der einen Zeitraum von mindestens sechs Monaten bzw. einem Studiensemester umfasst.\nDie Vorlage anderer geeigneter Nachweise ist m\u00f6glich.\n(2) Auswahlkriterien nach \u00a7 9 Abs. 2\n1. Ergebnis eines Auswahlgespr\u00e4chs\nNicht zutreffend.\n2. Leistungen, die mit der Abschlusspr\u00fcfung des grundst\u00e4ndigen Studiums nach Abs. 1\ni. V. m. \u00a7 5 Abs. 1 Nr. 1 nachgewiesen sind\nDie Durchschnittsnote der Abschlusspr\u00fcfung des grundst\u00e4ndigen Hochschulstudiums nach\nAbs. 1 bildet die Teilnote 1 als Basis zur Bestimmung der Auswahlnote.  Abweichend von Satz\n1 bildet in den F\u00e4llen des \u00a7 3 Abs. 2 Nr. 1 Satz 2 die Durchschnittsnote nach \u00a7 3 Abs. 2 Nr. 1 Satz 3 die Teilnote 1. Bei ausl\u00e4ndischen Bildungsnachweisen ist die Durchschnittsnote nach\ndeutsc her Deutung als Teilnote 1 zu ber\u00fccksichtigen.\nZus\u00e4tzlich werden die Einzelnoten folgender F\u00e4cher der Abschlusspr\u00fcfung des grundst\u00e4ndigen Hochschulstudiums, die \u00fcber die Eignung f\u00fcr den gew\u00e4hlten Studiengang\nbesonderen Aufschluss geben, f\u00fcr die Auswahl herangezogen:\n- Technische Mechanik (Dynamik),\n- Elektrotechnik,\n- Messtechnik,\n- Regelungstechnik,\n- Elektrische Antriebe.\nDabei wird eine Note zwischen 1,0 und 1,7 in einem der o. g. F\u00e4cher jeweils mit dem Wert 0,1\nbewertet. Die kumulierte Gesamtzahl bildet die Teil note 2.", "metadata": {"file_path": "/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf"}, "type": "Document"}, {"page_content": "\n\u00a7 21a Mechatronik (MME) Vollzeitstudium\n(1) Studiengangspezifische Zugangsvoraussetzungen gem\u00e4\u00df \u00a7 5  Abs. 1\nZugangsvoraussetzungen f\u00fcr den Masterstudiengang Mechatronik sind:\n1. Ein mit der Note 2,9 oder besser abgeschlossenes grundst\u00e4ndiges Hochschulstudium\ngem\u00e4\u00df \u00a7 5 Abs. 1 Nr. 1 in einem Studiengang der Fachrichtungen Maschinenbau,\nElektrotechnik, Fahrzeugtechnik, Mechatronik, Feinwerktechnik oder einer verwandten\nFachrichtung.\n2. Englischkenntnisse, \u00e4quivalent zu Niveau- Stufe B1 des Europ\u00e4ischen Referenzrahmens\nf\u00fcr das Lernen und Lehren von Fremdsprachen. Als \u00e4quivalent zu einem Zertifikat \u00fcber die\nNiveau -Stufe B1 gelten insbesondere folgende Nachweise:\nI. das Schulabschlusszeugnis, aus dem der Besuch des Englischunterrichts bis zum\nErreichen des mittleren Bildungsabschlusses (10. Klass e) bzw. bis zum Erreichen\nder Fachhochschulreife hervorgeht oder\nII. ein Notenspiegel, aus dem die bestandene Pr\u00fcfungsleistung \u00fcber eine\nLehrveranstaltung im Rahmen des grundst\u00e4ndigen Studiums hervorgeht, die die\nenglische Sprache zum Inhalt hatte oder\nIII. eine Bescheinigung \u00fcber den mindestens sechsmonatigen Aufenthalt an einer Schule, Hochschule oder anderen Bildungsinstitution mit Englisch als\nUnterrichtssprache oder\nIV. eine Bescheinigung \u00fcber den Aufenthalt im englischsprachigen Ausland, der einen Zeitraum von mindestens sechs Monaten bzw. einem Studiensemester umfasst.\nDie Vorlage anderer geeigneter Nachweise ist m\u00f6glich.\n(2) Auswahlkriterien nach \u00a7 9 Abs. 2\n1. Ergebnis eines Auswahlgespr\u00e4chs\nNicht zutreffend.\n2. Leistungen, die mit der Abschlusspr\u00fcfung des grundst\u00e4ndigen Studiums nach Abs. 1\ni. V. m. \u00a7 5 Abs. 1 Nr. 1 nachgewiesen sind\nDie Durchschnittsnote der Abschlusspr\u00fcfung des grundst\u00e4ndigen Hochschulstudiums nach\nAbs. 1 bildet die Teilnote 1 als Basis zur Bestimmung der Auswahlnote. Abweichend von Satz\n1 bildet in den F\u00e4llen des \u00a7 3 Abs. 2 Nr. 1 Satz 2 die Durchschnittsnote nach \u00a7 3 Abs. 2 Nr. 1\nSatz 3 die Teilnote 1. Bei ausl\u00e4ndischen Bildungsnachweisen ist die Durchschnittsnote nach\ndeutscher Deutung als Teilnote 1 zu ber\u00fccksichtigen.\nZus\u00e4tzlich werden die Einzelnoten folgender F\u00e4cher der Abschlusspr\u00fcfung des grundst\u00e4ndigen Hochschulstudiums, die \u00fcber die Eignung f\u00fcr den gew\u00e4hlten Studiengang\nbesonderen Aufschluss geben, f\u00fcr die Auswahl herangezogen:\n- Technische Mechanik (Dynamik),\n- Elektrotechnik,\n- Messt echnik,\n- Regelungstechnik,\n- Elektrische Antriebe.\nDabei wird eine Note zwischen 1,0 und 1,7 in einem der o. g. F\u00e4cher jeweils mit dem Wert 0,1\nbewertet. Die kumulierte Gesamtzahl bildet die Teilnote 2.", "metadata": {"file_path": "/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf"}, "type": "Document"}, {"page_content": "\n\u00a7 9 Zugangs - und Auswahlkriterien in den Masterstudieng\u00e4ngen\n(1)  1Im Besonderen Teil (\u00a7\u00a7 12 -26) dieser Satzung k\u00f6nnen ein oder mehrere der in Absatz 2\ngenannten Auswahlkriterien als weitere Zugangskriterien festgelegt werden. 2N\u00e4heres\nregelt der B esondere Teil f\u00fcr den jeweiligen Studiengang (\u00a7\u00a7 12 -26).\n(2)  1F\u00fcr die Bildung der Ranglisten f\u00fcr das erste Fachsemester in den Masterstudieng\u00e4ngen\nwird, neben dem Ergebnis des fachlich einschl\u00e4gigen Hochschulabschlusses oder des\ngleichwertigen Abschlusses,  mindestens eines der folgenden  Auswahlkriterien\nber\u00fccksichtigt:\n1. Leistungen, die in dem Studium erbracht wurden, das Voraussetzung f\u00fcr den Zugang\nzu dem Masterstudiengang ist ,   Seite 8 von 43 2. Englischkenntnisse , n\u00e4heres regelt der Besondere Teil f\u00fcr den jeweiligen Studiengang\n(\u00a7\u00a7 12 -26),\n3. Berufst\u00e4tigkeit und Qualifikationen:\na) Art einer abgeschlossenen Berufsausbildung oder einer Berufst\u00e4tigkeit in einem\nanerkannten Ausbildungsberuf  oder eine andere einschl\u00e4gige Berufst\u00e4tigkeit , die \u00fcber\ndie fachspezifische Eignung Auskunft gibt, jeweils  einzeln und in Kombination, und\nb) Qualifikation en, die \u00fcber die fachspezifische Leistung Auskunft geben, jeweils einzeln\noder in Kombination,\n4. das Ergebnis eines fachspezifischen Studieneignungstests ,\n5. das Ergebnis des Auswahlgespr\u00e4chs/anderen m\u00fcndlichen Verfahrens  gem\u00e4\u00df \u00a7 9a ,\n6. ein Motivationsschreiben,\n7. eine schriftliche Abhandlung (Essay).\n2N\u00e4heres sowie die Gewichtung regelt der B esondere Teil f\u00fcr den jeweiligen Studiengang (\u00a7\u00a7\n12-26).\n(2) 1Die Auswahl f\u00fcr h\u00f6here Fachsemester erfolgt gem\u00e4\u00df \u00a7 7 HZG i. V. m. \u00a7 32 HZVO.", "metadata": {"file_path": "/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf"}, "type": "Document"}]
        context = [entry['page_content'] for entry in data]
        eval_prompt = create_eval_prompt("""Welche Dokumente kÃ¶nnen als Nachweis fÃ¼r deutsche Sprachkenntnisse akzeptiert werden?""", str(context))

        from typing import (
        Optional,

        )
        from langchain_core.runnables import RunnableConfig, ensure_config
        


        '''
        from transformers.pipelines.text_generation import ReturnType
        def postprocess(self, model_outputs, return_type=ReturnType.FULL_TEXT, clean_up_tokenization_spaces=True):
            return model_outputs
        
        from transformers import TextClassificationPipeline
        from transformers import Pipeline


        with ClassExtender(Pipeline, postprocess), ClassExtender(TextClassificationPipeline, postprocess):
            output = llm.invoke(eval_prompt)
        '''

        from typing import Any, List, Optional

        from langchain_core.callbacks import CallbackManagerForLLMRun
        from langchain_core.outputs import Generation, LLMResult
        def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
            # List to hold all results
            text_generations: List[str] = []
            model_outputs: List[Any] = []
            pipeline_kwargs = kwargs.get("pipeline_kwargs", {})

            for i in range(0, len(prompts), self.batch_size):
                batch_prompts = prompts[i : i + self.batch_size]

                # Process batch of prompts
                responses = self.pipeline(
                    batch_prompts,
                    **pipeline_kwargs,
                )

                # Process each response in the batch
                for j, response in enumerate(responses):
                    if isinstance(response, list):
                        # if model returns multiple generations, pick the top one
                        response = response[0]

                    if self.pipeline.task == "text-generation":
                        text = response["generated_text"]
                        model_output = {
                            "logits": response["model_outputs"]["logits"],
                            "sequences": response["model_outputs"]["sequences"],
                            "input_ids": response["input_ids"]
                        }
                    elif self.pipeline.task == "text2text-generation":
                        text = response["generated_text"]
                    elif self.pipeline.task == "summarization":
                        text = response["summary_text"]
                    elif self.pipeline.task in "translation":
                        text = response["translation_text"]
                    else:
                        raise ValueError(
                            f"Got invalid task {self.pipeline.task}, "
                            f"currently only {VALID_TASKS} are supported"
                        )

                    # Append the processed text to results
                    text_generations.append(text)
                    if model_output is not None: model_outputs.append(model_output)

            return LLMResult(
                generations=[[Generation(text=text)] for text in text_generations],
                llm_output={
                    "model_outputs": model_outputs
                }
            )
        
        def invoke(self, input: str) -> LLMResult:
            return (
                self.generate_prompt(
                    [self._convert_input(input)],
                )
            )
        
        output: LLMResult = LLMResult(generations=[], llm_output={})
        
        with ClassExtender(type(llm), invoke), ClassExtender(type(llm), _generate):
            output = llm.invoke(eval_prompt) # type: ignore

        if output.llm_output is None:
            print("MISSING LLM Outpout") 
            return
        print(output.llm_output["model_outputs"])
        logits = output.llm_output["model_outputs"][0]["logits"]
        sequences = output.llm_output["model_outputs"][0]["sequences"]
        input_ids = output.llm_output["model_outputs"][0]["input_ids"]

        import torch
        import torch.nn.functional as F


        # Assuming `logits` is a tensor of shape (sequence_length, vocab_size)
        log_probs = F.log_softmax(torch.stack(list(logits), dim=0).squeeze(), dim=-1)
        #log_probs = torch.log(probs)
        nll = F.nll_loss(log_probs, sequences[-1][input_ids.shape[1]:], reduction='none') # mask away prompt
        average_nll = nll.mean()
        # Perplexity is the exponentiation of the average NLL
        perplexity = torch.exp(average_nll)
        print("PERPLEXITY: ", perplexity)

        print("\n\nLOGPROBS: ", log_probs)

        def score(log_probs, sequence_ids):
            sequence_tokens = [tokenizer.decode(id) for id in sequence_ids]
            token_logprobs = []
            for k in range(1, sequence_ids.shape[0]):
                token_logprobs.append(log_probs[k-1, sequence_ids[k]])
            return sequence_tokens, token_logprobs

        
        result = score(log_probs, sequences[-1][input_ids.shape[1]:] )

        sequence_ids = sequences[-1][input_ids.shape[1]:]

        sequence_tokens = [tokenizer.decode(id) for id in sequence_ids]

        print("SCORE: ", result)

        print("NLL: ", nll)

        for tok, log_prob in zip(sequence_tokens, nll):

            # | token | token string | logits | probability

            print(f"| {tok:8s} | {log_prob:.3f} | {torch.exp(log_prob):.2%}")

if __name__ == "__main__":
    main()



'''
def _forward(self, model_inputs, **generate_kwargs):
        input_ids = model_inputs["input_ids"]
        attention_mask = model_inputs.get("attention_mask", None)
        # Allow empty prompts
        if input_ids.shape[1] == 0:
            input_ids = None
            attention_mask = None
            in_b = 1
        else:
            in_b = input_ids.shape[0]
        prompt_text = model_inputs.pop("prompt_text")

        # If there is a prefix, we may need to adjust the generation length. Do so without permanently modifying
        # generate_kwargs, as some of the parameterization may come from the initialization of the pipeline.
        prefix_length = generate_kwargs.pop("prefix_length", 0)
        if prefix_length > 0:
            has_max_new_tokens = "max_new_tokens" in generate_kwargs or (
                "generation_config" in generate_kwargs
                and generate_kwargs["generation_config"].max_new_tokens is not None
            )
            if not has_max_new_tokens:
                generate_kwargs["max_length"] = generate_kwargs.get("max_length") or self.model.config.max_length
                generate_kwargs["max_length"] += prefix_length
            has_min_new_tokens = "min_new_tokens" in generate_kwargs or (
                "generation_config" in generate_kwargs
                and generate_kwargs["generation_config"].min_new_tokens is not None
            )
            if not has_min_new_tokens and "min_length" in generate_kwargs:
                generate_kwargs["min_length"] += prefix_length

        # BS x SL
        output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
        #print(generated_sequence)
        #warnings.warn(
        #            str(generated_sequence)
        #)
        # fix this for deconderoutputonly -> get sequence[0].shape or shape[1]
        generated_sequence = output.sequences[0]
        #warnings.warn(
        #           str(generated_sequence)
        #)
        out_b = generated_sequence.shape[0]
        if self.framework == "pt":
            generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])
        elif self.framework == "tf":
            generated_sequence = tf.reshape(generated_sequence, (in_b, out_b // in_b, *generated_sequence.shape[1:]))
        return {"generated_sequence": generated_sequence, "input_ids": input_ids, "prompt_text": prompt_text, "output": output}

'''
