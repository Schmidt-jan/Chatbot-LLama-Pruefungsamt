{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch torchvision torchaudio peft accelerate sentencepiece gguf\n",
    "#!pip install -U git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python llama.cpp/convert-hf-to-gguf.py unload --outfile model/mistral-rag-instruct.gguf --outtype f16\n",
    "#./llama.cpp/llama-quantize ./model/mistral-rag-instruct.gguf ./model/mistral-rag-instruct.Q4.gguf Q4_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging PEFT (LoRA) Adapter into base model and saving it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch; torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56a32f3a89a4a9eabb4951275102f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "# checkpoints/raft/checkpoint-425\n",
    "# checkpoints/raft-v2/checkpoint-250\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", torch_dtype=torch.bfloat16, device_map={\"\": \"cpu\"})\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, \"checkpoints/raft-v2/checkpoint-250\", device_map={\"\": \"cpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bfloat16'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.to_dict()[\"torch_dtype\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloading and merging model:   0%|          | 0/679 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloading and merging model: 100%|██████████| 679/679 [00:16<00:00, 40.26it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.merge_and_unload(safe_merge=True, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.embed_tokens.weight',\n",
       "              tensor([[-2.0336e-36,  3.3208e-37, -1.5517e-35,  ..., -4.9371e-36,\n",
       "                       -7.9934e-36, -5.9480e-36],\n",
       "                      [-4.4250e-03, -2.0981e-05, -5.6458e-03,  ..., -6.5804e-05,\n",
       "                       -1.0300e-03,  8.5354e-05],\n",
       "                      [-1.4572e-03,  8.6975e-04,  1.7357e-04,  ...,  1.0910e-03,\n",
       "                        3.1471e-04,  3.0994e-05],\n",
       "                      ...,\n",
       "                      [ 4.4556e-03, -1.4877e-03,  3.8338e-04,  ..., -5.3942e-06,\n",
       "                        5.5237e-03,  1.2436e-03],\n",
       "                      [-5.3024e-04,  3.3264e-03,  3.2501e-03,  ...,  1.1139e-03,\n",
       "                        1.4572e-03,  9.0408e-04],\n",
       "                      [-3.8147e-03, -6.0654e-04, -5.8594e-03,  ...,  4.2419e-03,\n",
       "                       -4.3106e-04, -2.5024e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.0.self_attn.q_proj.weight',\n",
       "              tensor([[ 9.2983e-06,  7.7057e-04, -3.6240e-05,  ...,  3.7079e-03,\n",
       "                       -1.8716e-05, -7.4768e-04],\n",
       "                      [-9.1195e-06, -4.7922e-05,  4.1246e-05,  ...,  1.6117e-04,\n",
       "                        3.0756e-05,  3.7956e-04],\n",
       "                      [ 6.2943e-05, -4.9591e-04, -1.4782e-05,  ..., -6.0730e-03,\n",
       "                        3.6955e-05, -1.8787e-04],\n",
       "                      ...,\n",
       "                      [-7.9870e-06, -1.4648e-03,  5.1260e-06,  ...,  4.5776e-04,\n",
       "                        2.3723e-05,  4.4250e-04],\n",
       "                      [ 5.9366e-05, -1.6708e-03,  4.2677e-05,  ..., -2.3956e-03,\n",
       "                       -4.0293e-05,  7.0953e-04],\n",
       "                      [-1.2279e-05,  1.7166e-03, -1.6451e-05,  ..., -1.8024e-04,\n",
       "                       -3.3140e-05, -5.8746e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.0.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.6689e-05, -3.4027e-03,  7.4387e-05,  ..., -1.6479e-02,\n",
       "                        1.9646e-04, -1.4648e-03],\n",
       "                      [ 6.1095e-06,  1.8387e-03, -5.7220e-05,  ...,  6.8054e-03,\n",
       "                       -1.0824e-04,  1.4343e-03],\n",
       "                      [ 3.4720e-06,  3.2349e-03, -4.2439e-05,  ...,  1.8188e-02,\n",
       "                       -1.4591e-04,  8.3160e-04],\n",
       "                      ...,\n",
       "                      [-3.5667e-04,  2.6855e-03, -4.1962e-04,  ...,  2.5787e-03,\n",
       "                        3.5095e-04, -6.2180e-04],\n",
       "                      [-2.5940e-04, -3.5248e-03, -2.6512e-04,  ...,  1.3809e-03,\n",
       "                        4.8828e-04, -2.5177e-03],\n",
       "                      [ 3.6049e-04, -2.5330e-03,  4.4632e-04,  ..., -2.3499e-03,\n",
       "                       -3.7384e-04,  1.1902e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.0.self_attn.v_proj.weight',\n",
       "              tensor([[-4.2534e-04, -2.7008e-03, -5.9509e-04,  ...,  4.3640e-03,\n",
       "                        2.0409e-04,  1.9455e-03],\n",
       "                      [-2.9373e-04,  5.3711e-03, -1.7262e-04,  ...,  3.7842e-03,\n",
       "                       -8.5068e-04, -2.7180e-05],\n",
       "                      [ 1.8692e-04,  3.5553e-03, -2.9564e-04,  ..., -2.7299e-05,\n",
       "                        1.8254e-06, -5.5847e-03],\n",
       "                      ...,\n",
       "                      [ 2.3270e-04,  1.9226e-03, -2.1553e-04,  ...,  3.2501e-03,\n",
       "                       -1.8921e-03, -1.8311e-03],\n",
       "                      [-3.3379e-04, -4.6997e-03, -4.2343e-04,  ...,  2.9602e-03,\n",
       "                        6.5613e-04, -1.1902e-03],\n",
       "                      [-6.8665e-04, -3.1433e-03,  6.2561e-04,  ..., -8.6594e-04,\n",
       "                        7.3624e-04,  1.8234e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.0.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.0529e-03,  4.7302e-03,  4.9744e-03,  ..., -7.3624e-04,\n",
       "                       -9.2697e-04,  4.7112e-04],\n",
       "                      [-6.1035e-04, -1.7471e-03, -1.3199e-03,  ...,  5.7983e-04,\n",
       "                       -9.4604e-04, -9.2697e-04],\n",
       "                      [-1.6937e-03,  9.9945e-04, -2.0027e-05,  ...,  8.7357e-04,\n",
       "                        3.6316e-03, -3.3417e-03],\n",
       "                      ...,\n",
       "                      [ 4.6997e-03, -1.4725e-03,  1.8311e-03,  ..., -3.4485e-03,\n",
       "                        1.7853e-03,  4.6692e-03],\n",
       "                      [ 2.6245e-03, -3.1586e-03, -2.5635e-03,  ..., -6.4850e-04,\n",
       "                       -2.5787e-03, -1.1444e-05],\n",
       "                      [ 4.6692e-03, -3.1433e-03, -2.3804e-03,  ...,  1.2512e-03,\n",
       "                        7.6675e-04, -4.8828e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.0.mlp.gate_proj.weight',\n",
       "              tensor([[-4.6692e-03, -5.1880e-04, -1.6251e-03,  ...,  2.0294e-03,\n",
       "                        3.0365e-03,  2.7618e-03],\n",
       "                      [ 2.4872e-03, -6.4850e-04,  1.3046e-03,  ...,  1.1978e-03,\n",
       "                        3.4637e-03,  8.5831e-04],\n",
       "                      [-1.3504e-03,  1.5717e-03,  9.2697e-04,  ..., -4.1504e-03,\n",
       "                       -1.2741e-03, -6.2943e-04],\n",
       "                      ...,\n",
       "                      [ 1.8082e-03, -5.1117e-04, -3.4027e-03,  ...,  4.5395e-04,\n",
       "                       -3.3875e-03,  2.1362e-03],\n",
       "                      [ 1.3962e-03, -4.3945e-03,  2.2650e-05,  ..., -7.8201e-04,\n",
       "                        1.0986e-03,  3.4790e-03],\n",
       "                      [ 2.4109e-03,  1.1873e-04,  4.0588e-03,  ...,  2.0447e-03,\n",
       "                       -1.6556e-03, -4.4861e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.0.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0001, -0.0006, -0.0004,  ...,  0.0052,  0.0042,  0.0004],\n",
       "                      [-0.0033, -0.0015, -0.0020,  ...,  0.0013,  0.0016, -0.0008],\n",
       "                      [-0.0001,  0.0014, -0.0006,  ...,  0.0004, -0.0008, -0.0005],\n",
       "                      ...,\n",
       "                      [-0.0030, -0.0019,  0.0014,  ..., -0.0042, -0.0017,  0.0052],\n",
       "                      [ 0.0029, -0.0024, -0.0063,  ..., -0.0006, -0.0063,  0.0044],\n",
       "                      [-0.0014, -0.0017,  0.0006,  ...,  0.0021, -0.0052, -0.0009]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.0.mlp.down_proj.weight',\n",
       "              tensor([[-0.0028, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],\n",
       "                      [ 0.0013, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],\n",
       "                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],\n",
       "                      ...,\n",
       "                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],\n",
       "                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],\n",
       "                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.0.input_layernorm.weight',\n",
       "              tensor([-3.9816e-05, -1.2512e-02, -2.0981e-05,  ...,  3.7598e-02,\n",
       "                      -1.8024e-04,  7.6599e-03], dtype=torch.bfloat16)),\n",
       "             ('model.layers.0.post_attention_layernorm.weight',\n",
       "              tensor([0.4277, 0.4219, 0.4043,  ..., 0.4297, 0.4082, 0.4102],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.1.self_attn.q_proj.weight',\n",
       "              tensor([[ 1.4267e-03, -1.0757e-03, -7.1526e-06,  ...,  2.1973e-03,\n",
       "                        5.8746e-04,  2.6550e-03],\n",
       "                      [-1.0824e-04, -4.9973e-04,  4.7445e-05,  ..., -1.7166e-03,\n",
       "                        1.9684e-03,  6.9141e-05],\n",
       "                      [ 3.5095e-03, -2.2583e-03, -4.2319e-06,  ...,  2.6398e-03,\n",
       "                       -3.7537e-03,  7.6294e-04],\n",
       "                      ...,\n",
       "                      [ 1.3733e-03, -3.2806e-03,  1.4901e-05,  ..., -2.8229e-03,\n",
       "                       -6.3782e-03, -2.4261e-03],\n",
       "                      [ 2.3193e-03,  5.8289e-03, -1.5497e-05,  ..., -1.6212e-04,\n",
       "                        4.6082e-03,  1.8539e-03],\n",
       "                      [ 5.3101e-03, -2.3956e-03,  6.6757e-06,  ..., -2.8534e-03,\n",
       "                       -6.7749e-03, -3.1433e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.1.self_attn.k_proj.weight',\n",
       "              tensor([[-2.7313e-03,  9.8877e-03,  1.4827e-06,  ..., -3.2196e-03,\n",
       "                        1.2817e-03,  2.0142e-03],\n",
       "                      [ 5.7602e-04,  3.6774e-03,  7.3433e-05,  ..., -5.1975e-05,\n",
       "                       -7.0190e-03,  2.2888e-03],\n",
       "                      [-2.9602e-03,  3.4180e-03,  1.6403e-04,  ..., -1.2207e-03,\n",
       "                        1.9302e-03, -9.9182e-04],\n",
       "                      ...,\n",
       "                      [ 2.0294e-03, -2.8229e-03,  1.3828e-05,  ..., -1.6022e-03,\n",
       "                       -9.9182e-04, -1.5030e-03],\n",
       "                      [ 2.0905e-03,  3.1738e-03,  3.1710e-05,  ..., -2.0294e-03,\n",
       "                        1.7700e-03,  3.4790e-03],\n",
       "                      [ 6.8665e-03, -1.9684e-03, -3.5465e-06,  ...,  2.4109e-03,\n",
       "                        2.7008e-03, -7.4387e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.1.self_attn.v_proj.weight',\n",
       "              tensor([[-4.9133e-03, -3.1128e-03, -2.4223e-04,  ...,  2.7466e-04,\n",
       "                       -2.1696e-05, -1.2283e-03],\n",
       "                      [ 4.3945e-03, -4.6253e-05, -3.7998e-06,  ..., -6.9885e-03,\n",
       "                        1.4191e-03, -3.4180e-03],\n",
       "                      [-2.7657e-04,  1.4114e-03,  6.2943e-05,  ...,  2.2278e-03,\n",
       "                       -2.7657e-04,  1.9684e-03],\n",
       "                      ...,\n",
       "                      [-2.7771e-03, -3.6163e-03, -9.6321e-05,  ..., -3.0518e-03,\n",
       "                        4.5471e-03, -1.3447e-04],\n",
       "                      [ 3.9673e-04, -4.0283e-03, -1.2493e-04,  ..., -5.6458e-04,\n",
       "                       -1.2512e-03,  3.8300e-03],\n",
       "                      [-2.0409e-04,  6.2561e-04,  3.1662e-04,  ...,  1.3885e-03,\n",
       "                       -2.1973e-03,  9.9945e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.1.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.2779e-04, -6.1417e-04, -1.6098e-03,  ..., -1.0376e-03,\n",
       "                        2.4414e-04, -3.3855e-05],\n",
       "                      [-3.4485e-03, -3.8910e-04, -8.6784e-05,  ..., -3.2196e-03,\n",
       "                        1.0681e-03,  9.1934e-04],\n",
       "                      [ 6.0272e-04, -7.5531e-04,  2.8534e-03,  ...,  2.5024e-03,\n",
       "                       -2.7008e-03, -5.7983e-04],\n",
       "                      ...,\n",
       "                      [-1.0834e-03, -3.9368e-03, -2.6703e-03,  ...,  1.2817e-03,\n",
       "                       -1.9989e-03,  1.7395e-03],\n",
       "                      [ 3.9291e-04,  1.5411e-03,  3.6049e-04,  ...,  7.6294e-03,\n",
       "                        5.7983e-04, -2.2583e-03],\n",
       "                      [-3.2902e-05,  3.2501e-03,  4.2534e-04,  ...,  5.2490e-03,\n",
       "                       -3.1281e-03, -2.2888e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.1.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0044,  0.0059,  0.0017,  ..., -0.0057, -0.0033,  0.0012],\n",
       "                      [-0.0037, -0.0006,  0.0044,  ...,  0.0016, -0.0015, -0.0060],\n",
       "                      [ 0.0052, -0.0024,  0.0020,  ...,  0.0009,  0.0055,  0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0011,  0.0067, -0.0042,  ...,  0.0005,  0.0019,  0.0004],\n",
       "                      [ 0.0021, -0.0021, -0.0008,  ...,  0.0007,  0.0005, -0.0051],\n",
       "                      [ 0.0022, -0.0010, -0.0024,  ..., -0.0014, -0.0001, -0.0013]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.1.mlp.up_proj.weight',\n",
       "              tensor([[-0.0074,  0.0053,  0.0020,  ..., -0.0018,  0.0024, -0.0027],\n",
       "                      [ 0.0017,  0.0017,  0.0067,  ...,  0.0021,  0.0099,  0.0006],\n",
       "                      [ 0.0007,  0.0013,  0.0017,  ..., -0.0025, -0.0020,  0.0064],\n",
       "                      ...,\n",
       "                      [ 0.0020,  0.0019,  0.0024,  ..., -0.0027, -0.0009, -0.0020],\n",
       "                      [ 0.0028,  0.0011, -0.0005,  ...,  0.0019, -0.0024, -0.0006],\n",
       "                      [-0.0007, -0.0005,  0.0008,  ...,  0.0025,  0.0006,  0.0015]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.1.mlp.down_proj.weight',\n",
       "              tensor([[-0.0020, -0.0043,  0.0006,  ...,  0.0013,  0.0082, -0.0005],\n",
       "                      [-0.0027,  0.0025, -0.0017,  ...,  0.0049, -0.0053,  0.0008],\n",
       "                      [ 0.0052, -0.0028,  0.0005,  ...,  0.0054,  0.0040,  0.0074],\n",
       "                      ...,\n",
       "                      [-0.0025,  0.0024,  0.0025,  ..., -0.0058, -0.0002,  0.0022],\n",
       "                      [-0.0013, -0.0012, -0.0048,  ..., -0.0010, -0.0010, -0.0014],\n",
       "                      [-0.0029, -0.0014, -0.0040,  ...,  0.0005,  0.0028,  0.0036]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.1.input_layernorm.weight',\n",
       "              tensor([1.6797e-01, 1.0840e-01, 2.0117e-06,  ..., 1.7480e-01, 1.0791e-01,\n",
       "                      6.9824e-02], dtype=torch.bfloat16)),\n",
       "             ('model.layers.1.post_attention_layernorm.weight',\n",
       "              tensor([0.6484, 0.6406, 0.6562,  ..., 0.6367, 0.6602, 0.6523],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.2.self_attn.q_proj.weight',\n",
       "              tensor([[-4.1809e-03, -9.9945e-04, -3.3569e-03,  ...,  1.5488e-03,\n",
       "                        4.9438e-03, -8.9645e-04],\n",
       "                      [-2.0294e-03,  1.6174e-03,  4.4441e-04,  ..., -1.7395e-03,\n",
       "                        3.6621e-03,  1.5564e-03],\n",
       "                      [ 2.3041e-03, -2.2583e-03,  1.7548e-03,  ...,  1.5564e-03,\n",
       "                        1.0376e-03,  6.7139e-03],\n",
       "                      ...,\n",
       "                      [ 1.7929e-03, -1.1749e-03,  1.4526e-02,  ...,  1.0681e-02,\n",
       "                        5.9509e-03, -7.7209e-03],\n",
       "                      [-6.3477e-03, -1.6556e-03, -2.0294e-03,  ..., -6.1951e-03,\n",
       "                       -2.6398e-03, -4.4861e-03],\n",
       "                      [ 8.2016e-04,  3.7994e-03,  1.9989e-03,  ...,  1.7319e-03,\n",
       "                       -9.1553e-05, -1.0071e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.2.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0038, -0.0018,  0.0057,  ..., -0.0002, -0.0007, -0.0003],\n",
       "                      [ 0.0010,  0.0063, -0.0048,  ...,  0.0025, -0.0040,  0.0032],\n",
       "                      [-0.0021, -0.0016,  0.0042,  ..., -0.0006,  0.0035, -0.0022],\n",
       "                      ...,\n",
       "                      [ 0.0009,  0.0003,  0.0123,  ...,  0.0028,  0.0002, -0.0029],\n",
       "                      [ 0.0004,  0.0006,  0.0089,  ..., -0.0062, -0.0079,  0.0021],\n",
       "                      [ 0.0070, -0.0029,  0.0062,  ..., -0.0084, -0.0044, -0.0132]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.2.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0006,  0.0005, -0.0015,  ...,  0.0014,  0.0057, -0.0034],\n",
       "                      [-0.0005, -0.0010,  0.0006,  ...,  0.0021,  0.0017, -0.0060],\n",
       "                      [-0.0016,  0.0003, -0.0002,  ...,  0.0025,  0.0028,  0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0033,  0.0007, -0.0027,  ..., -0.0014,  0.0012, -0.0004],\n",
       "                      [-0.0025,  0.0024, -0.0004,  ..., -0.0016,  0.0007, -0.0011],\n",
       "                      [ 0.0035, -0.0016,  0.0021,  ...,  0.0029,  0.0032, -0.0018]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.2.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0036,  0.0005, -0.0007,  ...,  0.0044,  0.0013,  0.0010],\n",
       "                      [-0.0016,  0.0012,  0.0035,  ...,  0.0004,  0.0002,  0.0033],\n",
       "                      [-0.0010,  0.0015,  0.0009,  ..., -0.0050,  0.0017, -0.0028],\n",
       "                      ...,\n",
       "                      [ 0.0011,  0.0014,  0.0033,  ...,  0.0010,  0.0035,  0.0011],\n",
       "                      [-0.0013,  0.0005,  0.0003,  ..., -0.0019, -0.0019, -0.0037],\n",
       "                      [-0.0010, -0.0026, -0.0028,  ...,  0.0033,  0.0002,  0.0020]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.2.mlp.gate_proj.weight',\n",
       "              tensor([[-5.3406e-03, -1.9312e-05,  8.3160e-04,  ..., -2.7161e-03,\n",
       "                       -1.0834e-03,  1.1139e-03],\n",
       "                      [ 4.4060e-04,  4.4250e-03,  5.9509e-03,  ...,  1.8539e-03,\n",
       "                       -2.0142e-03, -2.8381e-03],\n",
       "                      [-9.8419e-04,  2.3651e-03, -4.7607e-03,  ..., -3.9062e-03,\n",
       "                       -7.5340e-05, -6.6223e-03],\n",
       "                      ...,\n",
       "                      [ 1.4114e-04,  2.7466e-03, -3.1281e-03,  ..., -1.6937e-03,\n",
       "                       -6.1035e-04,  2.4567e-03],\n",
       "                      [-3.2654e-03,  4.0283e-03,  4.7913e-03,  ...,  1.7242e-03,\n",
       "                       -7.5989e-03,  4.6387e-03],\n",
       "                      [ 3.3264e-03,  5.6152e-03, -1.2589e-04,  ...,  1.5488e-03,\n",
       "                       -1.4954e-03, -6.7444e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.2.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0003,  0.0041, -0.0057,  ..., -0.0002, -0.0008, -0.0024],\n",
       "                      [-0.0024, -0.0107, -0.0028,  ...,  0.0020, -0.0031, -0.0032],\n",
       "                      [-0.0048,  0.0008, -0.0016,  ..., -0.0005, -0.0010, -0.0030],\n",
       "                      ...,\n",
       "                      [ 0.0050, -0.0023, -0.0036,  ..., -0.0018, -0.0015, -0.0033],\n",
       "                      [ 0.0033, -0.0071, -0.0013,  ...,  0.0017, -0.0041,  0.0060],\n",
       "                      [ 0.0025,  0.0053,  0.0050,  ...,  0.0045, -0.0013, -0.0005]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.2.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0013, -0.0003, -0.0004,  ...,  0.0044,  0.0024,  0.0034],\n",
       "                      [-0.0052, -0.0074, -0.0010,  ..., -0.0008, -0.0028,  0.0051],\n",
       "                      [-0.0002, -0.0021, -0.0035,  ..., -0.0020, -0.0009, -0.0015],\n",
       "                      ...,\n",
       "                      [ 0.0008,  0.0041, -0.0005,  ...,  0.0020,  0.0026,  0.0036],\n",
       "                      [-0.0025, -0.0014,  0.0006,  ...,  0.0019, -0.0029,  0.0035],\n",
       "                      [-0.0022, -0.0061,  0.0021,  ..., -0.0006, -0.0001, -0.0007]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.2.input_layernorm.weight',\n",
       "              tensor([0.8594, 0.6992, 0.6523,  ..., 0.8750, 0.7617, 0.8359],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.2.post_attention_layernorm.weight',\n",
       "              tensor([0.8359, 0.8203, 0.8516,  ..., 0.8203, 0.8438, 0.8281],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.3.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0012, -0.0006,  0.0030,  ...,  0.0008, -0.0019, -0.0030],\n",
       "                      [-0.0047, -0.0006,  0.0004,  ...,  0.0013,  0.0003, -0.0006],\n",
       "                      [-0.0042,  0.0005,  0.0058,  ...,  0.0041, -0.0001, -0.0035],\n",
       "                      ...,\n",
       "                      [ 0.0005,  0.0020,  0.0030,  ..., -0.0009,  0.0056,  0.0029],\n",
       "                      [ 0.0042, -0.0075, -0.0041,  ...,  0.0036, -0.0044, -0.0041],\n",
       "                      [ 0.0112, -0.0023,  0.0125,  ..., -0.0046, -0.0023, -0.0022]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.3.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0028, -0.0030, -0.0019,  ..., -0.0061,  0.0031,  0.0009],\n",
       "                      [-0.0069, -0.0032,  0.0012,  ..., -0.0018,  0.0035, -0.0094],\n",
       "                      [-0.0020,  0.0027,  0.0143,  ...,  0.0042, -0.0014, -0.0127],\n",
       "                      ...,\n",
       "                      [-0.0187,  0.0004, -0.0035,  ...,  0.0096,  0.0029, -0.0024],\n",
       "                      [ 0.0042,  0.0017, -0.0002,  ..., -0.0025,  0.0039, -0.0066],\n",
       "                      [-0.0031,  0.0013, -0.0042,  ..., -0.0003,  0.0002,  0.0009]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.3.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0022,  0.0038, -0.0055,  ..., -0.0001,  0.0099,  0.0034],\n",
       "                      [-0.0023, -0.0002,  0.0027,  ..., -0.0012, -0.0015, -0.0035],\n",
       "                      [ 0.0008, -0.0015,  0.0001,  ...,  0.0020,  0.0009, -0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0015, -0.0002,  0.0033,  ...,  0.0002, -0.0018,  0.0004],\n",
       "                      [-0.0005, -0.0039,  0.0034,  ...,  0.0005,  0.0019, -0.0006],\n",
       "                      [ 0.0004,  0.0013,  0.0034,  ..., -0.0009,  0.0021,  0.0036]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.3.self_attn.o_proj.weight',\n",
       "              tensor([[-4.8828e-03,  4.0894e-03, -1.6785e-04,  ..., -1.8997e-03,\n",
       "                       -1.5640e-03,  3.6621e-03],\n",
       "                      [ 5.2185e-03,  6.2256e-03,  1.2970e-03,  ..., -3.4027e-03,\n",
       "                       -3.7537e-03,  3.6469e-03],\n",
       "                      [ 2.0599e-03,  9.8419e-04, -1.6880e-04,  ...,  5.9814e-03,\n",
       "                        3.6469e-03, -9.2983e-06],\n",
       "                      ...,\n",
       "                      [ 1.6022e-03, -3.4027e-03, -1.4191e-03,  ...,  2.9907e-03,\n",
       "                       -3.3188e-04, -5.5847e-03],\n",
       "                      [ 2.4567e-03,  3.9978e-03, -6.1989e-05,  ..., -1.0681e-03,\n",
       "                        3.3875e-03, -7.4005e-04],\n",
       "                      [ 7.7209e-03,  5.4626e-03, -2.2583e-03,  ...,  2.0752e-03,\n",
       "                       -1.7014e-03,  1.2894e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.3.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0024,  0.0018, -0.0045,  ...,  0.0014,  0.0006,  0.0030],\n",
       "                      [ 0.0020,  0.0049,  0.0020,  ...,  0.0070,  0.0023,  0.0016],\n",
       "                      [-0.0008,  0.0044,  0.0050,  ..., -0.0053,  0.0025,  0.0022],\n",
       "                      ...,\n",
       "                      [-0.0017,  0.0010,  0.0023,  ...,  0.0016,  0.0059,  0.0006],\n",
       "                      [-0.0005, -0.0013, -0.0013,  ..., -0.0008,  0.0027, -0.0004],\n",
       "                      [-0.0037,  0.0037, -0.0015,  ..., -0.0010, -0.0016, -0.0022]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.3.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0060,  0.0026, -0.0014,  ...,  0.0010, -0.0004,  0.0025],\n",
       "                      [ 0.0014, -0.0043,  0.0006,  ...,  0.0050,  0.0008,  0.0011],\n",
       "                      [ 0.0040,  0.0017, -0.0012,  ...,  0.0057, -0.0033,  0.0005],\n",
       "                      ...,\n",
       "                      [ 0.0023,  0.0039, -0.0012,  ...,  0.0028, -0.0032,  0.0016],\n",
       "                      [ 0.0041,  0.0038, -0.0016,  ...,  0.0039,  0.0029,  0.0042],\n",
       "                      [-0.0035, -0.0032, -0.0002,  ..., -0.0022,  0.0020, -0.0004]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.3.mlp.down_proj.weight',\n",
       "              tensor([[ 1.2360e-03,  4.7913e-03,  1.9684e-03,  ..., -2.1515e-03,\n",
       "                       -4.0283e-03, -3.5553e-03],\n",
       "                      [ 2.3651e-03,  2.0447e-03,  6.9275e-03,  ...,  1.4572e-03,\n",
       "                       -9.1171e-04,  1.7319e-03],\n",
       "                      [ 2.4414e-04,  1.8463e-03,  2.8076e-03,  ..., -1.2970e-04,\n",
       "                       -1.7395e-03, -4.8523e-03],\n",
       "                      ...,\n",
       "                      [-1.8978e-04,  4.1809e-03, -5.7068e-03,  ...,  4.9744e-03,\n",
       "                        6.9427e-04, -4.4861e-03],\n",
       "                      [ 3.1281e-03, -3.6049e-04,  3.5248e-03,  ..., -7.8201e-05,\n",
       "                        1.5106e-03, -1.1215e-03],\n",
       "                      [ 2.3346e-03, -8.4686e-04, -2.8076e-03,  ..., -7.7057e-04,\n",
       "                       -3.1586e-03,  1.0147e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.3.input_layernorm.weight',\n",
       "              tensor([0.8945, 0.6602, 0.6016,  ..., 0.8984, 0.6992, 0.7148],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.3.post_attention_layernorm.weight',\n",
       "              tensor([0.9883, 1.0000, 1.0234,  ..., 0.9922, 0.9844, 0.9961],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.4.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0019,  0.0026,  0.0081,  ...,  0.0030,  0.0015,  0.0037],\n",
       "                      [ 0.0002, -0.0006,  0.0010,  ..., -0.0010, -0.0061, -0.0041],\n",
       "                      [-0.0032,  0.0018,  0.0017,  ..., -0.0006, -0.0048, -0.0031],\n",
       "                      ...,\n",
       "                      [ 0.0017,  0.0017, -0.0027,  ..., -0.0004, -0.0015, -0.0007],\n",
       "                      [ 0.0017, -0.0016, -0.0015,  ..., -0.0025, -0.0054, -0.0015],\n",
       "                      [ 0.0006, -0.0046,  0.0005,  ...,  0.0055, -0.0019, -0.0009]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.4.self_attn.k_proj.weight',\n",
       "              tensor([[-2.2888e-03,  4.3945e-03,  4.9744e-03,  ...,  3.4027e-03,\n",
       "                        1.8234e-03,  4.9744e-03],\n",
       "                      [ 3.3875e-03, -6.5918e-03, -1.6403e-04,  ...,  7.5531e-04,\n",
       "                        2.7466e-03, -3.1433e-03],\n",
       "                      [ 5.2185e-03, -1.0223e-03,  1.9531e-03,  ...,  2.8992e-04,\n",
       "                       -7.8201e-04,  6.7444e-03],\n",
       "                      ...,\n",
       "                      [-1.4725e-03,  3.9577e-05,  1.4954e-03,  ...,  4.0245e-04,\n",
       "                       -1.8787e-04, -1.4663e-05],\n",
       "                      [ 5.9204e-03, -7.2021e-03,  8.6670e-03,  ..., -7.2098e-04,\n",
       "                        1.5488e-03, -7.3853e-03],\n",
       "                      [-9.5825e-03, -2.9907e-03, -9.2163e-03,  ..., -7.2632e-03,\n",
       "                       -9.2163e-03,  4.2725e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.4.self_attn.v_proj.weight',\n",
       "              tensor([[-1.1139e-03, -7.7438e-04, -1.0834e-03,  ...,  8.7357e-04,\n",
       "                       -5.5313e-05, -6.3705e-04],\n",
       "                      [ 4.3640e-03,  3.6621e-03, -2.4438e-05,  ...,  5.7220e-04,\n",
       "                        2.1648e-04, -7.4863e-05],\n",
       "                      [ 1.4954e-03, -1.2207e-03, -1.9379e-03,  ...,  3.4714e-04,\n",
       "                       -3.4790e-03, -1.1978e-03],\n",
       "                      ...,\n",
       "                      [-1.8387e-03, -3.6163e-03,  1.8005e-03,  ...,  2.1973e-03,\n",
       "                        3.0136e-04,  2.2583e-03],\n",
       "                      [ 2.2430e-03,  1.8768e-03, -1.7395e-03,  ..., -2.1362e-03,\n",
       "                        2.7618e-03, -3.7689e-03],\n",
       "                      [-2.3651e-04,  9.4223e-04, -7.0572e-05,  ...,  8.5449e-04,\n",
       "                        2.3499e-03, -2.1820e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.4.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.6861e-03, -1.0223e-03,  2.5330e-03,  ...,  4.3640e-03,\n",
       "                       -9.6512e-04,  1.0443e-04],\n",
       "                      [ 1.9531e-03, -2.1744e-04, -5.1575e-03,  ...,  5.3787e-04,\n",
       "                       -4.6082e-03,  6.9141e-06],\n",
       "                      [-8.3542e-04, -5.1880e-03,  2.6855e-03,  ...,  3.3417e-03,\n",
       "                        5.9509e-04,  8.2779e-04],\n",
       "                      ...,\n",
       "                      [ 1.1597e-03, -1.4343e-03, -2.5330e-03,  ..., -2.8992e-03,\n",
       "                        4.0283e-03, -4.0627e-04],\n",
       "                      [-4.0588e-03,  2.2430e-03,  5.8594e-03,  ..., -1.8539e-03,\n",
       "                       -2.3041e-03, -2.0905e-03],\n",
       "                      [ 3.6926e-03,  1.9989e-03, -1.5945e-03,  ..., -1.7471e-03,\n",
       "                        3.1586e-03, -2.3041e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.4.mlp.gate_proj.weight',\n",
       "              tensor([[-2.1362e-04,  2.5177e-03,  2.9755e-03,  ..., -7.5531e-04,\n",
       "                       -1.7090e-03,  3.4790e-03],\n",
       "                      [ 3.6621e-03,  5.8889e-05, -1.3275e-03,  ..., -4.1580e-04,\n",
       "                        4.4441e-04, -4.2725e-03],\n",
       "                      [-2.7466e-03, -4.3297e-04,  5.9814e-03,  ..., -8.6594e-04,\n",
       "                       -3.6316e-03,  8.2397e-04],\n",
       "                      ...,\n",
       "                      [-4.6387e-03, -3.3264e-03,  3.9978e-03,  ...,  6.3782e-03,\n",
       "                        1.3580e-03,  4.5471e-03],\n",
       "                      [-2.8229e-03,  2.9297e-03, -5.2261e-04,  ...,  2.7618e-03,\n",
       "                        3.0823e-03,  1.2970e-03],\n",
       "                      [ 5.9891e-04,  1.8082e-03, -1.2817e-03,  ..., -1.5640e-03,\n",
       "                        3.2806e-03, -1.3733e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.4.mlp.up_proj.weight',\n",
       "              tensor([[-3.5858e-03,  2.2736e-03, -3.1586e-03,  ..., -2.8801e-04,\n",
       "                       -9.2773e-03, -4.4250e-03],\n",
       "                      [ 3.6011e-03,  5.5237e-03, -1.8311e-03,  ...,  9.9182e-04,\n",
       "                        9.5844e-05, -1.4877e-03],\n",
       "                      [ 1.2436e-03,  2.4109e-03,  2.5482e-03,  ...,  1.8616e-03,\n",
       "                       -2.0447e-03, -4.3030e-03],\n",
       "                      ...,\n",
       "                      [ 2.3956e-03, -1.0452e-03, -1.1520e-03,  ..., -2.4414e-03,\n",
       "                        2.0752e-03,  2.1973e-03],\n",
       "                      [ 2.8381e-03,  1.8005e-03, -1.2741e-03,  ...,  1.5717e-03,\n",
       "                       -1.8692e-03,  4.2725e-03],\n",
       "                      [ 2.2583e-03, -2.7161e-03,  8.6975e-04,  ..., -3.3112e-03,\n",
       "                        3.5858e-03, -9.5749e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.4.mlp.down_proj.weight',\n",
       "              tensor([[-2.1515e-03,  3.9062e-03,  2.7161e-03,  ...,  1.3580e-03,\n",
       "                        4.1008e-04, -8.7738e-04],\n",
       "                      [-5.0354e-04,  8.4877e-05, -3.7575e-04,  ...,  6.5231e-04,\n",
       "                        1.9531e-03, -2.1553e-04],\n",
       "                      [-5.9204e-03, -1.1139e-03,  6.3782e-03,  ...,  1.3275e-03,\n",
       "                       -1.2589e-03, -2.6703e-03],\n",
       "                      ...,\n",
       "                      [-3.6774e-03,  4.5013e-04, -9.3079e-04,  ..., -9.8419e-04,\n",
       "                       -4.6921e-04, -1.1826e-03],\n",
       "                      [-7.8125e-03, -3.5706e-03,  2.2888e-03,  ...,  6.3324e-04,\n",
       "                       -4.9744e-03, -1.2016e-04],\n",
       "                      [-2.8839e-03,  1.0452e-03, -1.8692e-03,  ...,  3.2501e-03,\n",
       "                        2.7313e-03,  1.6556e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.4.input_layernorm.weight',\n",
       "              tensor([1.1484, 0.9023, 0.9023,  ..., 1.1641, 0.9570, 0.8398],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.4.post_attention_layernorm.weight',\n",
       "              tensor([1.0469, 1.0703, 1.0859,  ..., 1.0469, 1.0625, 1.0703],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.5.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0004, -0.0052, -0.0023,  ...,  0.0002,  0.0011,  0.0006],\n",
       "                      [ 0.0031,  0.0012, -0.0030,  ..., -0.0035, -0.0003, -0.0015],\n",
       "                      [-0.0022,  0.0051,  0.0029,  ..., -0.0038,  0.0034, -0.0004],\n",
       "                      ...,\n",
       "                      [ 0.0029,  0.0066, -0.0049,  ..., -0.0053,  0.0036, -0.0016],\n",
       "                      [ 0.0024,  0.0022, -0.0023,  ...,  0.0054, -0.0030,  0.0016],\n",
       "                      [-0.0007,  0.0005, -0.0053,  ...,  0.0037,  0.0014, -0.0017]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.5.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0033, -0.0015,  0.0032,  ...,  0.0050, -0.0025, -0.0014],\n",
       "                      [ 0.0039,  0.0036,  0.0001,  ...,  0.0013,  0.0028, -0.0062],\n",
       "                      [-0.0031, -0.0006,  0.0029,  ...,  0.0058, -0.0091, -0.0082],\n",
       "                      ...,\n",
       "                      [ 0.0024, -0.0087,  0.0081,  ..., -0.0050,  0.0032, -0.0053],\n",
       "                      [ 0.0030,  0.0085, -0.0078,  ...,  0.0009, -0.0012,  0.0062],\n",
       "                      [ 0.0033,  0.0076, -0.0032,  ...,  0.0041, -0.0044,  0.0032]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.5.self_attn.v_proj.weight',\n",
       "              tensor([[-1.7090e-03, -5.1498e-04, -2.2888e-03,  ...,  2.5635e-03,\n",
       "                       -5.9814e-03, -3.8338e-04],\n",
       "                      [ 2.0599e-03, -3.1738e-03,  1.8997e-03,  ..., -5.3406e-04,\n",
       "                       -8.1062e-06, -2.1362e-03],\n",
       "                      [ 5.2185e-03,  2.5635e-03, -1.2493e-04,  ...,  3.5858e-04,\n",
       "                        1.0147e-03, -1.4801e-03],\n",
       "                      ...,\n",
       "                      [ 1.8463e-03, -4.1809e-03,  1.4648e-03,  ...,  1.7548e-03,\n",
       "                       -1.3275e-03,  3.4714e-04],\n",
       "                      [-3.0060e-03,  2.3365e-04,  1.7166e-03,  ..., -1.1673e-03,\n",
       "                        2.0447e-03,  2.4109e-03],\n",
       "                      [ 2.5940e-03, -3.1433e-03, -4.5013e-04,  ..., -1.1520e-03,\n",
       "                        1.1597e-03,  5.7220e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.5.self_attn.o_proj.weight',\n",
       "              tensor([[ 2.9449e-03,  1.0452e-03,  9.3460e-05,  ...,  3.2043e-04,\n",
       "                        2.9907e-03,  3.7842e-03],\n",
       "                      [ 1.2970e-03, -6.1035e-04,  1.9379e-03,  ...,  1.9684e-03,\n",
       "                       -3.4943e-03,  3.4943e-03],\n",
       "                      [-2.5940e-03, -8.6594e-04,  2.7618e-03,  ..., -3.0518e-03,\n",
       "                        1.9932e-04,  3.0823e-03],\n",
       "                      ...,\n",
       "                      [ 4.3488e-04, -1.7395e-03, -3.1281e-03,  ..., -2.1057e-03,\n",
       "                       -3.3569e-03, -4.6387e-03],\n",
       "                      [ 2.5787e-03,  8.7738e-04, -3.1662e-04,  ...,  1.5793e-03,\n",
       "                        1.4954e-03,  2.5787e-03],\n",
       "                      [ 3.9978e-03,  6.3782e-03,  5.2490e-03,  ..., -2.5787e-03,\n",
       "                        1.8616e-03,  4.2419e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.5.mlp.gate_proj.weight',\n",
       "              tensor([[ 9.7656e-04, -4.7607e-03, -2.5635e-03,  ..., -5.6152e-03,\n",
       "                        3.4943e-03, -1.1139e-03],\n",
       "                      [ 2.4109e-03,  4.3030e-03, -4.5586e-04,  ...,  1.9989e-03,\n",
       "                       -4.1580e-04,  3.4943e-03],\n",
       "                      [-8.4839e-03, -2.8381e-03,  1.0132e-02,  ...,  1.1826e-03,\n",
       "                       -1.0757e-03,  4.3335e-03],\n",
       "                      ...,\n",
       "                      [-1.6403e-03,  1.6403e-03,  5.8651e-05,  ...,  5.8289e-03,\n",
       "                        2.5177e-04, -1.6556e-03],\n",
       "                      [-5.9891e-04, -1.3809e-03,  3.0975e-03,  ..., -2.2888e-03,\n",
       "                        9.6130e-04,  4.3945e-03],\n",
       "                      [-3.5095e-03, -6.1340e-03,  1.4114e-04,  ...,  7.6294e-05,\n",
       "                       -1.9073e-03, -3.5400e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.5.mlp.up_proj.weight',\n",
       "              tensor([[-1.0529e-03, -2.6093e-03,  4.7874e-04,  ..., -3.8528e-04,\n",
       "                        4.7913e-03,  2.5177e-04],\n",
       "                      [ 1.5717e-03,  6.1951e-03, -3.6774e-03,  ..., -1.4420e-03,\n",
       "                       -2.3651e-03,  1.6785e-03],\n",
       "                      [-7.4387e-04, -2.2278e-03, -5.5695e-04,  ...,  2.3346e-03,\n",
       "                        5.6744e-05,  1.9989e-03],\n",
       "                      ...,\n",
       "                      [ 1.8692e-03,  8.8120e-04,  2.6550e-03,  ...,  5.7373e-03,\n",
       "                       -7.5150e-04,  1.1444e-03],\n",
       "                      [-1.0910e-03,  5.6763e-03,  6.3782e-03,  ..., -4.3030e-03,\n",
       "                        1.1444e-03, -2.2430e-03],\n",
       "                      [ 2.1667e-03, -3.5667e-04,  2.5177e-03,  ..., -1.9073e-03,\n",
       "                       -1.6937e-03, -9.0122e-05]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.5.mlp.down_proj.weight',\n",
       "              tensor([[ 8.0872e-04, -9.8419e-04,  1.1673e-03,  ..., -2.6398e-03,\n",
       "                       -1.3580e-03, -4.3030e-03],\n",
       "                      [ 2.3651e-03, -3.9673e-04, -3.9673e-04,  ..., -1.9226e-03,\n",
       "                        8.7891e-03,  4.0817e-04],\n",
       "                      [-3.2959e-03, -2.2430e-03,  4.2419e-03,  ...,  1.9302e-03,\n",
       "                        4.6997e-03,  1.4267e-03],\n",
       "                      ...,\n",
       "                      [ 3.9062e-03, -3.1891e-03, -4.1962e-04,  ..., -1.5335e-03,\n",
       "                       -3.7231e-03,  2.6093e-03],\n",
       "                      [ 3.2349e-03,  3.4485e-03,  2.3346e-03,  ..., -1.9226e-03,\n",
       "                        4.7112e-04,  2.1515e-03],\n",
       "                      [ 7.5912e-04, -3.2043e-03, -4.1485e-05,  ...,  2.1515e-03,\n",
       "                       -2.6093e-03, -3.2654e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.5.input_layernorm.weight',\n",
       "              tensor([1.5078, 1.1094, 1.1094,  ..., 1.3672, 1.2109, 1.2031],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.5.post_attention_layernorm.weight',\n",
       "              tensor([1.1875, 1.2422, 1.2344,  ..., 1.2188, 1.2109, 1.2188],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.6.self_attn.q_proj.weight',\n",
       "              tensor([[-5.7602e-04,  8.2397e-03,  1.7166e-04,  ..., -6.6223e-03,\n",
       "                       -3.7537e-03, -4.4556e-03],\n",
       "                      [-4.2114e-03,  1.9073e-03,  6.0120e-03,  ...,  2.0504e-05,\n",
       "                       -1.0681e-03,  1.0529e-03],\n",
       "                      [ 3.8910e-03, -7.1106e-03, -5.0049e-03,  ...,  2.1515e-03,\n",
       "                        2.7161e-03,  2.2583e-03],\n",
       "                      ...,\n",
       "                      [ 3.6926e-03,  5.0354e-04, -2.2411e-04,  ...,  3.4637e-03,\n",
       "                       -2.4414e-03,  2.1973e-03],\n",
       "                      [-6.4087e-04, -3.1891e-03,  1.1444e-03,  ...,  6.5613e-03,\n",
       "                       -2.0294e-03, -4.7607e-03],\n",
       "                      [ 3.7689e-03,  3.5858e-03, -7.2632e-03,  ..., -8.1787e-03,\n",
       "                        5.0354e-03,  6.8970e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.6.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.2207e-03,  8.5068e-04,  2.8687e-03,  ..., -4.2114e-03,\n",
       "                       -1.7471e-03,  3.9978e-03],\n",
       "                      [ 1.0010e-02, -7.7438e-04,  2.0599e-03,  ...,  8.0490e-04,\n",
       "                       -2.4796e-04, -8.9264e-04],\n",
       "                      [-2.7466e-03, -4.7302e-04,  1.4114e-03,  ..., -8.0490e-04,\n",
       "                       -1.0834e-03, -4.9174e-07],\n",
       "                      ...,\n",
       "                      [ 1.6479e-03,  4.5166e-03, -3.7079e-03,  ...,  4.8523e-03,\n",
       "                        4.6387e-03, -3.8757e-03],\n",
       "                      [ 2.4719e-03, -1.0864e-02, -3.8147e-03,  ..., -1.5259e-03,\n",
       "                       -2.8839e-03,  2.8687e-03],\n",
       "                      [ 4.2725e-03, -3.1128e-03, -3.8910e-03,  ...,  1.3367e-02,\n",
       "                       -1.0376e-03, -1.2756e-02]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.6.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0005, -0.0041,  0.0010,  ...,  0.0018,  0.0048, -0.0003],\n",
       "                      [ 0.0011, -0.0012, -0.0010,  ...,  0.0028,  0.0003, -0.0025],\n",
       "                      [-0.0023,  0.0015, -0.0019,  ...,  0.0004,  0.0034,  0.0006],\n",
       "                      ...,\n",
       "                      [ 0.0015, -0.0001,  0.0052,  ...,  0.0021, -0.0046,  0.0004],\n",
       "                      [-0.0035,  0.0053, -0.0024,  ...,  0.0007, -0.0027,  0.0001],\n",
       "                      [ 0.0012, -0.0046, -0.0004,  ...,  0.0017, -0.0032, -0.0016]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.6.self_attn.o_proj.weight',\n",
       "              tensor([[-4.7302e-03, -1.1749e-03, -1.2436e-03,  ..., -1.0252e-04,\n",
       "                       -6.2866e-03,  4.9591e-05],\n",
       "                      [ 3.4943e-03, -2.7161e-03,  1.6327e-03,  ..., -2.0752e-03,\n",
       "                       -3.1662e-04,  2.4719e-03],\n",
       "                      [-1.6327e-03, -2.5024e-03, -2.2430e-03,  ...,  1.5030e-03,\n",
       "                        2.2769e-05, -9.2697e-04],\n",
       "                      ...,\n",
       "                      [-1.0757e-03,  2.1210e-03, -5.8289e-03,  ...,  8.2779e-04,\n",
       "                        4.0894e-03,  3.9864e-04],\n",
       "                      [ 5.9509e-04,  3.6049e-04, -2.6245e-03,  ..., -6.5918e-03,\n",
       "                       -9.2316e-04, -2.3193e-03],\n",
       "                      [-1.5869e-03,  7.7515e-03,  5.1880e-04,  ..., -1.0605e-03,\n",
       "                       -1.1978e-03, -1.8997e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.6.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0003,  0.0030,  0.0005,  ..., -0.0051,  0.0021, -0.0007],\n",
       "                      [-0.0015, -0.0002, -0.0006,  ..., -0.0054, -0.0055,  0.0001],\n",
       "                      [ 0.0015,  0.0004, -0.0041,  ...,  0.0069,  0.0005,  0.0027],\n",
       "                      ...,\n",
       "                      [-0.0012, -0.0042, -0.0029,  ..., -0.0043,  0.0063, -0.0012],\n",
       "                      [-0.0043, -0.0033,  0.0054,  ..., -0.0016,  0.0052,  0.0002],\n",
       "                      [-0.0015, -0.0033,  0.0057,  ...,  0.0003, -0.0042, -0.0016]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.6.mlp.up_proj.weight',\n",
       "              tensor([[-5.8174e-05, -2.7618e-03,  4.8218e-03,  ...,  3.3617e-05,\n",
       "                        6.8359e-03, -7.3242e-04],\n",
       "                      [-1.3275e-03,  2.4414e-03, -4.3945e-03,  ..., -1.7700e-03,\n",
       "                       -4.2915e-04,  7.6294e-04],\n",
       "                      [ 2.6245e-03, -9.3079e-04,  6.5002e-03,  ...,  6.1417e-04,\n",
       "                        7.4768e-04,  9.1171e-04],\n",
       "                      ...,\n",
       "                      [ 9.7156e-06, -2.3651e-03,  2.8687e-03,  ..., -9.5367e-04,\n",
       "                       -2.0447e-03, -1.4191e-03],\n",
       "                      [-2.2888e-03, -1.0605e-03,  2.8992e-03,  ..., -2.5330e-03,\n",
       "                        1.9684e-03, -2.3651e-03],\n",
       "                      [-1.7395e-03,  3.5858e-03, -6.9275e-03,  ..., -3.4027e-03,\n",
       "                        1.0605e-03,  1.7624e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.6.mlp.down_proj.weight',\n",
       "              tensor([[ 2.1362e-03,  1.0834e-03,  2.2736e-03,  ..., -1.9073e-03,\n",
       "                       -2.4567e-03, -4.9973e-04],\n",
       "                      [-3.1662e-04,  2.1820e-03,  5.4932e-03,  ...,  1.7929e-03,\n",
       "                       -1.5945e-03,  4.0588e-03],\n",
       "                      [ 3.8147e-03, -9.3079e-04,  4.1504e-03,  ...,  3.3722e-03,\n",
       "                        2.0905e-03,  1.8311e-03],\n",
       "                      ...,\n",
       "                      [-2.0695e-04, -5.5542e-03,  3.1471e-04,  ...,  4.4250e-03,\n",
       "                       -2.0447e-03, -4.1809e-03],\n",
       "                      [ 4.7302e-03,  2.2125e-03,  5.1575e-03,  ..., -1.7166e-03,\n",
       "                       -7.5531e-04,  5.0659e-03],\n",
       "                      [-2.5940e-03,  4.8161e-05, -7.4005e-04,  ...,  4.8637e-05,\n",
       "                        5.9605e-05, -7.1335e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.6.input_layernorm.weight',\n",
       "              tensor([1.2344, 1.0703, 1.0391,  ..., 1.1719, 1.1641, 1.0781],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.6.post_attention_layernorm.weight',\n",
       "              tensor([1.2656, 1.3438, 1.3516,  ..., 1.2812, 1.3047, 1.2969],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.7.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0020, -0.0003, -0.0014,  ...,  0.0006, -0.0022,  0.0011],\n",
       "                      [-0.0020, -0.0050,  0.0024,  ..., -0.0011,  0.0021, -0.0055],\n",
       "                      [ 0.0021,  0.0037,  0.0033,  ...,  0.0012,  0.0025, -0.0032],\n",
       "                      ...,\n",
       "                      [ 0.0120, -0.0064,  0.0026,  ...,  0.0018,  0.0008, -0.0079],\n",
       "                      [-0.0127,  0.0015,  0.0048,  ..., -0.0032, -0.0060, -0.0026],\n",
       "                      [ 0.0040, -0.0110, -0.0010,  ..., -0.0015,  0.0118, -0.0066]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.7.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0010, -0.0011,  0.0028,  ..., -0.0026, -0.0033, -0.0027],\n",
       "                      [ 0.0029, -0.0007, -0.0025,  ..., -0.0008,  0.0037,  0.0015],\n",
       "                      [ 0.0012, -0.0038, -0.0036,  ..., -0.0001,  0.0030,  0.0006],\n",
       "                      ...,\n",
       "                      [-0.0056, -0.0057, -0.0006,  ..., -0.0018,  0.0056, -0.0077],\n",
       "                      [ 0.0115, -0.0001, -0.0058,  ..., -0.0098, -0.0052,  0.0075],\n",
       "                      [-0.0107,  0.0005,  0.0081,  ...,  0.0132,  0.0121,  0.0116]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.7.self_attn.v_proj.weight',\n",
       "              tensor([[-1.9836e-03,  2.7924e-03, -8.2016e-05,  ..., -1.3885e-03,\n",
       "                       -9.9945e-04, -2.0294e-03],\n",
       "                      [ 7.7820e-04,  6.3324e-04,  2.8610e-04,  ..., -2.6512e-04,\n",
       "                       -1.4572e-03,  5.0354e-04],\n",
       "                      [-3.8528e-04,  2.8229e-04,  1.5717e-03,  ...,  4.2725e-04,\n",
       "                       -4.0054e-04, -1.8768e-03],\n",
       "                      ...,\n",
       "                      [ 1.7357e-04, -8.3008e-03,  1.9379e-03,  ...,  4.4861e-03,\n",
       "                        4.5776e-03,  1.6251e-03],\n",
       "                      [ 1.5717e-03,  5.2795e-03, -8.4305e-04,  ...,  1.7405e-05,\n",
       "                       -3.6240e-04, -3.1738e-03],\n",
       "                      [ 3.6163e-03, -1.2741e-03, -9.9182e-04,  ...,  8.7357e-04,\n",
       "                        1.5717e-03,  1.3504e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.7.self_attn.o_proj.weight',\n",
       "              tensor([[-4.5395e-04,  4.1962e-04, -2.0218e-04,  ...,  1.6022e-04,\n",
       "                        4.3335e-03, -2.0142e-03],\n",
       "                      [ 4.5166e-03, -9.5749e-04, -6.4850e-04,  ..., -3.5400e-03,\n",
       "                        1.3428e-03,  2.5024e-03],\n",
       "                      [ 1.3885e-03,  1.1292e-03,  2.4109e-03,  ..., -2.9602e-03,\n",
       "                        1.4343e-03, -1.8311e-04],\n",
       "                      ...,\n",
       "                      [-1.3809e-03,  1.4420e-03,  4.2439e-05,  ...,  7.2098e-04,\n",
       "                       -3.2349e-03, -7.9346e-04],\n",
       "                      [-1.8234e-03, -5.0354e-03,  5.4169e-04,  ...,  3.5706e-03,\n",
       "                       -5.4932e-03, -1.1215e-03],\n",
       "                      [-4.2725e-04,  3.4332e-04, -2.8992e-03,  ...,  1.8387e-03,\n",
       "                       -1.5335e-03, -6.2561e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.7.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0021, -0.0040, -0.0080,  ...,  0.0013,  0.0007, -0.0018],\n",
       "                      [-0.0006, -0.0044,  0.0008,  ..., -0.0034, -0.0039,  0.0018],\n",
       "                      [ 0.0048,  0.0043, -0.0114,  ...,  0.0026, -0.0003,  0.0036],\n",
       "                      ...,\n",
       "                      [ 0.0039, -0.0047,  0.0033,  ...,  0.0023,  0.0009,  0.0029],\n",
       "                      [ 0.0024, -0.0033,  0.0022,  ...,  0.0047, -0.0016, -0.0028],\n",
       "                      [ 0.0013, -0.0026, -0.0007,  ..., -0.0014, -0.0063, -0.0005]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.7.mlp.up_proj.weight',\n",
       "              tensor([[-0.0022,  0.0008, -0.0001,  ...,  0.0037, -0.0018, -0.0022],\n",
       "                      [ 0.0054,  0.0024,  0.0031,  ..., -0.0018,  0.0003, -0.0019],\n",
       "                      [-0.0032,  0.0076, -0.0006,  ...,  0.0024,  0.0027,  0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0053,  0.0002,  0.0048,  ...,  0.0002, -0.0067, -0.0014],\n",
       "                      [-0.0039,  0.0031, -0.0013,  ..., -0.0037, -0.0017, -0.0003],\n",
       "                      [ 0.0016,  0.0014, -0.0021,  ..., -0.0007,  0.0038,  0.0010]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.7.mlp.down_proj.weight',\n",
       "              tensor([[ 4.0054e-05,  4.3030e-03, -3.0823e-03,  ...,  5.1880e-03,\n",
       "                       -1.4648e-03,  4.8218e-03],\n",
       "                      [-1.4496e-03, -5.0354e-03, -1.7166e-03,  ...,  1.9150e-03,\n",
       "                       -5.3406e-04,  1.2589e-03],\n",
       "                      [ 8.7738e-04,  2.2583e-03, -8.3923e-05,  ...,  7.2632e-03,\n",
       "                        1.6174e-03, -1.4191e-03],\n",
       "                      ...,\n",
       "                      [ 5.7678e-03, -1.8082e-03, -2.4109e-03,  ...,  1.6785e-03,\n",
       "                       -1.1978e-03, -8.3923e-04],\n",
       "                      [ 4.1199e-03, -8.9645e-04, -9.9182e-04,  ..., -2.1973e-03,\n",
       "                       -9.4986e-04, -4.3640e-03],\n",
       "                      [-4.0283e-03,  7.5989e-03, -1.9684e-03,  ..., -6.8359e-03,\n",
       "                       -2.5177e-04, -1.3733e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.7.input_layernorm.weight',\n",
       "              tensor([1.3516, 1.2266, 1.2969,  ..., 1.2891, 1.3438, 1.2266],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.7.post_attention_layernorm.weight',\n",
       "              tensor([1.3750, 1.5312, 1.5000,  ..., 1.3672, 1.3984, 1.4062],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.8.self_attn.q_proj.weight',\n",
       "              tensor([[ 3.0975e-03, -1.8539e-03, -4.3640e-03,  ..., -9.7656e-04,\n",
       "                       -4.2114e-03,  2.6131e-04],\n",
       "                      [-8.8882e-04,  1.9302e-03,  1.4648e-03,  ...,  4.6387e-03,\n",
       "                        1.0681e-03,  2.1515e-03],\n",
       "                      [ 5.4550e-04, -2.4719e-03, -1.2741e-03,  ..., -2.0294e-03,\n",
       "                       -1.4038e-03, -1.7471e-03],\n",
       "                      ...,\n",
       "                      [-2.3193e-03,  2.2125e-03, -8.2779e-04,  ...,  1.5411e-03,\n",
       "                        2.6584e-05,  5.4932e-03],\n",
       "                      [ 3.7994e-03,  2.6245e-03,  6.1035e-04,  ..., -4.2725e-03,\n",
       "                       -1.7319e-03, -5.6152e-03],\n",
       "                      [-5.2643e-04,  1.0757e-03,  3.6926e-03,  ...,  3.3569e-03,\n",
       "                       -1.5869e-03,  8.0872e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.8.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.2360e-03,  8.9645e-04, -3.5048e-05,  ..., -3.5858e-03,\n",
       "                        3.8605e-03,  6.4468e-04],\n",
       "                      [ 4.0531e-05,  1.7624e-03, -3.1433e-03,  ...,  2.2602e-04,\n",
       "                       -3.9368e-03, -2.6398e-03],\n",
       "                      [-9.3079e-04, -5.7220e-04,  1.3657e-03,  ..., -1.8311e-03,\n",
       "                        2.3041e-03,  4.3030e-03],\n",
       "                      ...,\n",
       "                      [ 4.5471e-03,  2.4414e-03, -2.8839e-03,  ...,  2.5787e-03,\n",
       "                       -1.4191e-03,  4.2343e-04],\n",
       "                      [-9.2697e-04,  6.4697e-03,  4.1809e-03,  ...,  2.2125e-03,\n",
       "                       -2.8076e-03, -5.3711e-03],\n",
       "                      [-1.1063e-03, -1.8539e-03,  3.3875e-03,  ...,  2.9449e-03,\n",
       "                        2.4872e-03,  4.9133e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.8.self_attn.v_proj.weight',\n",
       "              tensor([[-2.8992e-03,  2.1973e-03,  2.0142e-03,  ..., -2.7771e-03,\n",
       "                       -5.8365e-04, -4.3030e-03],\n",
       "                      [ 3.0041e-05, -2.0905e-03, -2.0142e-03,  ..., -2.2736e-03,\n",
       "                       -1.5793e-03,  9.0408e-04],\n",
       "                      [-2.7771e-03, -4.9591e-04,  1.5068e-04,  ...,  1.2016e-04,\n",
       "                       -1.4496e-03, -4.4250e-04],\n",
       "                      ...,\n",
       "                      [ 1.7471e-03, -1.0910e-03,  8.4877e-05,  ...,  1.5259e-04,\n",
       "                        2.8419e-04, -3.2959e-03],\n",
       "                      [ 8.9264e-04,  3.0327e-04, -6.2561e-04,  ...,  6.5231e-04,\n",
       "                       -1.1444e-03,  2.5024e-03],\n",
       "                      [-1.4191e-03,  3.7842e-03,  1.5869e-03,  ..., -2.0752e-03,\n",
       "                       -9.7656e-04, -2.4261e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.8.self_attn.o_proj.weight',\n",
       "              tensor([[-5.1498e-04, -1.2817e-03, -2.9755e-03,  ...,  2.9297e-03,\n",
       "                       -6.7139e-04,  9.0027e-04],\n",
       "                      [-4.2419e-03, -2.6131e-04,  4.1962e-04,  ..., -1.3809e-03,\n",
       "                       -2.2430e-03,  7.4387e-04],\n",
       "                      [-2.2583e-03,  2.6512e-04, -2.0447e-03,  ..., -7.2861e-04,\n",
       "                       -5.4121e-05,  2.2278e-03],\n",
       "                      ...,\n",
       "                      [-1.3809e-03,  1.4305e-04, -3.7956e-04,  ...,  1.8311e-03,\n",
       "                        5.2261e-04,  1.4420e-03],\n",
       "                      [-2.4719e-03, -5.2261e-04,  2.1210e-03,  ..., -2.2888e-03,\n",
       "                        1.0252e-04, -4.1809e-03],\n",
       "                      [ 1.0061e-04, -2.2430e-03,  1.4954e-03,  ...,  4.9210e-04,\n",
       "                       -1.9836e-03,  2.3804e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.8.mlp.gate_proj.weight',\n",
       "              tensor([[-4.2534e-04,  2.9602e-03,  1.1749e-03,  ...,  4.8218e-03,\n",
       "                        1.9455e-03, -2.0599e-03],\n",
       "                      [ 4.7913e-03, -9.4986e-04,  2.3961e-05,  ..., -5.6763e-03,\n",
       "                       -3.7384e-03, -3.4637e-03],\n",
       "                      [-4.2419e-03, -9.8419e-04,  2.8229e-03,  ..., -4.1809e-03,\n",
       "                        1.8082e-03, -4.4861e-03],\n",
       "                      ...,\n",
       "                      [-3.9368e-03,  1.1520e-03, -4.3640e-03,  ..., -3.0212e-03,\n",
       "                        3.6812e-04, -2.9449e-03],\n",
       "                      [ 1.0920e-04, -1.8845e-03, -1.8005e-03,  ...,  5.4932e-04,\n",
       "                        7.6294e-05,  4.1504e-03],\n",
       "                      [ 2.3346e-03, -2.1057e-03,  1.8311e-03,  ..., -3.4180e-03,\n",
       "                       -4.4556e-03, -7.1106e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.8.mlp.up_proj.weight',\n",
       "              tensor([[ 2.7008e-03,  3.3569e-04,  3.0975e-03,  ..., -1.0986e-03,\n",
       "                        1.9073e-03, -1.4191e-03],\n",
       "                      [ 9.9182e-04, -5.1575e-03,  1.1520e-03,  ...,  4.6387e-03,\n",
       "                       -2.1973e-03, -1.5640e-03],\n",
       "                      [-6.4392e-03, -1.8005e-03,  5.3787e-04,  ...,  1.3428e-03,\n",
       "                        2.1362e-03,  9.0408e-04],\n",
       "                      ...,\n",
       "                      [-5.1880e-03,  2.0599e-03,  3.4790e-03,  ...,  4.5776e-03,\n",
       "                        4.5776e-03, -5.3024e-04],\n",
       "                      [-4.2419e-03,  2.4567e-03,  2.8534e-03,  ..., -3.3760e-04,\n",
       "                       -1.9989e-03,  1.1826e-03],\n",
       "                      [ 3.6001e-05,  3.1891e-03, -2.0447e-03,  ..., -2.3193e-03,\n",
       "                       -6.5994e-04,  8.1177e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.8.mlp.down_proj.weight',\n",
       "              tensor([[-1.3733e-03, -1.1444e-03, -1.8539e-03,  ..., -4.5776e-03,\n",
       "                       -2.0599e-03, -1.2054e-03],\n",
       "                      [-3.7384e-03, -8.6212e-04, -3.2959e-03,  ...,  2.9755e-03,\n",
       "                        9.2030e-05,  4.3640e-03],\n",
       "                      [ 5.6152e-03,  1.7014e-03,  1.5411e-03,  ...,  3.7994e-03,\n",
       "                       -2.9907e-03, -6.3477e-03],\n",
       "                      ...,\n",
       "                      [-1.6632e-03,  5.1270e-03, -6.5231e-04,  ...,  4.8523e-03,\n",
       "                       -3.4714e-04, -9.4604e-04],\n",
       "                      [-2.0142e-03,  4.7302e-03,  1.8234e-03,  ..., -1.2512e-03,\n",
       "                       -1.9150e-03, -2.2888e-03],\n",
       "                      [-2.5482e-03, -4.8447e-04,  2.5635e-03,  ..., -8.9645e-04,\n",
       "                       -1.8692e-03,  7.3242e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.8.input_layernorm.weight',\n",
       "              tensor([1.3516, 1.3047, 1.4688,  ..., 1.2266, 1.4375, 1.2500],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.8.post_attention_layernorm.weight',\n",
       "              tensor([1.4141, 1.6406, 1.6328,  ..., 1.3984, 1.4688, 1.5078],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.9.self_attn.q_proj.weight',\n",
       "              tensor([[ 9.0790e-04,  1.9455e-03,  7.4005e-04,  ..., -1.8921e-03,\n",
       "                       -5.0659e-03, -7.5684e-03],\n",
       "                      [-1.4191e-03, -4.2114e-03,  2.9449e-03,  ..., -3.8910e-03,\n",
       "                        4.0283e-03, -4.7607e-03],\n",
       "                      [-8.6308e-05,  5.4626e-03, -1.0071e-03,  ..., -1.2970e-03,\n",
       "                       -3.9673e-03,  8.6308e-05],\n",
       "                      ...,\n",
       "                      [-2.6321e-04, -7.2021e-03, -5.5847e-03,  ..., -1.6093e-05,\n",
       "                        9.3384e-03, -8.4229e-03],\n",
       "                      [ 4.3945e-03,  1.6357e-02, -3.9673e-03,  ...,  5.7373e-03,\n",
       "                        3.4027e-03, -3.4790e-03],\n",
       "                      [ 5.7678e-03, -3.1281e-03, -3.9978e-03,  ...,  1.8539e-03,\n",
       "                       -9.7046e-03,  7.2479e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.9.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0002,  0.0002,  0.0012,  ...,  0.0031, -0.0018,  0.0043],\n",
       "                      [-0.0039, -0.0003, -0.0008,  ..., -0.0018,  0.0046,  0.0003],\n",
       "                      [ 0.0003,  0.0026,  0.0022,  ...,  0.0003, -0.0008,  0.0023],\n",
       "                      ...,\n",
       "                      [-0.0052,  0.0054, -0.0019,  ...,  0.0058, -0.0014, -0.0064],\n",
       "                      [-0.0005,  0.0092,  0.0029,  ...,  0.0095,  0.0006, -0.0037],\n",
       "                      [-0.0019, -0.0084,  0.0042,  ...,  0.0038, -0.0140,  0.0017]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.9.self_attn.v_proj.weight',\n",
       "              tensor([[-4.9133e-03, -1.2817e-03,  1.6174e-03,  ..., -2.1667e-03,\n",
       "                       -2.4719e-03,  6.3705e-04],\n",
       "                      [-7.0953e-04,  9.8705e-05, -3.2806e-03,  ..., -2.3041e-03,\n",
       "                        1.7242e-03, -1.7548e-03],\n",
       "                      [ 2.4719e-03,  2.3804e-03, -2.5177e-03,  ...,  1.1597e-03,\n",
       "                       -1.8997e-03,  8.6975e-04],\n",
       "                      ...,\n",
       "                      [-9.7656e-04,  1.6689e-04,  1.9226e-03,  ..., -1.0490e-04,\n",
       "                        5.8746e-04, -2.7008e-03],\n",
       "                      [ 1.7548e-03, -3.1433e-03,  1.0147e-03,  ...,  1.1520e-03,\n",
       "                       -2.3346e-03,  2.1667e-03],\n",
       "                      [-2.7771e-03,  3.1891e-03,  8.0109e-04,  ...,  4.8637e-04,\n",
       "                        1.4591e-04,  5.2261e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.9.self_attn.o_proj.weight',\n",
       "              tensor([[-5.0964e-03,  1.3351e-03,  2.8687e-03,  ...,  8.0872e-04,\n",
       "                        9.0027e-04,  9.6893e-04],\n",
       "                      [-1.5869e-03, -8.1253e-04,  2.6245e-03,  ...,  7.5531e-04,\n",
       "                        1.0681e-03,  2.2125e-03],\n",
       "                      [ 4.3945e-03,  9.3079e-04,  3.2806e-03,  ...,  3.7537e-03,\n",
       "                       -7.1049e-05,  2.2583e-03],\n",
       "                      ...,\n",
       "                      [-3.1433e-03, -2.3956e-03, -2.5368e-04,  ...,  4.3945e-03,\n",
       "                       -8.9645e-04,  4.9210e-04],\n",
       "                      [-1.9379e-03,  1.0529e-03, -9.9182e-04,  ...,  2.6703e-03,\n",
       "                        6.1417e-04, -4.7913e-03],\n",
       "                      [ 3.8147e-04, -2.2583e-03,  2.5940e-03,  ..., -1.8768e-03,\n",
       "                       -2.6398e-03, -5.6076e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.9.mlp.gate_proj.weight',\n",
       "              tensor([[-2.5024e-03,  9.4986e-04,  3.5095e-03,  ...,  1.8005e-03,\n",
       "                       -5.9509e-04, -6.5231e-04],\n",
       "                      [-1.8234e-03,  4.1199e-04, -8.6212e-04,  ...,  4.4556e-03,\n",
       "                       -1.1292e-03, -4.8065e-04],\n",
       "                      [ 7.2956e-05,  3.1586e-03,  2.2736e-03,  ..., -3.2501e-03,\n",
       "                       -8.4839e-03,  1.9455e-03],\n",
       "                      ...,\n",
       "                      [ 8.7280e-03,  9.4604e-03, -1.2665e-03,  ..., -1.0376e-03,\n",
       "                        3.3112e-03, -4.1199e-03],\n",
       "                      [ 3.2196e-03, -1.1215e-03, -1.4954e-03,  ..., -5.0354e-03,\n",
       "                        3.4180e-03,  2.2888e-03],\n",
       "                      [-2.9206e-05,  4.6082e-03, -3.7231e-03,  ...,  2.0447e-03,\n",
       "                       -2.3193e-03,  5.5847e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.9.mlp.up_proj.weight',\n",
       "              tensor([[-0.0023, -0.0012,  0.0015,  ...,  0.0019,  0.0005,  0.0027],\n",
       "                      [-0.0061, -0.0045, -0.0006,  ...,  0.0053,  0.0006,  0.0012],\n",
       "                      [ 0.0025,  0.0008, -0.0035,  ...,  0.0012,  0.0011, -0.0020],\n",
       "                      ...,\n",
       "                      [-0.0005, -0.0114, -0.0007,  ..., -0.0043, -0.0023, -0.0039],\n",
       "                      [-0.0018,  0.0007,  0.0008,  ...,  0.0005, -0.0007,  0.0020],\n",
       "                      [ 0.0022, -0.0006, -0.0042,  ..., -0.0029, -0.0037, -0.0036]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.9.mlp.down_proj.weight',\n",
       "              tensor([[ 4.7302e-03, -1.5793e-03,  2.1820e-03,  ..., -3.9062e-03,\n",
       "                        1.9836e-03,  7.2021e-03],\n",
       "                      [-4.4632e-04, -5.5237e-03,  7.1716e-04,  ..., -8.4839e-03,\n",
       "                        3.1471e-05,  1.1215e-03],\n",
       "                      [-4.3030e-03, -3.0975e-03, -2.7618e-03,  ...,  4.6692e-03,\n",
       "                       -1.6632e-03,  4.4250e-03],\n",
       "                      ...,\n",
       "                      [-2.9182e-04, -1.3580e-03,  4.0588e-03,  ...,  1.6022e-03,\n",
       "                        6.1646e-03,  1.2512e-03],\n",
       "                      [ 1.6174e-03,  6.6376e-04, -1.8539e-03,  ..., -5.0735e-04,\n",
       "                       -3.3722e-03, -4.6692e-03],\n",
       "                      [-1.9684e-03, -5.3787e-04, -4.6692e-03,  ..., -2.1820e-03,\n",
       "                        4.7493e-04, -5.3787e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.9.input_layernorm.weight',\n",
       "              tensor([1.6328, 1.6875, 1.8359,  ..., 1.4219, 1.7266, 1.6328],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.9.post_attention_layernorm.weight',\n",
       "              tensor([1.4453, 1.7656, 1.7344,  ..., 1.3750, 1.4688, 1.5391],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.10.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0019,  0.0006, -0.0019,  ..., -0.0024, -0.0010,  0.0002],\n",
       "                      [-0.0013, -0.0013, -0.0002,  ..., -0.0028, -0.0012,  0.0023],\n",
       "                      [ 0.0007, -0.0042, -0.0021,  ..., -0.0025, -0.0004,  0.0096],\n",
       "                      ...,\n",
       "                      [-0.0052, -0.0002, -0.0035,  ..., -0.0013, -0.0032,  0.0059],\n",
       "                      [-0.0015, -0.0038, -0.0026,  ...,  0.0118, -0.0012, -0.0030],\n",
       "                      [ 0.0018,  0.0023, -0.0042,  ...,  0.0057,  0.0028, -0.0013]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.10.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0054, -0.0043, -0.0041,  ..., -0.0004, -0.0006,  0.0007],\n",
       "                      [-0.0028, -0.0050, -0.0083,  ...,  0.0040,  0.0042, -0.0023],\n",
       "                      [-0.0009,  0.0010, -0.0025,  ..., -0.0118, -0.0019,  0.0005],\n",
       "                      ...,\n",
       "                      [-0.0089, -0.0016, -0.0018,  ...,  0.0033,  0.0054, -0.0099],\n",
       "                      [-0.0053, -0.0063,  0.0066,  ..., -0.0036, -0.0092,  0.0034],\n",
       "                      [ 0.0038,  0.0121,  0.0093,  ..., -0.0038, -0.0029, -0.0089]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.10.self_attn.v_proj.weight',\n",
       "              tensor([[-8.3618e-03, -5.6458e-03, -5.5313e-04,  ..., -1.1444e-03,\n",
       "                       -2.1515e-03,  2.1820e-03],\n",
       "                      [ 1.0757e-03,  2.0142e-03, -1.3428e-03,  ...,  4.6387e-03,\n",
       "                        7.9346e-03, -1.0834e-03],\n",
       "                      [ 4.5204e-04,  3.9978e-03,  3.4637e-03,  ...,  4.1962e-04,\n",
       "                        2.9297e-03, -1.5259e-03],\n",
       "                      ...,\n",
       "                      [-1.5335e-03,  3.6774e-03, -4.7913e-03,  ...,  4.7112e-04,\n",
       "                        4.1389e-04,  4.4441e-04],\n",
       "                      [-8.2016e-04, -2.5177e-03, -3.0175e-07,  ...,  1.4210e-04,\n",
       "                       -3.6049e-04,  1.2665e-03],\n",
       "                      [-1.8005e-03, -4.8876e-06,  3.4943e-03,  ..., -4.1199e-03,\n",
       "                        1.9684e-03, -2.4414e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.10.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0042, -0.0004,  0.0007,  ..., -0.0031,  0.0013, -0.0016],\n",
       "                      [ 0.0031, -0.0030, -0.0030,  ...,  0.0001, -0.0005, -0.0029],\n",
       "                      [ 0.0021, -0.0002, -0.0008,  ..., -0.0034,  0.0060,  0.0022],\n",
       "                      ...,\n",
       "                      [ 0.0031,  0.0006,  0.0005,  ..., -0.0018,  0.0015,  0.0003],\n",
       "                      [ 0.0039, -0.0006, -0.0012,  ..., -0.0013, -0.0034, -0.0016],\n",
       "                      [-0.0006,  0.0017,  0.0013,  ...,  0.0024,  0.0029,  0.0010]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.10.mlp.gate_proj.weight',\n",
       "              tensor([[ 1.4877e-04,  4.7112e-04, -3.6163e-03,  ...,  1.7853e-03,\n",
       "                       -1.5564e-03, -2.8229e-04],\n",
       "                      [ 4.7607e-03,  1.2512e-03, -5.4626e-03,  ..., -5.4932e-03,\n",
       "                       -3.0365e-03,  5.2261e-04],\n",
       "                      [ 5.4321e-03, -4.2915e-04,  4.7607e-03,  ...,  3.2501e-03,\n",
       "                        7.2327e-03,  2.7924e-03],\n",
       "                      ...,\n",
       "                      [ 1.2665e-03, -2.2278e-03,  2.8687e-03,  ..., -1.9836e-03,\n",
       "                       -1.5335e-03, -2.4109e-03],\n",
       "                      [-1.3924e-04,  1.2779e-04, -2.4872e-03,  ..., -1.4954e-03,\n",
       "                        3.5553e-03, -2.1057e-03],\n",
       "                      [ 2.9922e-05,  1.6594e-04,  7.3624e-04,  ...,  4.6921e-04,\n",
       "                        9.9945e-04, -6.1340e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.10.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0020,  0.0082, -0.0034,  ...,  0.0079,  0.0031, -0.0025],\n",
       "                      [-0.0018,  0.0003,  0.0017,  ..., -0.0034, -0.0011, -0.0034],\n",
       "                      [ 0.0039,  0.0056, -0.0054,  ...,  0.0009,  0.0007,  0.0021],\n",
       "                      ...,\n",
       "                      [ 0.0033, -0.0030,  0.0030,  ..., -0.0027,  0.0018,  0.0026],\n",
       "                      [-0.0031,  0.0009, -0.0052,  ...,  0.0013, -0.0006, -0.0011],\n",
       "                      [ 0.0012,  0.0010,  0.0004,  ...,  0.0016, -0.0004,  0.0016]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.10.mlp.down_proj.weight',\n",
       "              tensor([[-0.0017,  0.0042,  0.0019,  ..., -0.0002,  0.0004,  0.0069],\n",
       "                      [-0.0030,  0.0034,  0.0031,  ..., -0.0022,  0.0012, -0.0041],\n",
       "                      [ 0.0017,  0.0014, -0.0034,  ...,  0.0008, -0.0025,  0.0030],\n",
       "                      ...,\n",
       "                      [-0.0006, -0.0032,  0.0027,  ...,  0.0002,  0.0007, -0.0059],\n",
       "                      [ 0.0009, -0.0012, -0.0023,  ..., -0.0021, -0.0048, -0.0007],\n",
       "                      [ 0.0020, -0.0034,  0.0014,  ..., -0.0045,  0.0008, -0.0023]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.10.input_layernorm.weight',\n",
       "              tensor([1.4219, 1.6719, 1.8828,  ..., 1.2109, 1.4688, 1.5781],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.10.post_attention_layernorm.weight',\n",
       "              tensor([1.4375, 1.8125, 1.8125,  ..., 1.3516, 1.4688, 1.5625],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.11.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0009, -0.0019,  0.0001,  ..., -0.0008,  0.0018,  0.0005],\n",
       "                      [-0.0012, -0.0033,  0.0007,  ..., -0.0010,  0.0018,  0.0038],\n",
       "                      [ 0.0012, -0.0024,  0.0016,  ..., -0.0008,  0.0008,  0.0012],\n",
       "                      ...,\n",
       "                      [-0.0023,  0.0038,  0.0024,  ...,  0.0022, -0.0028, -0.0059],\n",
       "                      [ 0.0005, -0.0075, -0.0024,  ...,  0.0012,  0.0014, -0.0026],\n",
       "                      [-0.0030, -0.0023, -0.0007,  ..., -0.0011, -0.0023, -0.0034]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.11.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0025, -0.0023,  0.0019,  ...,  0.0009,  0.0071,  0.0038],\n",
       "                      [-0.0035, -0.0022,  0.0023,  ..., -0.0035, -0.0027, -0.0012],\n",
       "                      [-0.0020,  0.0090, -0.0008,  ...,  0.0016, -0.0033, -0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0055, -0.0070, -0.0011,  ...,  0.0044,  0.0071, -0.0021],\n",
       "                      [ 0.0010,  0.0118, -0.0065,  ..., -0.0020, -0.0021,  0.0026],\n",
       "                      [ 0.0015,  0.0077,  0.0128,  ..., -0.0025,  0.0095, -0.0041]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.11.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0008, -0.0047, -0.0020,  ..., -0.0026, -0.0031, -0.0007],\n",
       "                      [-0.0049, -0.0031,  0.0099,  ..., -0.0022,  0.0012, -0.0033],\n",
       "                      [ 0.0021, -0.0015,  0.0044,  ..., -0.0018, -0.0043, -0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0007,  0.0021,  0.0007,  ..., -0.0044, -0.0018, -0.0001],\n",
       "                      [ 0.0030,  0.0018, -0.0009,  ...,  0.0019, -0.0009,  0.0002],\n",
       "                      [ 0.0048, -0.0017,  0.0018,  ..., -0.0025, -0.0017, -0.0006]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.11.self_attn.o_proj.weight',\n",
       "              tensor([[-1.2512e-03, -1.2283e-03,  1.1215e-03,  ..., -9.4604e-04,\n",
       "                        1.4648e-03, -2.0905e-03],\n",
       "                      [ 6.1035e-04,  2.7161e-03,  3.4332e-03,  ..., -3.2349e-03,\n",
       "                       -1.7700e-03,  4.0283e-03],\n",
       "                      [-2.4109e-03, -4.0588e-03, -3.0823e-03,  ..., -3.7956e-04,\n",
       "                       -1.0071e-03,  4.8828e-04],\n",
       "                      ...,\n",
       "                      [ 1.3504e-03,  2.7008e-03, -4.6692e-03,  ..., -1.4782e-04,\n",
       "                       -3.7384e-03,  1.9989e-03],\n",
       "                      [-3.8147e-03,  1.7776e-03, -2.4261e-03,  ...,  1.1368e-03,\n",
       "                        5.3787e-04,  4.4250e-03],\n",
       "                      [ 5.1260e-05, -4.0894e-03,  1.2064e-04,  ..., -1.2512e-03,\n",
       "                       -1.4267e-03,  7.2098e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.11.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0044,  0.0016, -0.0027,  ...,  0.0008,  0.0011,  0.0009],\n",
       "                      [-0.0062,  0.0062, -0.0015,  ..., -0.0033,  0.0027,  0.0054],\n",
       "                      [ 0.0006, -0.0009,  0.0055,  ...,  0.0004, -0.0004, -0.0015],\n",
       "                      ...,\n",
       "                      [ 0.0003, -0.0011,  0.0021,  ...,  0.0004,  0.0036, -0.0012],\n",
       "                      [ 0.0022, -0.0021,  0.0026,  ...,  0.0063,  0.0026,  0.0034],\n",
       "                      [ 0.0017, -0.0011, -0.0003,  ...,  0.0034, -0.0002, -0.0028]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.11.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0028, -0.0023, -0.0021,  ...,  0.0014, -0.0007, -0.0036],\n",
       "                      [ 0.0033, -0.0059, -0.0038,  ...,  0.0044,  0.0015, -0.0002],\n",
       "                      [-0.0018,  0.0002,  0.0007,  ..., -0.0049,  0.0075,  0.0017],\n",
       "                      ...,\n",
       "                      [ 0.0018,  0.0027,  0.0002,  ...,  0.0038, -0.0006,  0.0001],\n",
       "                      [-0.0002, -0.0065, -0.0013,  ..., -0.0002, -0.0018,  0.0033],\n",
       "                      [ 0.0025, -0.0069,  0.0036,  ...,  0.0035, -0.0055, -0.0041]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.11.mlp.down_proj.weight',\n",
       "              tensor([[ 2.6398e-03,  2.5330e-03, -9.8419e-04,  ...,  4.9744e-03,\n",
       "                        9.5367e-04, -1.6251e-03],\n",
       "                      [ 4.0627e-04, -5.4932e-03,  1.3123e-03,  ...,  4.4861e-03,\n",
       "                        2.4033e-04, -3.0060e-03],\n",
       "                      [ 1.5717e-03,  8.0872e-04,  2.2125e-04,  ...,  1.4267e-03,\n",
       "                        1.8082e-03, -2.9206e-05],\n",
       "                      ...,\n",
       "                      [-2.0790e-04,  3.4637e-03,  1.6594e-04,  ...,  5.9891e-04,\n",
       "                        2.8849e-05,  1.4305e-04],\n",
       "                      [ 8.4305e-04, -2.3651e-03,  4.6692e-03,  ..., -6.2561e-03,\n",
       "                        1.1063e-03,  1.8539e-03],\n",
       "                      [-3.6316e-03, -1.6861e-03,  3.4790e-03,  ...,  1.6708e-03,\n",
       "                       -1.4572e-03, -2.8687e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.11.input_layernorm.weight',\n",
       "              tensor([1.6094, 2.1250, 2.1094,  ..., 1.3203, 1.6953, 1.7344],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.11.post_attention_layernorm.weight',\n",
       "              tensor([1.5000, 2.0625, 1.8906,  ..., 1.3828, 1.5000, 1.6094],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.12.self_attn.q_proj.weight',\n",
       "              tensor([[-4.1504e-03, -3.2349e-03, -3.0670e-03,  ..., -2.9564e-04,\n",
       "                        1.1749e-03,  5.6267e-05],\n",
       "                      [ 4.5166e-03, -8.2397e-03,  1.7738e-04,  ..., -3.3569e-03,\n",
       "                       -3.7994e-03, -1.8539e-03],\n",
       "                      [-9.7656e-04, -7.3853e-03, -4.3640e-03,  ...,  9.4891e-05,\n",
       "                       -2.7771e-03,  1.5640e-03],\n",
       "                      ...,\n",
       "                      [-6.1035e-03,  4.4441e-04,  2.9449e-03,  ..., -8.8882e-04,\n",
       "                       -3.6469e-03,  2.7924e-03],\n",
       "                      [-4.9438e-03,  3.5248e-03, -1.0147e-03,  ...,  5.0659e-03,\n",
       "                        5.9814e-03,  1.9379e-03],\n",
       "                      [-3.6240e-04,  3.5858e-03,  1.6403e-03,  ..., -1.3733e-03,\n",
       "                       -3.6469e-03,  5.3406e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.12.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0025, -0.0031, -0.0004,  ..., -0.0016, -0.0008,  0.0053],\n",
       "                      [-0.0025,  0.0010, -0.0005,  ...,  0.0008, -0.0022, -0.0054],\n",
       "                      [ 0.0007, -0.0023,  0.0003,  ..., -0.0014,  0.0003, -0.0007],\n",
       "                      ...,\n",
       "                      [-0.0033, -0.0023,  0.0023,  ...,  0.0006,  0.0009, -0.0039],\n",
       "                      [-0.0008, -0.0008,  0.0010,  ..., -0.0010,  0.0006, -0.0018],\n",
       "                      [-0.0073,  0.0017, -0.0052,  ...,  0.0017,  0.0020, -0.0052]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.12.self_attn.v_proj.weight',\n",
       "              tensor([[ 1.5640e-03, -1.5182e-03, -2.5024e-03,  ...,  4.1008e-05,\n",
       "                        9.0122e-05,  1.2741e-03],\n",
       "                      [ 3.1471e-04,  3.3722e-03, -1.9836e-03,  ..., -1.7014e-03,\n",
       "                       -1.9150e-03, -6.9046e-04],\n",
       "                      [ 1.0681e-03,  2.1210e-03, -1.6022e-03,  ..., -3.5858e-03,\n",
       "                       -1.5793e-03,  2.2125e-03],\n",
       "                      ...,\n",
       "                      [ 1.6708e-03, -4.8523e-03, -2.4261e-03,  ..., -9.0790e-04,\n",
       "                       -1.9531e-03, -4.7302e-03],\n",
       "                      [ 3.6163e-03, -2.7618e-03, -1.0300e-03,  ..., -3.1471e-04,\n",
       "                       -1.4019e-04, -9.0027e-04],\n",
       "                      [ 3.0365e-03, -1.1139e-03, -7.4463e-03,  ...,  2.2278e-03,\n",
       "                       -1.7853e-03, -1.7700e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.12.self_attn.o_proj.weight',\n",
       "              tensor([[-2.5482e-03,  5.2643e-04, -1.4114e-03,  ...,  2.1667e-03,\n",
       "                        2.7924e-03,  1.3657e-03],\n",
       "                      [ 3.7231e-03,  4.0436e-04, -4.9133e-03,  ..., -1.4191e-03,\n",
       "                       -1.6479e-03, -9.8228e-05],\n",
       "                      [ 1.8215e-04, -9.1171e-04, -9.8419e-04,  ..., -5.7678e-03,\n",
       "                       -1.5182e-03, -3.1128e-03],\n",
       "                      ...,\n",
       "                      [-1.3504e-03,  2.4109e-03, -8.2779e-04,  ...,  1.3123e-03,\n",
       "                       -2.0142e-03, -1.3275e-03],\n",
       "                      [ 3.5095e-04, -2.8076e-03, -1.8082e-03,  ..., -2.4414e-04,\n",
       "                        4.0894e-03,  2.1210e-03],\n",
       "                      [ 2.0905e-03,  2.5482e-03, -1.0071e-03,  ..., -7.6294e-04,\n",
       "                       -5.4016e-03,  6.1798e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.12.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0022, -0.0021,  0.0020,  ..., -0.0004,  0.0004,  0.0011],\n",
       "                      [ 0.0038, -0.0003, -0.0054,  ..., -0.0017,  0.0013, -0.0019],\n",
       "                      [ 0.0050,  0.0007,  0.0052,  ..., -0.0032, -0.0024, -0.0023],\n",
       "                      ...,\n",
       "                      [ 0.0021,  0.0020, -0.0018,  ...,  0.0007,  0.0049,  0.0011],\n",
       "                      [-0.0062, -0.0031,  0.0057,  ...,  0.0033,  0.0039,  0.0005],\n",
       "                      [ 0.0030,  0.0049,  0.0010,  ..., -0.0013,  0.0001,  0.0034]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.12.mlp.up_proj.weight',\n",
       "              tensor([[ 2.7771e-03, -2.6550e-03,  1.9836e-03,  ..., -1.9302e-03,\n",
       "                       -1.1384e-05, -8.9169e-05],\n",
       "                      [-1.6556e-03,  1.8845e-03, -4.0588e-03,  ..., -1.5869e-03,\n",
       "                        2.7466e-03,  8.4877e-05],\n",
       "                      [ 5.7983e-04,  2.1973e-03, -2.8381e-03,  ..., -8.3542e-04,\n",
       "                       -2.7618e-03, -8.4305e-04],\n",
       "                      ...,\n",
       "                      [ 2.8687e-03,  2.0599e-03, -7.6675e-04,  ..., -7.5912e-04,\n",
       "                       -2.1973e-03,  5.4321e-03],\n",
       "                      [ 2.4872e-03,  1.8005e-03, -2.7313e-03,  ...,  4.8218e-03,\n",
       "                       -3.5286e-04, -4.6158e-04],\n",
       "                      [ 6.9580e-03,  6.9275e-03, -8.2397e-03,  ...,  4.4556e-03,\n",
       "                       -3.5400e-03,  9.9945e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.12.mlp.down_proj.weight',\n",
       "              tensor([[ 3.3569e-03, -1.8616e-03,  8.3447e-05,  ...,  2.2736e-03,\n",
       "                        3.5553e-03,  4.8523e-03],\n",
       "                      [-8.6212e-04,  1.8387e-03,  4.0894e-03,  ..., -2.5940e-03,\n",
       "                       -3.7689e-03,  3.4027e-03],\n",
       "                      [-5.2643e-04,  1.5640e-03,  6.9275e-03,  ..., -3.6716e-05,\n",
       "                        2.2583e-03, -3.4485e-03],\n",
       "                      ...,\n",
       "                      [ 2.4872e-03, -7.5531e-04, -1.9372e-06,  ...,  2.8534e-03,\n",
       "                        4.3335e-03,  1.2512e-03],\n",
       "                      [ 3.9978e-03, -1.7624e-03, -4.7302e-04,  ...,  9.4986e-04,\n",
       "                        2.6703e-03, -1.5945e-03],\n",
       "                      [ 9.5367e-04,  3.8910e-03, -3.6469e-03,  ...,  3.9978e-03,\n",
       "                       -4.3945e-03,  2.3041e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.12.input_layernorm.weight',\n",
       "              tensor([1.7500, 2.2812, 2.3594,  ..., 1.4609, 1.7969, 1.9609],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.12.post_attention_layernorm.weight',\n",
       "              tensor([1.6172, 2.3594, 2.1094,  ..., 1.4453, 1.6406, 1.7500],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.13.self_attn.q_proj.weight',\n",
       "              tensor([[-3.7766e-04,  6.7749e-03,  3.4790e-03,  ...,  4.1962e-05,\n",
       "                        4.4250e-04, -9.9945e-04],\n",
       "                      [-1.2436e-03, -6.0730e-03,  2.3804e-03,  ...,  3.2959e-03,\n",
       "                        3.3417e-03, -1.0986e-03],\n",
       "                      [ 4.3335e-03,  6.0730e-03,  2.0752e-03,  ...,  1.7090e-03,\n",
       "                       -1.1978e-03,  5.9128e-04],\n",
       "                      ...,\n",
       "                      [ 1.8005e-03,  4.8218e-03,  2.2888e-03,  ...,  1.5411e-03,\n",
       "                       -8.0490e-04,  4.3030e-03],\n",
       "                      [ 1.3733e-03, -3.7003e-04,  3.7842e-03,  ...,  2.2316e-04,\n",
       "                        5.2261e-04, -3.1281e-03],\n",
       "                      [-4.6997e-03,  2.1210e-03,  1.8463e-03,  ...,  2.0752e-03,\n",
       "                       -8.2016e-04, -3.5858e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.13.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0035, -0.0082, -0.0045,  ..., -0.0025, -0.0062,  0.0034],\n",
       "                      [ 0.0001, -0.0036,  0.0029,  ..., -0.0031,  0.0070,  0.0025],\n",
       "                      [ 0.0008, -0.0015,  0.0016,  ...,  0.0018,  0.0052,  0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0091, -0.0027, -0.0040,  ..., -0.0013, -0.0006, -0.0058],\n",
       "                      [-0.0050, -0.0052, -0.0041,  ..., -0.0004,  0.0034,  0.0033],\n",
       "                      [ 0.0028, -0.0066, -0.0029,  ..., -0.0051,  0.0002,  0.0027]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.13.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0002, -0.0012, -0.0003,  ...,  0.0009, -0.0018,  0.0004],\n",
       "                      [ 0.0014, -0.0038, -0.0039,  ...,  0.0004, -0.0007,  0.0002],\n",
       "                      [-0.0001,  0.0012,  0.0057,  ...,  0.0012, -0.0002, -0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0003, -0.0010, -0.0010,  ...,  0.0020,  0.0028,  0.0006],\n",
       "                      [-0.0005,  0.0016, -0.0007,  ..., -0.0048, -0.0017, -0.0023],\n",
       "                      [ 0.0042,  0.0047, -0.0015,  ..., -0.0043, -0.0018,  0.0003]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.13.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0005,  0.0006,  0.0050,  ..., -0.0029, -0.0024, -0.0017],\n",
       "                      [ 0.0039, -0.0011, -0.0008,  ..., -0.0017, -0.0039, -0.0004],\n",
       "                      [ 0.0006, -0.0002,  0.0047,  ..., -0.0031,  0.0013,  0.0006],\n",
       "                      ...,\n",
       "                      [ 0.0034, -0.0019,  0.0026,  ...,  0.0043,  0.0007, -0.0011],\n",
       "                      [-0.0014, -0.0024, -0.0034,  ...,  0.0010, -0.0001,  0.0038],\n",
       "                      [-0.0013, -0.0020, -0.0055,  ...,  0.0005, -0.0046,  0.0016]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.13.mlp.gate_proj.weight',\n",
       "              tensor([[-2.7466e-03, -6.4468e-04,  1.3580e-03,  ...,  4.4861e-03,\n",
       "                        1.0605e-03,  1.0452e-03],\n",
       "                      [-5.9204e-03,  1.8978e-04, -2.5330e-03,  ..., -2.3651e-03,\n",
       "                        1.9169e-04, -9.0790e-04],\n",
       "                      [-8.7280e-03,  5.6152e-03,  3.1128e-03,  ..., -2.0294e-03,\n",
       "                        4.2114e-03,  3.8757e-03],\n",
       "                      ...,\n",
       "                      [-1.2207e-03, -5.6152e-03,  1.1597e-03,  ..., -4.4861e-03,\n",
       "                        7.0953e-04, -3.5706e-03],\n",
       "                      [ 2.8687e-03,  5.4598e-05,  2.8610e-04,  ...,  2.8534e-03,\n",
       "                       -2.9297e-03,  3.1471e-04],\n",
       "                      [-4.4250e-03,  4.4823e-04, -2.8229e-03,  ..., -3.5248e-03,\n",
       "                       -3.8300e-03,  4.8523e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.13.mlp.up_proj.weight',\n",
       "              tensor([[-2.8839e-03, -7.6294e-05, -2.3961e-05,  ...,  2.5482e-03,\n",
       "                       -1.9150e-03,  3.1853e-04],\n",
       "                      [ 2.8534e-03,  2.4414e-03,  1.3199e-03,  ..., -2.5787e-03,\n",
       "                        4.3030e-03,  9.3079e-04],\n",
       "                      [-3.9673e-03, -1.8921e-03,  6.1646e-03,  ...,  5.6839e-04,\n",
       "                        4.3335e-03, -3.9978e-03],\n",
       "                      ...,\n",
       "                      [ 2.7466e-03, -8.3008e-03,  5.0735e-04,  ...,  2.6703e-03,\n",
       "                       -1.2283e-03,  1.2970e-03],\n",
       "                      [-6.7139e-04,  3.0670e-03, -6.9809e-04,  ..., -2.3041e-03,\n",
       "                        7.4005e-04, -2.0294e-03],\n",
       "                      [-3.3264e-03, -1.3809e-03, -6.2561e-04,  ...,  1.3428e-03,\n",
       "                        1.1826e-03,  5.2795e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.13.mlp.down_proj.weight',\n",
       "              tensor([[-0.0001,  0.0008,  0.0010,  ..., -0.0021, -0.0025,  0.0027],\n",
       "                      [ 0.0009,  0.0001,  0.0012,  ..., -0.0056,  0.0014, -0.0011],\n",
       "                      [ 0.0016, -0.0026,  0.0031,  ..., -0.0007,  0.0028, -0.0044],\n",
       "                      ...,\n",
       "                      [-0.0012, -0.0025, -0.0033,  ..., -0.0003,  0.0023,  0.0022],\n",
       "                      [ 0.0023,  0.0045,  0.0037,  ..., -0.0031, -0.0025, -0.0039],\n",
       "                      [-0.0011,  0.0016, -0.0009,  ..., -0.0009,  0.0017, -0.0007]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.13.input_layernorm.weight',\n",
       "              tensor([1.8047, 2.7188, 2.6250,  ..., 1.4297, 1.8047, 2.0781],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.13.post_attention_layernorm.weight',\n",
       "              tensor([1.7422, 2.7188, 2.2344,  ..., 1.5469, 1.7188, 1.8359],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.14.self_attn.q_proj.weight',\n",
       "              tensor([[-4.1199e-03,  3.1891e-03, -2.3346e-03,  ..., -4.3640e-03,\n",
       "                       -2.2054e-05,  3.9673e-03],\n",
       "                      [ 7.5400e-06,  7.6904e-03,  6.1417e-04,  ..., -9.3079e-04,\n",
       "                        9.6512e-04, -1.9989e-03],\n",
       "                      [ 5.6763e-03, -5.7373e-03,  4.3335e-03,  ...,  7.5912e-04,\n",
       "                        2.1362e-03,  1.9932e-04],\n",
       "                      ...,\n",
       "                      [ 1.1978e-03,  1.2436e-03,  9.6436e-03,  ...,  5.8899e-03,\n",
       "                        8.9111e-03, -5.0049e-03],\n",
       "                      [ 7.1411e-03,  5.3711e-03, -9.3994e-03,  ...,  9.6130e-04,\n",
       "                       -1.1841e-02, -8.0872e-04],\n",
       "                      [-2.7008e-03, -7.3853e-03, -2.3937e-04,  ..., -6.5613e-04,\n",
       "                       -1.5640e-03,  2.5635e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.14.self_attn.k_proj.weight',\n",
       "              tensor([[ 8.4305e-04, -3.0518e-04,  3.2616e-04,  ..., -2.1362e-03,\n",
       "                       -4.1199e-04, -1.8539e-03],\n",
       "                      [-1.6251e-03,  1.3123e-03,  2.1458e-04,  ..., -3.2501e-03,\n",
       "                        2.4872e-03,  1.2589e-03],\n",
       "                      [-1.3580e-03,  2.9922e-05, -1.0300e-03,  ...,  2.0294e-03,\n",
       "                       -1.7776e-03,  2.1057e-03],\n",
       "                      ...,\n",
       "                      [ 5.8594e-03,  2.6093e-03,  3.1738e-03,  ...,  2.2736e-03,\n",
       "                       -3.9368e-03, -8.0490e-04],\n",
       "                      [-2.5330e-03, -3.8910e-03,  1.7242e-03,  ..., -4.3640e-03,\n",
       "                       -1.5198e-02,  3.6926e-03],\n",
       "                      [-3.8910e-03, -9.5749e-04,  5.5237e-03,  ..., -2.2888e-03,\n",
       "                        5.0049e-03, -1.0452e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.14.self_attn.v_proj.weight',\n",
       "              tensor([[ 1.3580e-03, -2.1362e-03,  2.0447e-03,  ..., -3.3417e-03,\n",
       "                       -2.5024e-03,  1.9150e-03],\n",
       "                      [-2.5787e-03, -2.7657e-04,  3.7231e-03,  ...,  1.6327e-03,\n",
       "                        2.8229e-03,  1.1978e-03],\n",
       "                      [-8.4305e-04, -1.6708e-03, -1.7548e-04,  ...,  4.8218e-03,\n",
       "                       -1.5488e-03,  1.3199e-03],\n",
       "                      ...,\n",
       "                      [-4.2677e-05,  2.0142e-03, -1.9226e-03,  ..., -1.4648e-03,\n",
       "                       -2.5940e-03, -1.9989e-03],\n",
       "                      [ 3.4332e-03, -3.0365e-03,  3.7689e-03,  ...,  1.6403e-03,\n",
       "                        9.6893e-04, -2.3041e-03],\n",
       "                      [ 1.9455e-04,  3.6478e-05, -1.3580e-03,  ...,  4.5776e-03,\n",
       "                        1.4420e-03, -3.0518e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.14.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0015, -0.0011, -0.0003,  ...,  0.0009,  0.0016,  0.0023],\n",
       "                      [-0.0016, -0.0032, -0.0005,  ..., -0.0052, -0.0007, -0.0021],\n",
       "                      [-0.0016, -0.0023,  0.0032,  ...,  0.0011,  0.0003, -0.0024],\n",
       "                      ...,\n",
       "                      [-0.0014,  0.0014, -0.0003,  ...,  0.0064,  0.0028,  0.0025],\n",
       "                      [ 0.0028,  0.0016,  0.0018,  ...,  0.0012,  0.0026,  0.0003],\n",
       "                      [ 0.0028, -0.0001, -0.0022,  ..., -0.0004, -0.0009, -0.0009]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.14.mlp.gate_proj.weight',\n",
       "              tensor([[-5.1575e-03, -4.2114e-03, -2.3956e-03,  ...,  3.9673e-03,\n",
       "                        4.9438e-03,  5.6152e-03],\n",
       "                      [ 4.8399e-05,  1.1749e-03, -4.5166e-03,  ..., -5.3711e-03,\n",
       "                        5.1575e-03, -3.4332e-03],\n",
       "                      [-3.0975e-03,  1.6098e-03, -3.8452e-03,  ...,  7.5150e-04,\n",
       "                        2.8687e-03, -3.8605e-03],\n",
       "                      ...,\n",
       "                      [-2.4719e-03,  1.7776e-03,  4.3335e-03,  ...,  8.1787e-03,\n",
       "                        6.2561e-03,  2.9297e-03],\n",
       "                      [ 1.0681e-03, -3.4332e-04,  1.5945e-03,  ...,  3.0994e-05,\n",
       "                       -1.6251e-03, -4.2725e-03],\n",
       "                      [-4.0588e-03, -5.6076e-04, -2.8534e-03,  ...,  6.0654e-04,\n",
       "                        1.8463e-03,  1.0529e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.14.mlp.up_proj.weight',\n",
       "              tensor([[-7.4768e-04,  2.7924e-03, -2.8610e-04,  ...,  3.9368e-03,\n",
       "                        3.1281e-03,  2.3956e-03],\n",
       "                      [ 2.6550e-03, -7.1411e-03, -2.2278e-03,  ...,  5.1498e-04,\n",
       "                        3.2196e-03, -8.5831e-05],\n",
       "                      [-9.5749e-04, -1.9531e-03,  3.5667e-04,  ..., -1.8463e-03,\n",
       "                       -2.9602e-03,  1.1292e-03],\n",
       "                      ...,\n",
       "                      [ 2.7008e-03,  3.3569e-03,  2.1820e-03,  ...,  2.1057e-03,\n",
       "                        3.7003e-04,  3.6316e-03],\n",
       "                      [-2.4872e-03,  1.9150e-03,  4.5166e-03,  ...,  8.7738e-04,\n",
       "                       -3.6774e-03, -4.8218e-03],\n",
       "                      [ 5.4321e-03, -1.9531e-03,  3.1891e-03,  ..., -4.1504e-03,\n",
       "                        7.5150e-04, -3.5706e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.14.mlp.down_proj.weight',\n",
       "              tensor([[-0.0008,  0.0043,  0.0011,  ..., -0.0005,  0.0005,  0.0016],\n",
       "                      [-0.0024, -0.0029, -0.0010,  ...,  0.0027, -0.0004,  0.0045],\n",
       "                      [ 0.0028, -0.0052, -0.0004,  ...,  0.0002, -0.0078,  0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0024,  0.0035,  0.0028,  ..., -0.0050,  0.0037, -0.0028],\n",
       "                      [ 0.0026,  0.0005, -0.0018,  ...,  0.0020, -0.0017, -0.0009],\n",
       "                      [-0.0016,  0.0005, -0.0017,  ..., -0.0001, -0.0011,  0.0012]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.14.input_layernorm.weight',\n",
       "              tensor([1.9844, 2.9375, 2.7656,  ..., 1.5469, 1.9062, 2.1875],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.14.post_attention_layernorm.weight',\n",
       "              tensor([1.7969, 3.1094, 2.2969,  ..., 1.6406, 1.7812, 1.8828],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.15.self_attn.q_proj.weight',\n",
       "              tensor([[-6.9046e-04,  3.2806e-03,  1.8406e-04,  ...,  5.0354e-04,\n",
       "                       -4.1504e-03, -1.1368e-03],\n",
       "                      [-2.5749e-04, -4.1809e-03,  8.2397e-04,  ...,  1.1978e-03,\n",
       "                        1.9455e-03,  1.3046e-03],\n",
       "                      [-3.0823e-03,  6.3477e-03,  6.7139e-04,  ...,  2.7313e-03,\n",
       "                       -1.5030e-03,  2.4109e-03],\n",
       "                      ...,\n",
       "                      [-6.3782e-03,  4.1809e-03,  6.1035e-03,  ..., -4.4250e-03,\n",
       "                        4.7913e-03,  5.1498e-04],\n",
       "                      [ 4.7302e-04, -1.6212e-05,  4.6082e-03,  ...,  4.2114e-03,\n",
       "                       -8.3008e-03,  1.0681e-03],\n",
       "                      [ 2.1515e-03,  2.2583e-03,  1.8311e-03,  ..., -4.1199e-03,\n",
       "                        2.0752e-03, -5.6152e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.15.self_attn.k_proj.weight',\n",
       "              tensor([[ 2.8839e-03, -6.2256e-03,  2.1553e-04,  ..., -7.4804e-06,\n",
       "                        2.4261e-03, -6.1798e-04],\n",
       "                      [-2.8076e-03,  6.3477e-03, -1.4648e-03,  ..., -3.5706e-03,\n",
       "                       -1.3123e-03,  2.4414e-03],\n",
       "                      [ 1.6327e-03, -6.5918e-03, -4.6082e-03,  ...,  1.7014e-03,\n",
       "                       -9.3460e-04,  1.6785e-03],\n",
       "                      ...,\n",
       "                      [-7.7209e-03, -2.9755e-04,  9.3384e-03,  ...,  4.0588e-03,\n",
       "                        4.1809e-03, -5.4016e-03],\n",
       "                      [ 4.0283e-03,  7.5150e-04,  3.1738e-03,  ..., -5.5847e-03,\n",
       "                       -3.3569e-03, -3.3951e-04],\n",
       "                      [-1.4877e-03, -2.3804e-03,  8.3008e-03,  ...,  4.5471e-03,\n",
       "                        1.9043e-02,  9.7046e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.15.self_attn.v_proj.weight',\n",
       "              tensor([[ 4.8523e-03, -1.5411e-03, -1.3046e-03,  ..., -4.4556e-03,\n",
       "                        2.0123e-04,  2.7008e-03],\n",
       "                      [-1.1110e-04,  1.0605e-03,  2.1362e-03,  ..., -1.4267e-03,\n",
       "                       -1.7834e-04,  3.7689e-03],\n",
       "                      [-2.0027e-04, -4.1809e-03,  2.8381e-03,  ...,  3.1433e-03,\n",
       "                        4.6997e-03, -8.5068e-04],\n",
       "                      ...,\n",
       "                      [ 5.0354e-04,  1.4687e-04, -9.1171e-04,  ..., -3.5706e-03,\n",
       "                       -1.9302e-03, -2.2278e-03],\n",
       "                      [ 1.2589e-03, -7.7248e-05,  7.1335e-04,  ..., -2.7924e-03,\n",
       "                        4.6387e-03,  1.5869e-03],\n",
       "                      [ 3.2501e-03, -9.6512e-04, -1.4038e-03,  ...,  7.0190e-04,\n",
       "                        8.8215e-05, -1.7166e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.15.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0029, -0.0027,  0.0003,  ..., -0.0020,  0.0037,  0.0022],\n",
       "                      [-0.0011,  0.0008, -0.0019,  ...,  0.0036,  0.0037,  0.0015],\n",
       "                      [ 0.0012,  0.0028,  0.0024,  ..., -0.0023,  0.0037,  0.0035],\n",
       "                      ...,\n",
       "                      [-0.0033, -0.0011,  0.0026,  ..., -0.0045, -0.0004, -0.0037],\n",
       "                      [ 0.0004,  0.0006,  0.0005,  ...,  0.0002, -0.0045,  0.0018],\n",
       "                      [ 0.0027,  0.0020, -0.0008,  ..., -0.0019, -0.0020,  0.0015]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.15.mlp.gate_proj.weight',\n",
       "              tensor([[ 3.8528e-04, -3.2654e-03, -2.0294e-03,  ...,  4.1199e-03,\n",
       "                        3.0975e-03,  1.4725e-03],\n",
       "                      [-2.7313e-03,  2.3193e-03, -6.8665e-04,  ...,  9.9945e-04,\n",
       "                       -7.0095e-05,  5.7068e-03],\n",
       "                      [-2.2430e-03, -2.1973e-03,  6.3324e-04,  ...,  1.2512e-03,\n",
       "                        5.6458e-04,  2.2430e-03],\n",
       "                      ...,\n",
       "                      [ 2.4414e-03,  1.0395e-04,  1.2741e-03,  ..., -3.2043e-03,\n",
       "                       -6.2561e-04, -1.8311e-03],\n",
       "                      [ 1.4801e-03,  2.9564e-04,  1.9169e-04,  ..., -8.4305e-04,\n",
       "                       -3.0365e-03, -3.8147e-03],\n",
       "                      [-3.2196e-03,  2.6093e-03, -3.4943e-03,  ...,  7.7438e-04,\n",
       "                        5.8746e-04,  1.0376e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.15.mlp.up_proj.weight',\n",
       "              tensor([[-1.0147e-03,  4.5776e-03, -7.4768e-04,  ..., -5.2185e-03,\n",
       "                       -3.7689e-03,  1.5869e-03],\n",
       "                      [-6.0201e-06,  1.5335e-03, -3.0365e-03,  ..., -4.4556e-03,\n",
       "                        3.4332e-03, -2.5177e-04],\n",
       "                      [ 2.6855e-03, -3.5248e-03, -1.5869e-03,  ..., -2.6703e-03,\n",
       "                       -1.1597e-03, -2.4872e-03],\n",
       "                      ...,\n",
       "                      [-1.9741e-04,  1.5354e-04, -1.4038e-03,  ...,  1.8768e-03,\n",
       "                       -5.1270e-03,  1.0071e-03],\n",
       "                      [ 5.0354e-04, -4.8523e-03, -1.2054e-03,  ..., -2.4567e-03,\n",
       "                       -8.5831e-04,  5.5313e-04],\n",
       "                      [-8.1253e-04,  2.3041e-03, -3.2349e-03,  ...,  2.2888e-03,\n",
       "                        7.5378e-03, -5.1880e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.15.mlp.down_proj.weight',\n",
       "              tensor([[ 9.8944e-06,  9.0408e-04,  1.6098e-03,  ..., -3.5553e-03,\n",
       "                       -3.3875e-03, -4.2725e-03],\n",
       "                      [-3.3951e-04,  1.4496e-04,  4.4107e-05,  ..., -1.4343e-03,\n",
       "                        2.0027e-04,  3.0708e-04],\n",
       "                      [ 1.0681e-03, -1.9989e-03, -1.5869e-03,  ..., -3.8300e-03,\n",
       "                       -3.8452e-03, -5.3024e-04],\n",
       "                      ...,\n",
       "                      [ 1.1063e-03, -2.4414e-03, -1.3428e-03,  ..., -1.9226e-03,\n",
       "                       -2.4109e-03,  4.2725e-03],\n",
       "                      [ 4.1199e-03,  1.3560e-06, -1.1978e-03,  ..., -4.1199e-03,\n",
       "                        3.5477e-04, -2.9297e-03],\n",
       "                      [ 3.5286e-04, -2.2888e-04, -8.7261e-05,  ...,  6.6376e-04,\n",
       "                        9.9182e-04,  1.9932e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.15.input_layernorm.weight',\n",
       "              tensor([2.2031, 3.2812, 3.0156,  ..., 1.8828, 2.2188, 2.4219],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.15.post_attention_layernorm.weight',\n",
       "              tensor([1.9688, 3.8438, 2.4219,  ..., 1.8516, 1.9688, 2.0156],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.16.self_attn.q_proj.weight',\n",
       "              tensor([[-1.1902e-03,  4.1809e-03, -5.7983e-04,  ...,  2.1057e-03,\n",
       "                        1.4648e-03, -1.1921e-04],\n",
       "                      [ 5.2261e-04, -3.1586e-03, -2.3651e-03,  ...,  2.7771e-03,\n",
       "                        2.1210e-03, -1.6022e-03],\n",
       "                      [-8.2970e-05,  3.8605e-03,  3.7384e-04,  ...,  7.4768e-04,\n",
       "                       -2.7313e-03,  9.7656e-04],\n",
       "                      ...,\n",
       "                      [ 2.7313e-03, -2.5749e-04,  1.5926e-04,  ..., -4.3030e-03,\n",
       "                        5.6763e-03, -1.4782e-04],\n",
       "                      [-1.7929e-03,  1.5030e-03,  1.5259e-03,  ..., -3.6621e-04,\n",
       "                        4.1809e-03,  3.9673e-03],\n",
       "                      [ 1.4191e-03,  1.5030e-03, -2.7771e-03,  ..., -1.9379e-03,\n",
       "                       -2.7618e-03, -6.7444e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.16.self_attn.k_proj.weight',\n",
       "              tensor([[-9.9659e-05,  3.5248e-03,  3.5706e-03,  ...,  3.2654e-03,\n",
       "                        9.0408e-04, -1.6327e-03],\n",
       "                      [-1.8158e-03, -4.5013e-04,  3.7956e-04,  ..., -2.2736e-03,\n",
       "                        1.9226e-03,  1.9989e-03],\n",
       "                      [ 1.0681e-04,  1.7548e-03,  1.7776e-03,  ...,  1.5564e-03,\n",
       "                        3.3875e-03, -1.4267e-03],\n",
       "                      ...,\n",
       "                      [ 9.0027e-04, -1.5335e-03, -2.2736e-03,  ..., -1.4114e-03,\n",
       "                       -1.7700e-03,  4.6730e-04],\n",
       "                      [-1.3351e-03,  2.6550e-03, -2.5330e-03,  ...,  2.7771e-03,\n",
       "                       -4.5166e-03, -3.2806e-03],\n",
       "                      [ 9.6893e-04,  5.6763e-03, -5.4321e-03,  ..., -1.1414e-02,\n",
       "                        2.5787e-03, -6.6528e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.16.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0005,  0.0010, -0.0044,  ..., -0.0033,  0.0006, -0.0010],\n",
       "                      [-0.0003, -0.0005,  0.0011,  ..., -0.0026, -0.0027,  0.0022],\n",
       "                      [ 0.0024, -0.0023, -0.0011,  ..., -0.0024, -0.0019, -0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0010, -0.0001, -0.0022,  ...,  0.0014,  0.0010, -0.0039],\n",
       "                      [-0.0003,  0.0012, -0.0008,  ..., -0.0028,  0.0004,  0.0034],\n",
       "                      [-0.0019,  0.0009,  0.0004,  ..., -0.0019,  0.0052,  0.0027]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.16.self_attn.o_proj.weight',\n",
       "              tensor([[ 2.8992e-03, -2.1210e-03,  2.7924e-03,  ...,  2.0294e-03,\n",
       "                       -2.7771e-03,  2.5635e-03],\n",
       "                      [ 7.1335e-04,  5.6839e-04,  3.2806e-03,  ..., -1.6022e-03,\n",
       "                       -5.3024e-04, -1.1520e-03],\n",
       "                      [-4.6082e-03, -1.2207e-03,  1.3256e-04,  ..., -4.5013e-04,\n",
       "                       -2.1267e-04, -4.7684e-04],\n",
       "                      ...,\n",
       "                      [-3.5858e-03, -3.7003e-04,  7.9632e-05,  ..., -2.3956e-03,\n",
       "                        8.6594e-04,  3.4943e-03],\n",
       "                      [ 1.2131e-03,  2.8610e-05, -2.9907e-03,  ...,  1.6098e-03,\n",
       "                        3.5553e-03,  2.1820e-03],\n",
       "                      [-2.7313e-03, -1.6327e-03, -2.7618e-03,  ..., -2.8381e-03,\n",
       "                       -1.3504e-03,  2.6245e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.16.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0025, -0.0045,  0.0014,  ..., -0.0054, -0.0004, -0.0050],\n",
       "                      [ 0.0035,  0.0020, -0.0002,  ...,  0.0024,  0.0045, -0.0032],\n",
       "                      [ 0.0025, -0.0002, -0.0049,  ...,  0.0019, -0.0020,  0.0058],\n",
       "                      ...,\n",
       "                      [ 0.0023,  0.0022, -0.0014,  ...,  0.0023,  0.0056, -0.0062],\n",
       "                      [ 0.0006, -0.0003, -0.0008,  ..., -0.0044, -0.0036, -0.0008],\n",
       "                      [-0.0003, -0.0016,  0.0002,  ...,  0.0006,  0.0003,  0.0032]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.16.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0019, -0.0045, -0.0027,  ..., -0.0017, -0.0003, -0.0043],\n",
       "                      [-0.0001,  0.0035,  0.0008,  ...,  0.0026, -0.0023,  0.0019],\n",
       "                      [-0.0026,  0.0010, -0.0021,  ..., -0.0018, -0.0068,  0.0022],\n",
       "                      ...,\n",
       "                      [-0.0025, -0.0008, -0.0029,  ..., -0.0031, -0.0016, -0.0029],\n",
       "                      [-0.0031,  0.0025, -0.0011,  ...,  0.0018,  0.0046, -0.0048],\n",
       "                      [-0.0081, -0.0004,  0.0039,  ...,  0.0011, -0.0026, -0.0003]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.16.mlp.down_proj.weight',\n",
       "              tensor([[ 2.4872e-03, -2.4567e-03, -4.0283e-03,  ..., -1.7929e-03,\n",
       "                       -6.8665e-03, -8.7891e-03],\n",
       "                      [-7.3547e-03,  6.6223e-03,  9.2316e-04,  ..., -4.6082e-03,\n",
       "                        9.4604e-04,  1.7242e-03],\n",
       "                      [-3.7079e-03, -6.8188e-05, -2.5779e-06,  ..., -2.6245e-03,\n",
       "                        1.0223e-03,  4.0894e-03],\n",
       "                      ...,\n",
       "                      [ 4.6997e-03,  1.0681e-03,  1.2894e-03,  ..., -2.3193e-03,\n",
       "                        1.1749e-03, -5.2185e-03],\n",
       "                      [-1.1978e-03, -1.7548e-03, -7.1411e-03,  ..., -3.0670e-03,\n",
       "                        3.2806e-04, -4.1771e-04],\n",
       "                      [-4.3030e-03,  3.0212e-03,  3.4027e-03,  ..., -4.4556e-03,\n",
       "                       -1.8158e-03, -1.8539e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.16.input_layernorm.weight',\n",
       "              tensor([2.3594, 3.6875, 2.9844,  ..., 1.9609, 2.2812, 2.4688],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.16.post_attention_layernorm.weight',\n",
       "              tensor([2.1406, 4.5000, 2.5156,  ..., 2.0000, 2.1250, 2.1719],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.17.self_attn.q_proj.weight',\n",
       "              tensor([[-3.7689e-03, -7.0190e-03,  2.2736e-03,  ..., -1.0834e-03,\n",
       "                       -5.3711e-03, -2.3193e-03],\n",
       "                      [-2.6703e-03, -3.0670e-03,  6.4850e-05,  ...,  3.4180e-03,\n",
       "                       -3.4904e-04,  4.0894e-03],\n",
       "                      [-1.6022e-03,  6.9275e-03, -6.8359e-03,  ..., -1.4343e-03,\n",
       "                       -2.3346e-03, -3.9978e-03],\n",
       "                      ...,\n",
       "                      [ 8.2397e-04, -1.0925e-02,  2.1515e-03,  ..., -4.3945e-03,\n",
       "                       -4.2725e-04, -1.4973e-04],\n",
       "                      [ 1.1673e-03,  9.3079e-04, -1.2589e-03,  ...,  4.2419e-03,\n",
       "                       -2.4109e-03, -2.8381e-03],\n",
       "                      [ 4.8523e-03,  7.8125e-03,  9.9182e-04,  ..., -6.0730e-03,\n",
       "                        5.3101e-03, -2.4109e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.17.self_attn.k_proj.weight',\n",
       "              tensor([[ 7.1335e-04,  4.2419e-03, -4.3640e-03,  ..., -3.6774e-03,\n",
       "                        3.6163e-03,  3.6812e-04],\n",
       "                      [-9.0790e-04,  4.5471e-03,  3.7994e-03,  ..., -4.6082e-03,\n",
       "                        4.4823e-04, -3.0518e-03],\n",
       "                      [ 2.6550e-03, -3.9978e-03, -1.5736e-04,  ..., -2.5330e-03,\n",
       "                       -5.9128e-04,  7.5817e-05],\n",
       "                      ...,\n",
       "                      [ 4.5471e-03, -1.0193e-02,  9.2316e-04,  ..., -2.1839e-04,\n",
       "                       -2.2278e-03,  6.4392e-03],\n",
       "                      [-5.5237e-03, -2.1057e-03,  6.8359e-03,  ...,  9.8267e-03,\n",
       "                       -6.3477e-03,  1.9684e-03],\n",
       "                      [ 4.2419e-03,  9.0942e-03,  3.0708e-04,  ..., -1.0147e-03,\n",
       "                        4.7302e-04, -8.4839e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.17.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0014,  0.0045, -0.0005,  ..., -0.0056,  0.0020,  0.0049],\n",
       "                      [-0.0003, -0.0009, -0.0005,  ..., -0.0017,  0.0059,  0.0041],\n",
       "                      [-0.0025, -0.0008,  0.0007,  ..., -0.0070,  0.0014, -0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0004,  0.0029,  0.0015,  ..., -0.0055,  0.0019,  0.0009],\n",
       "                      [-0.0045,  0.0020,  0.0012,  ..., -0.0024, -0.0002, -0.0002],\n",
       "                      [-0.0043,  0.0003,  0.0016,  ..., -0.0040,  0.0053,  0.0005]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.17.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0003, -0.0006, -0.0032,  ...,  0.0042,  0.0016,  0.0022],\n",
       "                      [ 0.0016, -0.0013,  0.0005,  ..., -0.0010, -0.0032, -0.0021],\n",
       "                      [ 0.0002, -0.0017,  0.0022,  ..., -0.0021,  0.0036,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0030, -0.0046, -0.0026,  ...,  0.0062,  0.0038, -0.0029],\n",
       "                      [ 0.0025,  0.0002, -0.0039,  ...,  0.0026,  0.0010, -0.0029],\n",
       "                      [ 0.0042,  0.0026, -0.0027,  ..., -0.0035, -0.0029, -0.0051]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.17.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0051,  0.0039,  0.0010,  ...,  0.0034, -0.0023,  0.0028],\n",
       "                      [-0.0016,  0.0004, -0.0004,  ..., -0.0009, -0.0038,  0.0016],\n",
       "                      [-0.0035, -0.0016,  0.0013,  ...,  0.0041,  0.0012, -0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0017,  0.0007,  0.0004,  ..., -0.0021,  0.0021,  0.0005],\n",
       "                      [ 0.0048,  0.0026, -0.0026,  ...,  0.0085, -0.0044, -0.0025],\n",
       "                      [-0.0050,  0.0024,  0.0029,  ..., -0.0031,  0.0005,  0.0005]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.17.mlp.up_proj.weight',\n",
       "              tensor([[-0.0012,  0.0037,  0.0022,  ..., -0.0003,  0.0045, -0.0043],\n",
       "                      [ 0.0050,  0.0084, -0.0038,  ...,  0.0012,  0.0063, -0.0031],\n",
       "                      [-0.0016, -0.0033,  0.0036,  ...,  0.0016,  0.0006,  0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0011, -0.0014, -0.0069,  ...,  0.0024, -0.0015,  0.0004],\n",
       "                      [-0.0027,  0.0020,  0.0043,  ..., -0.0034,  0.0003, -0.0041],\n",
       "                      [-0.0006,  0.0018,  0.0001,  ..., -0.0063, -0.0053, -0.0016]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.17.mlp.down_proj.weight',\n",
       "              tensor([[ 3.1738e-03, -1.7624e-03,  2.4872e-03,  ...,  2.2888e-03,\n",
       "                       -3.0975e-03, -3.0670e-03],\n",
       "                      [ 3.3112e-03, -1.6556e-03,  6.8359e-03,  ...,  5.8365e-04,\n",
       "                       -2.2125e-03,  1.0986e-03],\n",
       "                      [ 4.7493e-04, -1.3275e-03, -2.4567e-03,  ..., -2.8610e-04,\n",
       "                        5.7983e-03, -4.8637e-04],\n",
       "                      ...,\n",
       "                      [-4.7493e-04, -1.4267e-03, -3.5095e-04,  ..., -4.1809e-03,\n",
       "                       -3.8605e-03, -1.7395e-03],\n",
       "                      [-1.1215e-03, -2.5940e-03, -1.5793e-03,  ...,  9.7752e-05,\n",
       "                        6.4087e-04, -1.1139e-03],\n",
       "                      [-2.4719e-03,  3.4180e-03, -1.9302e-03,  ...,  2.0905e-03,\n",
       "                       -1.7853e-03,  1.8768e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.17.input_layernorm.weight',\n",
       "              tensor([2.0781, 3.5312, 2.3906,  ..., 1.8203, 1.9375, 2.1406],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.17.post_attention_layernorm.weight',\n",
       "              tensor([2.3125, 4.4375, 2.6719,  ..., 2.1562, 2.3125, 2.3281],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.18.self_attn.q_proj.weight',\n",
       "              tensor([[ 3.1891e-03, -2.3804e-03, -5.9891e-04,  ...,  8.8882e-04,\n",
       "                        2.2736e-03, -4.5967e-04],\n",
       "                      [ 1.7242e-03,  2.5482e-03,  7.2098e-04,  ..., -8.2016e-04,\n",
       "                       -1.9312e-05,  1.5945e-03],\n",
       "                      [ 1.2512e-03, -2.4109e-03,  5.8746e-04,  ...,  2.0142e-03,\n",
       "                        9.0790e-04, -1.1826e-03],\n",
       "                      ...,\n",
       "                      [-3.0518e-03,  6.8970e-03, -6.7749e-03,  ...,  1.1978e-03,\n",
       "                       -4.1962e-04, -3.3951e-04],\n",
       "                      [ 2.7924e-03,  2.4719e-03,  1.4877e-04,  ..., -4.8218e-03,\n",
       "                       -4.6387e-03, -1.4954e-03],\n",
       "                      [ 2.1362e-04,  5.8594e-03,  7.0496e-03,  ...,  6.9275e-03,\n",
       "                        1.9073e-03,  1.6632e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.18.self_attn.k_proj.weight',\n",
       "              tensor([[-1.3885e-03,  1.1292e-03,  4.7684e-04,  ..., -1.2512e-03,\n",
       "                        1.2589e-03, -1.3275e-03],\n",
       "                      [ 2.3603e-05,  8.8215e-05, -2.0752e-03,  ..., -2.7657e-04,\n",
       "                       -2.7771e-03,  1.4725e-03],\n",
       "                      [-4.9210e-04,  9.9182e-04, -7.4387e-05,  ..., -8.6212e-04,\n",
       "                       -1.0147e-03,  3.0365e-03],\n",
       "                      ...,\n",
       "                      [-3.4943e-03, -9.9182e-04, -2.8687e-03,  ..., -8.5831e-04,\n",
       "                        4.9133e-03, -9.5215e-03],\n",
       "                      [ 4.5776e-04,  7.4005e-04,  4.7913e-03,  ..., -2.2125e-03,\n",
       "                       -6.6528e-03, -8.8882e-04],\n",
       "                      [ 6.5002e-03, -2.8534e-03,  3.4180e-03,  ...,  1.0132e-02,\n",
       "                       -1.0986e-02, -2.1820e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.18.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0003,  0.0006,  0.0009,  ...,  0.0037, -0.0010,  0.0019],\n",
       "                      [ 0.0008, -0.0006,  0.0011,  ...,  0.0006,  0.0014,  0.0003],\n",
       "                      [-0.0030,  0.0011,  0.0006,  ...,  0.0013,  0.0019,  0.0035],\n",
       "                      ...,\n",
       "                      [ 0.0014,  0.0022,  0.0035,  ..., -0.0021,  0.0016, -0.0029],\n",
       "                      [ 0.0030,  0.0010,  0.0040,  ..., -0.0017, -0.0033,  0.0012],\n",
       "                      [-0.0018, -0.0015,  0.0006,  ...,  0.0019,  0.0003, -0.0005]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.18.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0051,  0.0023,  0.0032,  ..., -0.0026,  0.0014, -0.0029],\n",
       "                      [ 0.0003, -0.0010, -0.0024,  ..., -0.0001,  0.0011,  0.0056],\n",
       "                      [-0.0008,  0.0018, -0.0018,  ...,  0.0016,  0.0024, -0.0045],\n",
       "                      ...,\n",
       "                      [ 0.0051, -0.0012, -0.0003,  ..., -0.0008, -0.0013,  0.0011],\n",
       "                      [ 0.0017,  0.0009, -0.0034,  ..., -0.0003,  0.0007, -0.0027],\n",
       "                      [-0.0007, -0.0014, -0.0083,  ..., -0.0045,  0.0024, -0.0026]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.18.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0013,  0.0029,  0.0031,  ...,  0.0041,  0.0010, -0.0012],\n",
       "                      [ 0.0006, -0.0007, -0.0008,  ..., -0.0010,  0.0019, -0.0021],\n",
       "                      [ 0.0064,  0.0002,  0.0023,  ...,  0.0013, -0.0032,  0.0018],\n",
       "                      ...,\n",
       "                      [-0.0033, -0.0021,  0.0019,  ...,  0.0108,  0.0028, -0.0028],\n",
       "                      [ 0.0010, -0.0008,  0.0021,  ...,  0.0005,  0.0004, -0.0013],\n",
       "                      [-0.0012,  0.0082,  0.0011,  ..., -0.0016,  0.0035,  0.0024]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.18.mlp.up_proj.weight',\n",
       "              tensor([[-0.0007,  0.0019,  0.0042,  ...,  0.0059,  0.0019, -0.0004],\n",
       "                      [-0.0007,  0.0003,  0.0010,  ...,  0.0003,  0.0020,  0.0006],\n",
       "                      [-0.0059,  0.0011, -0.0012,  ...,  0.0030,  0.0004, -0.0052],\n",
       "                      ...,\n",
       "                      [ 0.0007, -0.0057,  0.0002,  ..., -0.0005, -0.0013,  0.0011],\n",
       "                      [ 0.0032, -0.0004,  0.0029,  ..., -0.0011, -0.0003, -0.0027],\n",
       "                      [ 0.0006, -0.0020, -0.0019,  ..., -0.0012, -0.0041,  0.0014]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.18.mlp.down_proj.weight',\n",
       "              tensor([[-0.0031, -0.0049, -0.0035,  ...,  0.0036, -0.0027,  0.0017],\n",
       "                      [ 0.0045, -0.0026,  0.0031,  ..., -0.0046, -0.0014,  0.0018],\n",
       "                      [ 0.0002, -0.0061, -0.0002,  ...,  0.0004, -0.0031,  0.0003],\n",
       "                      ...,\n",
       "                      [ 0.0017, -0.0020, -0.0005,  ..., -0.0006,  0.0016, -0.0006],\n",
       "                      [-0.0053, -0.0001, -0.0005,  ...,  0.0008,  0.0026, -0.0023],\n",
       "                      [ 0.0053,  0.0025, -0.0028,  ...,  0.0006, -0.0061,  0.0010]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.18.input_layernorm.weight',\n",
       "              tensor([2.2812, 3.4531, 2.6406,  ..., 2.1094, 2.2500, 2.3594],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.18.post_attention_layernorm.weight',\n",
       "              tensor([2.4844, 5.0938, 2.7656,  ..., 2.3125, 2.4375, 2.4375],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.19.self_attn.q_proj.weight',\n",
       "              tensor([[-1.5259e-03,  1.2741e-03,  3.4027e-03,  ...,  2.4719e-03,\n",
       "                       -6.2180e-04, -1.2207e-03],\n",
       "                      [ 2.4567e-03,  2.4567e-03, -1.2283e-03,  ..., -1.0443e-04,\n",
       "                       -4.7684e-04,  4.3335e-03],\n",
       "                      [ 3.6926e-03, -8.5449e-03, -2.3804e-03,  ...,  2.2583e-03,\n",
       "                        6.4697e-03,  4.5166e-03],\n",
       "                      ...,\n",
       "                      [ 5.3024e-04,  7.3242e-03,  6.5918e-03,  ..., -2.7618e-03,\n",
       "                        6.7902e-04,  2.6855e-03],\n",
       "                      [-3.0365e-03,  9.4986e-04,  4.3640e-03,  ..., -4.9744e-03,\n",
       "                        4.0627e-04,  2.6245e-03],\n",
       "                      [ 1.5869e-03,  7.4768e-03,  2.9449e-03,  ..., -4.6968e-05,\n",
       "                        1.8845e-03,  2.1820e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.19.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0051,  0.0074,  0.0026,  ...,  0.0067,  0.0013, -0.0019],\n",
       "                      [-0.0004, -0.0057,  0.0042,  ..., -0.0005, -0.0024, -0.0002],\n",
       "                      [-0.0033, -0.0061, -0.0058,  ...,  0.0052, -0.0021,  0.0004],\n",
       "                      ...,\n",
       "                      [ 0.0020,  0.0023,  0.0036,  ...,  0.0010, -0.0016, -0.0021],\n",
       "                      [ 0.0009, -0.0149,  0.0061,  ...,  0.0045, -0.0079, -0.0109],\n",
       "                      [ 0.0018, -0.0012, -0.0043,  ..., -0.0089,  0.0026, -0.0068]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.19.self_attn.v_proj.weight',\n",
       "              tensor([[-2.9297e-03, -3.6163e-03,  2.3956e-03,  ...,  9.0408e-04,\n",
       "                       -4.4632e-04,  1.8616e-03],\n",
       "                      [ 1.3065e-04, -1.9531e-03,  2.6093e-03,  ..., -2.9755e-03,\n",
       "                       -1.7929e-03,  2.1973e-03],\n",
       "                      [-7.4005e-04, -4.5471e-03, -3.5095e-03,  ...,  6.2866e-03,\n",
       "                       -6.1798e-04,  6.0272e-04],\n",
       "                      ...,\n",
       "                      [-1.2131e-03,  6.1035e-04,  3.7842e-03,  ...,  9.7275e-04,\n",
       "                       -8.3618e-03,  1.5488e-03],\n",
       "                      [ 1.5182e-03,  7.0953e-04, -2.4567e-03,  ...,  4.4060e-04,\n",
       "                        3.2959e-03, -3.7537e-03],\n",
       "                      [ 1.0071e-03, -3.0823e-03, -3.7537e-03,  ..., -2.0752e-03,\n",
       "                        8.4877e-05, -5.5695e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.19.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0016, -0.0012, -0.0041,  ...,  0.0028,  0.0056,  0.0025],\n",
       "                      [ 0.0036,  0.0022, -0.0034,  ..., -0.0010,  0.0018, -0.0010],\n",
       "                      [ 0.0015, -0.0044,  0.0022,  ...,  0.0027,  0.0010, -0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0023, -0.0010, -0.0075,  ..., -0.0006, -0.0014, -0.0027],\n",
       "                      [-0.0012, -0.0002,  0.0038,  ..., -0.0009, -0.0022,  0.0022],\n",
       "                      [ 0.0044, -0.0005,  0.0024,  ..., -0.0015,  0.0009, -0.0014]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.19.mlp.gate_proj.weight',\n",
       "              tensor([[ 3.1281e-03, -3.0823e-03, -2.3956e-03,  ..., -8.0490e-04,\n",
       "                       -6.4392e-03, -3.7537e-03],\n",
       "                      [-1.2970e-04, -6.0272e-04, -1.1978e-03,  ..., -4.3640e-03,\n",
       "                        2.4414e-03, -3.7079e-03],\n",
       "                      [ 4.4861e-03,  8.5449e-03,  3.6049e-04,  ..., -8.7891e-03,\n",
       "                        2.1210e-03,  3.5858e-03],\n",
       "                      ...,\n",
       "                      [-3.8757e-03, -2.8992e-04,  4.5471e-03,  ..., -2.5177e-03,\n",
       "                       -6.9885e-03,  2.7657e-04],\n",
       "                      [-3.0670e-03,  1.1749e-03, -2.7924e-03,  ..., -7.0953e-04,\n",
       "                       -9.1934e-04,  1.8921e-03],\n",
       "                      [ 2.4414e-03,  1.2970e-03, -3.4180e-03,  ...,  7.9632e-05,\n",
       "                       -1.1063e-03,  1.6251e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.19.mlp.up_proj.weight',\n",
       "              tensor([[-5.0659e-03, -4.6692e-03, -4.5776e-03,  ...,  1.3809e-03,\n",
       "                        3.2196e-03, -3.7384e-03],\n",
       "                      [-1.7014e-03, -2.2316e-04, -7.0801e-03,  ...,  8.0490e-04,\n",
       "                       -1.2894e-03,  2.7771e-03],\n",
       "                      [-2.2888e-03, -5.0659e-03,  1.2436e-03,  ...,  3.5706e-03,\n",
       "                       -4.6387e-03, -1.1978e-03],\n",
       "                      ...,\n",
       "                      [ 1.5793e-03, -7.5150e-04, -1.4420e-03,  ...,  7.8735e-03,\n",
       "                       -8.4686e-04, -3.2501e-03],\n",
       "                      [-3.0670e-03, -7.4005e-04,  1.0254e-02,  ..., -2.4261e-03,\n",
       "                       -3.8300e-03, -1.4019e-04],\n",
       "                      [ 4.1164e-07, -2.0218e-04, -4.4556e-03,  ..., -1.1444e-03,\n",
       "                        4.7607e-03, -1.2436e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.19.mlp.down_proj.weight',\n",
       "              tensor([[-0.0016,  0.0082, -0.0051,  ...,  0.0021, -0.0020,  0.0003],\n",
       "                      [-0.0029, -0.0041, -0.0019,  ..., -0.0002,  0.0021,  0.0022],\n",
       "                      [-0.0009, -0.0033, -0.0011,  ..., -0.0042,  0.0032, -0.0062],\n",
       "                      ...,\n",
       "                      [ 0.0006,  0.0034, -0.0002,  ..., -0.0004, -0.0006, -0.0040],\n",
       "                      [ 0.0012,  0.0011, -0.0036,  ...,  0.0024,  0.0004,  0.0064],\n",
       "                      [-0.0046, -0.0003, -0.0012,  ..., -0.0021,  0.0046,  0.0022]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.19.input_layernorm.weight',\n",
       "              tensor([2.4219, 3.2656, 2.7031,  ..., 2.2500, 2.2812, 2.4375],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.19.post_attention_layernorm.weight',\n",
       "              tensor([2.6250, 3.7031, 2.9062,  ..., 2.5000, 2.6562, 2.6094],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.20.self_attn.q_proj.weight',\n",
       "              tensor([[-2.8534e-03, -4.5166e-03, -3.2349e-03,  ...,  3.3722e-03,\n",
       "                       -3.8338e-04, -1.4191e-03],\n",
       "                      [ 1.6174e-03,  2.4986e-04, -2.8992e-04,  ...,  2.0294e-03,\n",
       "                       -2.7008e-03,  2.8839e-03],\n",
       "                      [-1.4572e-03,  3.1586e-03, -2.7161e-03,  ...,  3.2043e-04,\n",
       "                       -2.4414e-03, -7.5989e-03],\n",
       "                      ...,\n",
       "                      [-1.8921e-03, -1.2684e-04, -4.3030e-03,  ..., -2.2278e-03,\n",
       "                       -5.9366e-05,  4.2114e-03],\n",
       "                      [ 8.2779e-04,  3.1433e-03,  1.6556e-03,  ...,  1.3199e-03,\n",
       "                        2.0599e-03,  1.8539e-03],\n",
       "                      [ 3.1281e-03,  1.3046e-03,  2.5482e-03,  ...,  1.1368e-03,\n",
       "                        4.2725e-03, -1.5106e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.20.self_attn.k_proj.weight',\n",
       "              tensor([[-4.6692e-03, -4.3945e-03, -4.8828e-03,  ...,  4.2114e-03,\n",
       "                       -2.4109e-03, -1.1444e-03],\n",
       "                      [ 6.4373e-05,  2.1362e-03, -1.8616e-03,  ...,  2.4414e-03,\n",
       "                       -1.2054e-03, -8.0490e-04],\n",
       "                      [ 1.4420e-03,  4.4250e-03, -4.3869e-04,  ..., -2.0752e-03,\n",
       "                       -2.9297e-03, -4.3335e-03],\n",
       "                      ...,\n",
       "                      [ 8.4305e-04,  4.5776e-03,  2.4109e-03,  ..., -4.9133e-03,\n",
       "                        4.3030e-03, -2.4109e-03],\n",
       "                      [-1.9150e-03,  9.4604e-04,  1.3580e-03,  ..., -3.1586e-03,\n",
       "                        6.7749e-03, -7.3242e-04],\n",
       "                      [-4.0627e-04, -4.7607e-03, -1.5564e-03,  ...,  7.3547e-03,\n",
       "                       -2.7466e-03, -4.2915e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.20.self_attn.v_proj.weight',\n",
       "              tensor([[-1.1292e-03,  2.3956e-03,  9.9945e-04,  ...,  1.0073e-05,\n",
       "                        1.2665e-03, -1.3199e-03],\n",
       "                      [ 9.9182e-04, -1.1826e-03,  4.2114e-03,  ...,  4.1389e-04,\n",
       "                       -1.4496e-03,  3.0708e-04],\n",
       "                      [ 3.1738e-03, -2.0905e-03,  1.4038e-03,  ...,  1.6708e-03,\n",
       "                        1.3580e-03,  1.0757e-03],\n",
       "                      ...,\n",
       "                      [ 9.2697e-04,  2.4719e-03, -2.0447e-03,  ...,  1.5564e-03,\n",
       "                       -2.3804e-03, -2.9449e-03],\n",
       "                      [-1.6098e-03, -5.9891e-04, -1.8311e-03,  ..., -1.4114e-03,\n",
       "                        5.4626e-03,  9.2697e-04],\n",
       "                      [ 1.1826e-03,  2.0142e-03, -1.6708e-03,  ..., -1.2741e-03,\n",
       "                        5.8594e-03, -2.5024e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.20.self_attn.o_proj.weight',\n",
       "              tensor([[-1.0681e-03, -2.4109e-03, -6.5231e-04,  ..., -5.7602e-04,\n",
       "                       -7.4005e-04, -4.2725e-03],\n",
       "                      [ 1.4400e-04,  3.7670e-05, -2.6703e-03,  ..., -1.0452e-03,\n",
       "                       -4.1809e-03,  6.8665e-04],\n",
       "                      [-9.8419e-04, -2.2125e-03,  1.9684e-03,  ..., -1.4267e-03,\n",
       "                       -2.0447e-03, -6.3705e-04],\n",
       "                      ...,\n",
       "                      [-2.3956e-03,  1.1444e-03, -9.4986e-04,  ...,  3.3722e-03,\n",
       "                        6.4850e-04,  1.4420e-03],\n",
       "                      [-3.4523e-04, -1.3504e-03,  3.4027e-03,  ..., -2.2697e-04,\n",
       "                       -6.1340e-03,  3.0823e-03],\n",
       "                      [-3.6316e-03, -2.4261e-03,  3.3264e-03,  ..., -1.1520e-03,\n",
       "                       -3.1586e-03,  1.7014e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.20.mlp.gate_proj.weight',\n",
       "              tensor([[-1.1368e-03, -3.2654e-03,  4.1199e-03,  ..., -4.4556e-03,\n",
       "                       -7.1716e-04,  1.3649e-05],\n",
       "                      [ 6.4468e-04,  1.9989e-03, -2.6855e-03,  ..., -2.3041e-03,\n",
       "                        3.0365e-03, -1.4725e-03],\n",
       "                      [-1.8883e-04, -8.6784e-05,  4.7913e-03,  ..., -4.5471e-03,\n",
       "                        2.4414e-03,  2.2278e-03],\n",
       "                      ...,\n",
       "                      [-3.2806e-03,  1.1215e-03,  3.6316e-03,  ...,  2.1057e-03,\n",
       "                        5.7068e-03,  4.3030e-03],\n",
       "                      [ 2.1267e-04,  6.3705e-04, -8.0585e-05,  ..., -1.2131e-03,\n",
       "                        5.1270e-03, -2.6855e-03],\n",
       "                      [ 2.3193e-03,  1.9073e-03, -3.8605e-03,  ..., -1.9836e-03,\n",
       "                       -3.3569e-03, -1.1902e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.20.mlp.up_proj.weight',\n",
       "              tensor([[ 7.1335e-04,  2.2430e-03, -2.6550e-03,  ...,  8.0585e-05,\n",
       "                       -2.7313e-03, -7.1716e-04],\n",
       "                      [ 3.2349e-03,  1.8616e-03,  3.0899e-04,  ...,  3.0899e-04,\n",
       "                        4.6997e-03, -9.5749e-04],\n",
       "                      [ 4.0588e-03,  5.1880e-03, -2.5330e-03,  ...,  2.1744e-04,\n",
       "                       -1.1520e-03,  1.0300e-03],\n",
       "                      ...,\n",
       "                      [-2.0599e-03, -1.0605e-03, -1.1749e-03,  ...,  1.1826e-03,\n",
       "                       -1.3351e-04,  6.6376e-04],\n",
       "                      [-2.2583e-03, -1.9302e-03, -2.6550e-03,  ...,  1.0223e-03,\n",
       "                       -1.3046e-03, -3.0670e-03],\n",
       "                      [ 3.2959e-03,  2.2583e-03,  1.8387e-03,  ...,  1.0157e-04,\n",
       "                        5.6839e-04,  3.3569e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.20.mlp.down_proj.weight',\n",
       "              tensor([[ 2.9445e-05, -5.5695e-04,  3.2196e-03,  ..., -6.5308e-03,\n",
       "                        4.0588e-03,  2.8381e-03],\n",
       "                      [-1.0147e-03,  2.5330e-03, -1.8463e-03,  ...,  5.5313e-04,\n",
       "                        3.2196e-03, -3.5667e-04],\n",
       "                      [ 8.2779e-04,  1.2894e-03,  9.3842e-04,  ...,  1.8924e-06,\n",
       "                       -6.8188e-05,  4.5586e-04],\n",
       "                      ...,\n",
       "                      [ 1.6327e-03, -4.2114e-03, -1.3504e-03,  ..., -3.1738e-03,\n",
       "                        1.6098e-03, -9.9182e-04],\n",
       "                      [-6.2943e-04,  1.2589e-03, -5.9891e-04,  ..., -1.0071e-03,\n",
       "                       -2.8839e-03, -1.7643e-04],\n",
       "                      [ 3.1433e-03, -1.4725e-03,  1.0757e-03,  ...,  1.5793e-03,\n",
       "                       -2.5482e-03,  3.7842e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.20.input_layernorm.weight',\n",
       "              tensor([2.5781, 3.3281, 2.4844,  ..., 2.2500, 2.3750, 2.3594],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.20.post_attention_layernorm.weight',\n",
       "              tensor([2.8594, 4.6562, 3.0625,  ..., 2.7500, 2.8906, 2.8125],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.21.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0052,  0.0034,  0.0022,  ..., -0.0026,  0.0028,  0.0014],\n",
       "                      [-0.0010,  0.0007,  0.0010,  ..., -0.0026, -0.0006,  0.0004],\n",
       "                      [-0.0049,  0.0015, -0.0003,  ..., -0.0018, -0.0007,  0.0019],\n",
       "                      ...,\n",
       "                      [-0.0013,  0.0011,  0.0002,  ..., -0.0052,  0.0030, -0.0040],\n",
       "                      [-0.0016,  0.0029, -0.0007,  ..., -0.0078,  0.0006, -0.0027],\n",
       "                      [ 0.0086,  0.0083, -0.0007,  ..., -0.0040, -0.0045,  0.0013]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.21.self_attn.k_proj.weight',\n",
       "              tensor([[ 3.9673e-03,  2.7275e-04, -2.2736e-03,  ...,  1.0071e-03,\n",
       "                        5.7678e-03, -1.1902e-03],\n",
       "                      [-7.7438e-04, -1.5869e-03,  1.6556e-03,  ..., -3.5248e-03,\n",
       "                        1.2436e-03,  1.5564e-03],\n",
       "                      [-4.4823e-04,  1.6117e-04, -2.6398e-03,  ...,  5.5695e-04,\n",
       "                        2.0905e-03,  2.6321e-04],\n",
       "                      ...,\n",
       "                      [-3.9673e-03, -7.7515e-03,  1.7776e-03,  ...,  1.8845e-03,\n",
       "                        9.9182e-04, -1.9684e-03],\n",
       "                      [-6.2561e-03,  2.1362e-03, -1.3580e-03,  ..., -8.6060e-03,\n",
       "                       -5.7678e-03,  1.9836e-03],\n",
       "                      [ 5.5847e-03, -3.3112e-03, -2.5392e-05,  ...,  2.9602e-03,\n",
       "                        5.7983e-03,  9.1934e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.21.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0018,  0.0003,  0.0014,  ...,  0.0036,  0.0004, -0.0032],\n",
       "                      [ 0.0021,  0.0025, -0.0028,  ...,  0.0020, -0.0027,  0.0033],\n",
       "                      [ 0.0014, -0.0052, -0.0004,  ..., -0.0045, -0.0030, -0.0055],\n",
       "                      ...,\n",
       "                      [-0.0027,  0.0007, -0.0014,  ...,  0.0037,  0.0018,  0.0006],\n",
       "                      [-0.0028,  0.0004, -0.0006,  ..., -0.0023, -0.0015, -0.0017],\n",
       "                      [-0.0004,  0.0005, -0.0007,  ...,  0.0014,  0.0002,  0.0034]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.21.self_attn.o_proj.weight',\n",
       "              tensor([[-2.3937e-04,  2.3651e-03, -1.0834e-03,  ..., -2.3041e-03,\n",
       "                       -3.7193e-04,  1.5259e-04],\n",
       "                      [-2.6321e-04, -1.8845e-03, -2.8534e-03,  ...,  1.4725e-03,\n",
       "                       -5.1117e-04,  1.9302e-03],\n",
       "                      [-7.0190e-04,  1.6327e-03, -2.9564e-04,  ...,  1.1292e-03,\n",
       "                        7.5531e-04,  3.7689e-03],\n",
       "                      ...,\n",
       "                      [ 5.3406e-04, -1.4267e-03, -6.5613e-03,  ...,  2.9602e-03,\n",
       "                        7.4768e-03, -7.3433e-05],\n",
       "                      [ 2.2736e-03,  1.8692e-03,  3.0212e-03,  ...,  2.0447e-03,\n",
       "                        3.9062e-03,  6.2256e-03],\n",
       "                      [ 3.2654e-03,  4.3335e-03,  3.1891e-03,  ..., -2.8229e-03,\n",
       "                        9.3842e-04, -9.4223e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.21.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0010, -0.0043, -0.0005,  ...,  0.0015, -0.0022,  0.0033],\n",
       "                      [-0.0005,  0.0036,  0.0009,  ...,  0.0049,  0.0035,  0.0008],\n",
       "                      [-0.0042, -0.0053, -0.0010,  ..., -0.0002, -0.0023, -0.0044],\n",
       "                      ...,\n",
       "                      [-0.0020,  0.0030, -0.0024,  ..., -0.0022,  0.0011,  0.0039],\n",
       "                      [-0.0085, -0.0003, -0.0008,  ..., -0.0003, -0.0020,  0.0013],\n",
       "                      [-0.0063, -0.0049, -0.0006,  ..., -0.0019, -0.0026, -0.0097]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.21.mlp.up_proj.weight',\n",
       "              tensor([[-9.5367e-04, -6.1798e-04,  4.1809e-03,  ..., -3.1128e-03,\n",
       "                        2.7084e-04,  4.6387e-03],\n",
       "                      [-8.6975e-04, -1.3428e-03,  9.0408e-04,  ..., -1.8692e-03,\n",
       "                        2.5482e-03,  1.6251e-03],\n",
       "                      [ 6.4392e-03,  1.3809e-03,  5.5542e-03,  ...,  2.9602e-03,\n",
       "                        2.5024e-03,  1.7014e-03],\n",
       "                      ...,\n",
       "                      [ 7.2937e-03, -1.0071e-03, -5.0049e-03,  ...,  4.6997e-03,\n",
       "                       -4.4556e-03,  1.9531e-03],\n",
       "                      [-4.6997e-03, -4.6349e-04, -5.7068e-03,  ..., -1.1826e-03,\n",
       "                       -2.8801e-04, -5.4169e-04],\n",
       "                      [ 1.6880e-04,  3.9978e-03,  2.8229e-03,  ..., -3.8910e-03,\n",
       "                       -2.2430e-03, -3.0518e-05]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.21.mlp.down_proj.weight',\n",
       "              tensor([[-0.0034,  0.0025,  0.0034,  ...,  0.0029,  0.0026,  0.0013],\n",
       "                      [-0.0050,  0.0016, -0.0020,  ..., -0.0009,  0.0003,  0.0011],\n",
       "                      [ 0.0046,  0.0017,  0.0037,  ..., -0.0035, -0.0024,  0.0061],\n",
       "                      ...,\n",
       "                      [-0.0017, -0.0020, -0.0014,  ..., -0.0053,  0.0017, -0.0038],\n",
       "                      [-0.0027,  0.0007,  0.0045,  ..., -0.0049,  0.0006,  0.0004],\n",
       "                      [ 0.0024, -0.0010, -0.0060,  ..., -0.0007,  0.0004,  0.0018]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.21.input_layernorm.weight',\n",
       "              tensor([2.4062, 2.7812, 2.5000,  ..., 2.2188, 2.3125, 2.4062],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.21.post_attention_layernorm.weight',\n",
       "              tensor([3.0781, 3.9062, 3.3281,  ..., 2.9688, 3.1094, 3.0469],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.22.self_attn.q_proj.weight',\n",
       "              tensor([[ 1.4496e-03, -3.2959e-03, -1.2360e-03,  ...,  3.9978e-03,\n",
       "                        2.1057e-03,  7.1106e-03],\n",
       "                      [-6.0730e-03, -2.6245e-03, -2.3041e-03,  ..., -5.7678e-03,\n",
       "                       -4.1809e-03, -3.0518e-05],\n",
       "                      [ 3.7994e-03,  5.7220e-04,  8.6212e-04,  ..., -3.8605e-03,\n",
       "                        3.5400e-03,  4.2343e-04],\n",
       "                      ...,\n",
       "                      [ 9.6893e-04,  8.5831e-04, -4.0588e-03,  ...,  9.7656e-04,\n",
       "                        3.0670e-03,  2.5787e-03],\n",
       "                      [-3.6011e-03, -3.3112e-03, -8.8501e-04,  ..., -3.0060e-03,\n",
       "                       -6.2866e-03,  3.0212e-03],\n",
       "                      [-3.7384e-03,  3.7231e-03, -2.6398e-03,  ...,  4.4556e-03,\n",
       "                        2.2507e-04, -3.8338e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.22.self_attn.k_proj.weight',\n",
       "              tensor([[-1.9646e-04,  1.0071e-03, -2.1076e-04,  ..., -2.4872e-03,\n",
       "                        3.8452e-03, -1.5564e-03],\n",
       "                      [ 4.5013e-04,  3.8719e-04,  5.0354e-03,  ...,  4.8447e-04,\n",
       "                        1.7624e-03,  3.0365e-03],\n",
       "                      [ 6.7139e-04, -5.3406e-04,  2.4719e-03,  ..., -4.5586e-04,\n",
       "                        1.0757e-03, -7.4387e-04],\n",
       "                      ...,\n",
       "                      [-1.2512e-03,  2.2173e-05, -2.8992e-03,  ..., -9.2773e-03,\n",
       "                        2.3603e-05,  6.7139e-04],\n",
       "                      [ 8.0566e-03, -5.0964e-03,  2.7466e-04,  ..., -8.0872e-04,\n",
       "                       -6.4087e-03,  2.6093e-03],\n",
       "                      [-3.1891e-03,  2.3346e-03,  1.1368e-03,  ...,  2.1820e-03,\n",
       "                       -3.0212e-03,  2.0142e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.22.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0047, -0.0042, -0.0026,  ...,  0.0002,  0.0023,  0.0013],\n",
       "                      [ 0.0018,  0.0004, -0.0006,  ..., -0.0023,  0.0042,  0.0081],\n",
       "                      [-0.0019, -0.0016,  0.0022,  ...,  0.0010, -0.0026, -0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0042, -0.0025,  0.0025,  ...,  0.0029,  0.0021,  0.0003],\n",
       "                      [ 0.0003,  0.0012, -0.0031,  ...,  0.0069, -0.0010,  0.0027],\n",
       "                      [-0.0003,  0.0006, -0.0025,  ...,  0.0004, -0.0036,  0.0013]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.22.self_attn.o_proj.weight',\n",
       "              tensor([[-2.8992e-04, -1.6785e-03,  5.4626e-03,  ..., -1.1826e-03,\n",
       "                        1.5335e-03,  9.2697e-04],\n",
       "                      [ 2.7275e-04, -5.2643e-04, -2.0599e-03,  ...,  2.5787e-03,\n",
       "                       -1.3962e-03,  1.3275e-03],\n",
       "                      [-3.2654e-03, -1.4191e-03, -1.5869e-03,  ...,  6.3477e-03,\n",
       "                       -1.1215e-03, -1.3504e-03],\n",
       "                      ...,\n",
       "                      [ 8.5449e-04, -2.7924e-03, -7.4768e-04,  ...,  1.9989e-03,\n",
       "                        2.7618e-03,  2.5482e-03],\n",
       "                      [-3.1281e-03,  6.6833e-03,  2.4109e-03,  ...,  3.6469e-03,\n",
       "                        8.6212e-04, -6.7902e-04],\n",
       "                      [-4.9210e-04,  8.9722e-03,  3.6469e-03,  ...,  1.3962e-03,\n",
       "                        3.8300e-03, -2.4080e-05]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.22.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0048,  0.0016,  0.0028,  ...,  0.0002, -0.0017, -0.0028],\n",
       "                      [-0.0016, -0.0048,  0.0014,  ...,  0.0075, -0.0039, -0.0049],\n",
       "                      [-0.0045, -0.0021,  0.0094,  ...,  0.0036, -0.0007,  0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0004, -0.0015, -0.0016,  ...,  0.0012, -0.0003, -0.0034],\n",
       "                      [ 0.0013,  0.0025, -0.0094,  ..., -0.0023, -0.0030,  0.0013],\n",
       "                      [ 0.0023,  0.0034, -0.0001,  ..., -0.0008, -0.0013, -0.0020]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.22.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0052,  0.0009, -0.0044,  ..., -0.0048,  0.0013, -0.0014],\n",
       "                      [ 0.0063, -0.0031, -0.0029,  ...,  0.0031,  0.0017, -0.0009],\n",
       "                      [ 0.0008,  0.0006, -0.0007,  ...,  0.0023,  0.0029,  0.0038],\n",
       "                      ...,\n",
       "                      [ 0.0039,  0.0023,  0.0005,  ...,  0.0034,  0.0002, -0.0034],\n",
       "                      [-0.0003, -0.0004,  0.0011,  ...,  0.0025, -0.0002,  0.0009],\n",
       "                      [-0.0070, -0.0020,  0.0015,  ...,  0.0022,  0.0030, -0.0003]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.22.mlp.down_proj.weight',\n",
       "              tensor([[-0.0028, -0.0005,  0.0012,  ...,  0.0031,  0.0050, -0.0015],\n",
       "                      [-0.0033, -0.0022,  0.0020,  ...,  0.0067,  0.0076, -0.0011],\n",
       "                      [-0.0023, -0.0046,  0.0009,  ..., -0.0003,  0.0044,  0.0022],\n",
       "                      ...,\n",
       "                      [ 0.0011,  0.0005,  0.0026,  ..., -0.0061,  0.0030, -0.0054],\n",
       "                      [-0.0017,  0.0045,  0.0036,  ..., -0.0049, -0.0003,  0.0028],\n",
       "                      [ 0.0021,  0.0026, -0.0016,  ...,  0.0008, -0.0011,  0.0003]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.22.input_layernorm.weight',\n",
       "              tensor([2.4375, 2.5469, 2.4688,  ..., 2.3125, 2.3281, 2.4219],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.22.post_attention_layernorm.weight',\n",
       "              tensor([3.2344, 3.7500, 3.3906,  ..., 3.1094, 3.2344, 3.1562],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.23.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0060, -0.0015, -0.0015,  ..., -0.0025, -0.0010,  0.0016],\n",
       "                      [-0.0011,  0.0024,  0.0019,  ...,  0.0007,  0.0042, -0.0017],\n",
       "                      [-0.0003,  0.0040, -0.0017,  ..., -0.0029,  0.0012, -0.0014],\n",
       "                      ...,\n",
       "                      [-0.0098, -0.0044,  0.0034,  ..., -0.0056, -0.0012,  0.0005],\n",
       "                      [ 0.0018,  0.0005, -0.0019,  ...,  0.0007, -0.0022,  0.0008],\n",
       "                      [-0.0026, -0.0018,  0.0016,  ..., -0.0036, -0.0034,  0.0012]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.23.self_attn.k_proj.weight',\n",
       "              tensor([[ 2.5482e-03, -2.0599e-03, -4.8523e-03,  ...,  1.2970e-03,\n",
       "                        6.7139e-04,  5.5237e-03],\n",
       "                      [-4.3030e-03,  6.8665e-03, -1.2589e-03,  ..., -5.1270e-03,\n",
       "                       -2.7313e-03, -4.4861e-03],\n",
       "                      [-3.4904e-04,  5.0964e-03,  2.1210e-03,  ..., -2.7771e-03,\n",
       "                        4.0283e-03,  2.3651e-03],\n",
       "                      ...,\n",
       "                      [-5.7936e-05, -2.5272e-05, -4.0894e-03,  ..., -2.4414e-03,\n",
       "                       -2.9297e-03, -7.2861e-04],\n",
       "                      [ 4.1504e-03, -8.7891e-03, -3.6469e-03,  ..., -5.1270e-03,\n",
       "                        1.9379e-03, -3.2959e-03],\n",
       "                      [-1.6479e-03, -2.8229e-03,  3.0518e-03,  ...,  1.2589e-03,\n",
       "                        2.6398e-03, -8.9111e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.23.self_attn.v_proj.weight',\n",
       "              tensor([[-6.6528e-03,  2.6894e-04,  4.4861e-03,  ..., -7.8964e-04,\n",
       "                       -3.3617e-05, -4.8828e-03],\n",
       "                      [-2.0313e-04,  1.2741e-03, -4.1199e-03,  ..., -9.3579e-06,\n",
       "                        3.3417e-03,  1.6098e-03],\n",
       "                      [ 1.2493e-04, -4.9133e-03, -3.6011e-03,  ..., -7.5150e-04,\n",
       "                        8.9645e-04, -3.8757e-03],\n",
       "                      ...,\n",
       "                      [ 1.7624e-03, -9.1553e-05,  1.0452e-03,  ..., -2.4261e-03,\n",
       "                       -2.4319e-04, -3.3112e-03],\n",
       "                      [ 2.0905e-03,  6.1417e-04,  5.7068e-03,  ...,  2.3804e-03,\n",
       "                       -1.9379e-03, -2.0981e-04],\n",
       "                      [-1.4191e-03,  3.0212e-03, -7.7057e-04,  ..., -2.5177e-03,\n",
       "                        4.0283e-03, -1.2894e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.23.self_attn.o_proj.weight',\n",
       "              tensor([[ 6.6757e-04, -1.9379e-03, -2.8534e-03,  ...,  1.1902e-03,\n",
       "                        2.2736e-03, -1.6403e-03],\n",
       "                      [ 9.0027e-04, -1.6785e-03,  3.0518e-03,  ..., -3.7575e-04,\n",
       "                       -2.0599e-03, -5.1117e-04],\n",
       "                      [ 9.4223e-04,  2.2888e-03,  5.1575e-03,  ...,  7.0496e-03,\n",
       "                       -1.7853e-03,  1.1749e-03],\n",
       "                      ...,\n",
       "                      [-1.0834e-03, -7.4863e-05, -1.2512e-03,  ...,  2.1667e-03,\n",
       "                        4.6349e-04, -2.0905e-03],\n",
       "                      [ 1.7357e-04, -3.6621e-04,  5.1498e-04,  ..., -5.3787e-04,\n",
       "                        1.2207e-03, -2.7618e-03],\n",
       "                      [-8.3923e-04,  1.0452e-03, -2.6855e-03,  ..., -2.1057e-03,\n",
       "                        9.7275e-04,  3.8452e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.23.mlp.gate_proj.weight',\n",
       "              tensor([[ 4.6997e-03,  1.9836e-03,  1.1826e-03,  ...,  4.9210e-04,\n",
       "                       -5.7373e-03, -8.6975e-04],\n",
       "                      [ 3.4332e-03, -9.2773e-03,  3.1586e-03,  ...,  3.3569e-04,\n",
       "                        5.9509e-04, -1.6689e-04],\n",
       "                      [ 7.4463e-03, -3.6774e-03, -1.1978e-03,  ...,  3.3722e-03,\n",
       "                        2.7618e-03,  5.4932e-04],\n",
       "                      ...,\n",
       "                      [-3.1586e-03, -1.9073e-03,  3.6316e-03,  ...,  1.8768e-03,\n",
       "                       -2.4567e-03, -2.5177e-04],\n",
       "                      [ 1.0452e-03,  1.9531e-03,  3.1128e-03,  ...,  2.0294e-03,\n",
       "                       -2.0146e-05, -2.4261e-03],\n",
       "                      [-1.5717e-03,  4.9744e-03, -1.8954e-05,  ..., -1.7700e-03,\n",
       "                       -7.3242e-04, -5.7373e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.23.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0028,  0.0002,  0.0020,  ...,  0.0010,  0.0003,  0.0051],\n",
       "                      [-0.0004, -0.0003,  0.0045,  ..., -0.0005,  0.0033,  0.0006],\n",
       "                      [ 0.0008,  0.0046, -0.0037,  ..., -0.0029,  0.0007, -0.0010],\n",
       "                      ...,\n",
       "                      [-0.0056,  0.0005, -0.0051,  ...,  0.0052,  0.0039, -0.0005],\n",
       "                      [ 0.0016,  0.0005, -0.0029,  ...,  0.0006, -0.0006, -0.0008],\n",
       "                      [ 0.0010,  0.0029,  0.0001,  ..., -0.0060,  0.0009,  0.0021]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.23.mlp.down_proj.weight',\n",
       "              tensor([[-2.7924e-03,  2.7466e-03,  3.5667e-04,  ...,  3.3875e-03,\n",
       "                        8.2016e-04,  3.1891e-03],\n",
       "                      [-4.2419e-03, -2.0752e-03,  1.1520e-03,  ..., -3.2043e-03,\n",
       "                       -3.6001e-05,  8.4686e-04],\n",
       "                      [-4.1199e-03, -1.7166e-04, -1.1368e-03,  ..., -2.1362e-04,\n",
       "                       -2.2125e-03, -1.6632e-03],\n",
       "                      ...,\n",
       "                      [-1.9302e-03, -2.5482e-03, -2.6093e-03,  ..., -1.8997e-03,\n",
       "                       -2.3193e-03,  7.2479e-04],\n",
       "                      [-9.7656e-04, -7.5817e-05, -5.2261e-04,  ..., -7.5531e-04,\n",
       "                       -1.9150e-03,  6.1035e-03],\n",
       "                      [ 2.4605e-04, -3.5095e-03,  4.4861e-03,  ..., -9.9182e-04,\n",
       "                       -1.7700e-03, -3.0365e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.23.input_layernorm.weight',\n",
       "              tensor([2.5156, 2.6719, 2.5000,  ..., 2.3281, 2.4062, 2.4531],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.23.post_attention_layernorm.weight',\n",
       "              tensor([3.4062, 3.7969, 3.5156,  ..., 3.2344, 3.3281, 3.3281],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.24.self_attn.q_proj.weight',\n",
       "              tensor([[-1.8120e-05, -1.9989e-03,  6.6757e-04,  ...,  6.6757e-04,\n",
       "                        1.9455e-03, -1.8082e-03],\n",
       "                      [-1.3809e-03, -3.2187e-05,  4.9744e-03,  ..., -4.1199e-03,\n",
       "                       -8.1635e-04,  1.1396e-04],\n",
       "                      [ 2.4109e-03,  3.3112e-03,  5.8594e-03,  ...,  1.9684e-03,\n",
       "                       -2.4567e-03, -6.3705e-04],\n",
       "                      ...,\n",
       "                      [-3.7689e-03,  3.2196e-03,  1.8215e-04,  ...,  2.2430e-03,\n",
       "                        3.6774e-03,  8.2970e-05],\n",
       "                      [ 8.2493e-05, -1.3428e-03, -6.0120e-03,  ..., -5.6458e-03,\n",
       "                       -5.2643e-04, -5.4932e-03],\n",
       "                      [ 3.6469e-03,  6.6528e-03,  1.0986e-03,  ...,  5.1880e-04,\n",
       "                        7.6599e-03,  1.1826e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.24.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0033, -0.0038,  0.0035,  ...,  0.0037, -0.0023,  0.0019],\n",
       "                      [ 0.0011, -0.0029,  0.0008,  ...,  0.0039,  0.0013,  0.0002],\n",
       "                      [-0.0030, -0.0045, -0.0002,  ...,  0.0043,  0.0002, -0.0027],\n",
       "                      ...,\n",
       "                      [ 0.0015, -0.0007, -0.0049,  ...,  0.0054,  0.0054,  0.0067],\n",
       "                      [ 0.0089,  0.0120,  0.0012,  ...,  0.0005,  0.0142, -0.0081],\n",
       "                      [-0.0053,  0.0024, -0.0096,  ...,  0.0011, -0.0008,  0.0035]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.24.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0009,  0.0016, -0.0022,  ...,  0.0015, -0.0042, -0.0016],\n",
       "                      [ 0.0005,  0.0017, -0.0026,  ..., -0.0025, -0.0045,  0.0027],\n",
       "                      [-0.0038,  0.0019,  0.0056,  ...,  0.0038,  0.0009,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0059, -0.0065,  0.0011,  ..., -0.0011,  0.0051,  0.0007],\n",
       "                      [ 0.0020, -0.0035, -0.0029,  ...,  0.0012, -0.0003, -0.0047],\n",
       "                      [ 0.0033,  0.0036, -0.0027,  ..., -0.0003,  0.0006,  0.0021]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.24.self_attn.o_proj.weight',\n",
       "              tensor([[ 3.4790e-03, -2.9602e-03,  5.0964e-03,  ..., -1.2159e-04,\n",
       "                       -3.4180e-03, -3.6240e-05],\n",
       "                      [ 1.3046e-03,  2.4109e-03,  9.4986e-04,  ...,  2.9945e-04,\n",
       "                       -1.0757e-03,  4.6539e-04],\n",
       "                      [-4.8828e-03,  2.8381e-03, -2.6245e-03,  ...,  4.6692e-03,\n",
       "                        3.0975e-03, -1.3962e-03],\n",
       "                      ...,\n",
       "                      [ 8.3923e-04, -5.7220e-04, -3.2806e-03,  ..., -1.1902e-03,\n",
       "                        1.6251e-03,  4.0588e-03],\n",
       "                      [ 8.8501e-04, -2.1973e-03, -2.5787e-03,  ..., -1.1215e-03,\n",
       "                        6.8283e-04, -3.5858e-03],\n",
       "                      [ 2.3041e-03, -9.2030e-05,  1.7548e-03,  ..., -5.7983e-03,\n",
       "                        3.0365e-03, -1.0147e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.24.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0055,  0.0032,  0.0020,  ...,  0.0023,  0.0011,  0.0061],\n",
       "                      [ 0.0016, -0.0036,  0.0021,  ..., -0.0027,  0.0060, -0.0055],\n",
       "                      [ 0.0044, -0.0027, -0.0012,  ...,  0.0023, -0.0004, -0.0017],\n",
       "                      ...,\n",
       "                      [-0.0006, -0.0049,  0.0008,  ..., -0.0082, -0.0065, -0.0020],\n",
       "                      [-0.0003, -0.0010,  0.0017,  ...,  0.0008, -0.0074,  0.0033],\n",
       "                      [ 0.0049, -0.0023,  0.0026,  ..., -0.0028, -0.0026, -0.0006]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.24.mlp.up_proj.weight',\n",
       "              tensor([[ 3.7689e-03, -5.1880e-03, -2.1667e-03,  ...,  2.5635e-03,\n",
       "                        1.2970e-04,  6.5231e-04],\n",
       "                      [-1.7853e-03,  3.7003e-04,  2.1210e-03,  ..., -2.9907e-03,\n",
       "                        1.1215e-03, -3.1586e-03],\n",
       "                      [ 1.3351e-03,  3.8910e-03, -2.6245e-03,  ...,  1.3828e-04,\n",
       "                        1.2741e-03, -2.3499e-03],\n",
       "                      ...,\n",
       "                      [ 4.3945e-03,  3.7766e-04, -1.7643e-04,  ..., -1.7242e-03,\n",
       "                        1.3885e-03,  3.0823e-03],\n",
       "                      [ 1.6327e-03,  7.3910e-05,  3.9673e-03,  ..., -7.2479e-04,\n",
       "                       -1.9379e-03,  1.3657e-03],\n",
       "                      [ 1.0834e-03, -5.8289e-03, -1.2577e-05,  ...,  2.4719e-03,\n",
       "                        8.2016e-04,  1.3428e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.24.mlp.down_proj.weight',\n",
       "              tensor([[ 2.1667e-03, -5.4626e-03,  1.9531e-03,  ...,  2.3193e-03,\n",
       "                       -2.3746e-04,  6.2943e-04],\n",
       "                      [-5.0964e-03,  5.1575e-03,  3.3112e-03,  ..., -6.9885e-03,\n",
       "                        3.3875e-03, -1.0834e-03],\n",
       "                      [-6.4087e-03, -3.1948e-05, -3.7231e-03,  ...,  2.7847e-04,\n",
       "                        6.2943e-04,  1.6098e-03],\n",
       "                      ...,\n",
       "                      [-1.3885e-03,  3.0670e-03,  1.6403e-03,  ...,  2.0752e-03,\n",
       "                        1.5793e-03, -3.3722e-03],\n",
       "                      [-2.1973e-03,  6.6528e-03,  8.8882e-04,  ...,  1.9226e-03,\n",
       "                       -1.6174e-03, -2.2430e-03],\n",
       "                      [ 1.3733e-03,  2.2583e-03, -4.8523e-03,  ...,  1.7471e-03,\n",
       "                       -7.6294e-04,  1.1139e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.24.input_layernorm.weight',\n",
       "              tensor([2.8125, 2.8438, 2.7812,  ..., 2.5625, 2.6406, 2.6562],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.24.post_attention_layernorm.weight',\n",
       "              tensor([3.4844, 3.8438, 3.6250,  ..., 3.3750, 3.4375, 3.3906],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.25.self_attn.q_proj.weight',\n",
       "              tensor([[-9.2316e-04, -4.2114e-03, -5.0049e-03,  ..., -1.7395e-03,\n",
       "                        4.0588e-03,  2.8687e-03],\n",
       "                      [ 1.8539e-03,  2.7313e-03, -5.6076e-04,  ...,  4.8447e-04,\n",
       "                        2.5024e-03, -1.4954e-03],\n",
       "                      [ 1.5564e-03, -1.7929e-03,  8.9264e-04,  ..., -4.4250e-03,\n",
       "                        8.0109e-05,  1.5945e-03],\n",
       "                      ...,\n",
       "                      [-1.7166e-03,  2.6093e-03,  2.7466e-03,  ...,  1.0132e-02,\n",
       "                       -1.0071e-03,  7.1106e-03],\n",
       "                      [ 1.4587e-02, -3.1891e-03,  1.1414e-02,  ..., -6.1646e-03,\n",
       "                        1.0757e-03, -3.5095e-03],\n",
       "                      [ 1.1169e-02, -3.8452e-03,  6.3171e-03,  ...,  6.2256e-03,\n",
       "                       -9.8877e-03,  7.7820e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.25.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0052, -0.0035, -0.0053,  ..., -0.0013, -0.0002, -0.0023],\n",
       "                      [-0.0044,  0.0014,  0.0042,  ...,  0.0004,  0.0004,  0.0014],\n",
       "                      [-0.0003, -0.0022, -0.0016,  ...,  0.0013, -0.0021,  0.0002],\n",
       "                      ...,\n",
       "                      [ 0.0057, -0.0016,  0.0030,  ..., -0.0033, -0.0041,  0.0030],\n",
       "                      [-0.0052,  0.0016,  0.0012,  ..., -0.0043,  0.0034, -0.0056],\n",
       "                      [-0.0038, -0.0050,  0.0014,  ..., -0.0024, -0.0019,  0.0035]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.25.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0037, -0.0024,  0.0018,  ...,  0.0010,  0.0004, -0.0048],\n",
       "                      [-0.0033,  0.0019,  0.0010,  ..., -0.0036, -0.0013, -0.0065],\n",
       "                      [ 0.0067,  0.0025, -0.0061,  ...,  0.0032, -0.0032,  0.0010],\n",
       "                      ...,\n",
       "                      [-0.0007,  0.0003,  0.0028,  ...,  0.0020,  0.0057,  0.0009],\n",
       "                      [ 0.0019,  0.0030, -0.0034,  ..., -0.0029,  0.0016, -0.0035],\n",
       "                      [-0.0009,  0.0016, -0.0029,  ...,  0.0023,  0.0019,  0.0010]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.25.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.0147e-03, -2.2125e-03, -4.4556e-03,  ..., -2.1515e-03,\n",
       "                        7.1716e-03, -5.3406e-03],\n",
       "                      [-2.1362e-03,  2.2736e-03, -3.7231e-03,  ..., -5.0659e-03,\n",
       "                        1.5564e-03,  2.3460e-04],\n",
       "                      [-2.0905e-03,  1.7929e-03, -5.7459e-05,  ...,  3.3264e-03,\n",
       "                        1.8311e-03,  3.5858e-03],\n",
       "                      ...,\n",
       "                      [ 1.8539e-03,  4.1504e-03, -1.0376e-03,  ...,  3.8757e-03,\n",
       "                       -9.5844e-05, -4.3640e-03],\n",
       "                      [ 6.1035e-04, -2.3956e-03,  3.3569e-03,  ..., -2.3041e-03,\n",
       "                       -5.3406e-04,  8.8501e-03],\n",
       "                      [-2.2888e-03,  1.3447e-04,  1.4496e-03,  ..., -1.9379e-03,\n",
       "                       -1.3351e-03,  2.6093e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.25.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0046,  0.0030, -0.0019,  ...,  0.0026,  0.0017,  0.0052],\n",
       "                      [ 0.0003,  0.0031,  0.0049,  ...,  0.0023,  0.0015, -0.0056],\n",
       "                      [ 0.0011,  0.0001, -0.0055,  ...,  0.0014,  0.0011, -0.0035],\n",
       "                      ...,\n",
       "                      [-0.0029,  0.0001,  0.0058,  ...,  0.0052, -0.0027,  0.0010],\n",
       "                      [ 0.0010,  0.0021,  0.0030,  ..., -0.0005,  0.0028,  0.0067],\n",
       "                      [-0.0019,  0.0081, -0.0009,  ..., -0.0061,  0.0011,  0.0051]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.25.mlp.up_proj.weight',\n",
       "              tensor([[ 3.8452e-03,  7.8964e-04, -5.3024e-04,  ...,  3.8757e-03,\n",
       "                       -2.1057e-03,  2.8534e-03],\n",
       "                      [ 2.9449e-03, -7.2861e-04,  1.3542e-04,  ...,  1.5411e-03,\n",
       "                       -1.5259e-03, -1.1215e-03],\n",
       "                      [-3.0823e-03,  5.0354e-03,  7.4863e-05,  ...,  2.5024e-03,\n",
       "                        1.7319e-03, -3.2196e-03],\n",
       "                      ...,\n",
       "                      [-1.6098e-03,  9.1934e-04,  1.6251e-03,  ..., -2.6855e-03,\n",
       "                        4.8828e-03,  1.6861e-03],\n",
       "                      [ 3.5400e-03,  6.9427e-04, -1.8234e-03,  ...,  3.8910e-03,\n",
       "                       -2.2125e-03, -1.2436e-03],\n",
       "                      [ 5.0354e-03,  5.4626e-03, -8.5831e-04,  ...,  4.8523e-03,\n",
       "                        2.1667e-03,  2.0905e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.25.mlp.down_proj.weight',\n",
       "              tensor([[ 1.3275e-03, -2.0599e-03, -1.2054e-03,  ..., -6.3705e-04,\n",
       "                        3.7766e-04, -5.3406e-03],\n",
       "                      [-9.3079e-04, -1.7624e-03,  2.6703e-03,  ...,  1.2741e-03,\n",
       "                        2.2278e-03,  5.4932e-03],\n",
       "                      [-2.4872e-03,  4.1504e-03, -7.3242e-04,  ..., -3.0823e-03,\n",
       "                        2.4261e-03,  1.8921e-03],\n",
       "                      ...,\n",
       "                      [ 2.5940e-03,  5.0306e-05, -1.9226e-03,  ..., -1.8234e-03,\n",
       "                       -1.3123e-03,  3.3569e-03],\n",
       "                      [-3.8910e-04,  2.1820e-03, -2.3193e-03,  ..., -1.8845e-03,\n",
       "                        2.9755e-03,  6.7520e-04],\n",
       "                      [-3.7079e-03, -2.1839e-04, -3.9101e-04,  ...,  3.5286e-04,\n",
       "                       -5.0964e-03,  5.8365e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.25.input_layernorm.weight',\n",
       "              tensor([2.7500, 2.7969, 2.8906,  ..., 2.6250, 2.7344, 2.7031],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.25.post_attention_layernorm.weight',\n",
       "              tensor([3.5781, 3.9844, 3.7812,  ..., 3.4375, 3.6250, 3.5625],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.26.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0042, -0.0048,  0.0022,  ...,  0.0010, -0.0012, -0.0043],\n",
       "                      [ 0.0013,  0.0004,  0.0041,  ..., -0.0053,  0.0013,  0.0039],\n",
       "                      [ 0.0020,  0.0014, -0.0028,  ..., -0.0016, -0.0077, -0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0040,  0.0001,  0.0005,  ..., -0.0045,  0.0010, -0.0053],\n",
       "                      [ 0.0004, -0.0082,  0.0032,  ...,  0.0036, -0.0004, -0.0012],\n",
       "                      [ 0.0017,  0.0007,  0.0005,  ...,  0.0030,  0.0067,  0.0030]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.26.self_attn.k_proj.weight',\n",
       "              tensor([[-1.6785e-03, -4.4861e-03,  2.5330e-03,  ..., -4.9973e-04,\n",
       "                       -3.1738e-03,  1.5545e-04],\n",
       "                      [-9.7656e-04, -2.1057e-03, -8.8501e-03,  ..., -8.2779e-04,\n",
       "                       -5.6458e-04,  3.9368e-03],\n",
       "                      [-2.6321e-04,  1.6251e-03,  2.2430e-03,  ...,  1.4420e-03,\n",
       "                        1.3275e-03, -1.5411e-03],\n",
       "                      ...,\n",
       "                      [ 1.6117e-04, -4.3640e-03,  1.8845e-03,  ..., -2.7771e-03,\n",
       "                        1.3351e-03,  3.6163e-03],\n",
       "                      [ 1.3885e-03,  1.8005e-03, -2.4872e-03,  ...,  1.4572e-03,\n",
       "                       -5.3024e-04, -3.6469e-03],\n",
       "                      [-8.3008e-03,  2.2697e-04,  6.8665e-03,  ..., -6.0730e-03,\n",
       "                        5.0964e-03,  1.3769e-05]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.26.self_attn.v_proj.weight',\n",
       "              tensor([[ 3.6316e-03,  9.3079e-04,  2.6398e-03,  ...,  1.6937e-03,\n",
       "                       -2.6398e-03,  8.1787e-03],\n",
       "                      [ 4.1246e-05, -4.0283e-03, -5.1575e-03,  ..., -1.7242e-03,\n",
       "                       -4.7112e-04, -5.0354e-03],\n",
       "                      [-7.6294e-03, -4.5166e-03, -2.0447e-03,  ...,  2.6398e-03,\n",
       "                        2.0752e-03,  1.2131e-03],\n",
       "                      ...,\n",
       "                      [ 1.9455e-03, -1.9531e-03,  5.7983e-03,  ...,  1.5411e-03,\n",
       "                       -6.3477e-03,  7.7515e-03],\n",
       "                      [-6.1798e-04,  3.1586e-03,  5.4626e-03,  ..., -7.3624e-04,\n",
       "                        4.9133e-03,  2.6703e-03],\n",
       "                      [-1.1139e-03, -4.4441e-04, -1.7014e-03,  ..., -3.6469e-03,\n",
       "                        3.8147e-03, -2.8801e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.26.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0019,  0.0011, -0.0003,  ...,  0.0029, -0.0034, -0.0027],\n",
       "                      [-0.0003,  0.0056, -0.0004,  ..., -0.0036, -0.0004,  0.0005],\n",
       "                      [ 0.0054,  0.0021,  0.0005,  ..., -0.0014, -0.0015, -0.0008],\n",
       "                      ...,\n",
       "                      [-0.0014, -0.0098,  0.0021,  ...,  0.0003, -0.0024, -0.0015],\n",
       "                      [ 0.0008, -0.0032, -0.0018,  ..., -0.0018,  0.0038,  0.0014],\n",
       "                      [-0.0024, -0.0026, -0.0023,  ..., -0.0019,  0.0033,  0.0060]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.26.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0033,  0.0014, -0.0001,  ...,  0.0061,  0.0008, -0.0040],\n",
       "                      [-0.0019, -0.0054,  0.0011,  ..., -0.0046,  0.0029,  0.0008],\n",
       "                      [ 0.0076, -0.0030,  0.0003,  ..., -0.0010, -0.0047, -0.0022],\n",
       "                      ...,\n",
       "                      [ 0.0017, -0.0025,  0.0025,  ..., -0.0009,  0.0033, -0.0035],\n",
       "                      [-0.0032,  0.0020, -0.0028,  ...,  0.0023,  0.0054,  0.0019],\n",
       "                      [ 0.0008,  0.0040, -0.0005,  ...,  0.0053,  0.0011, -0.0050]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.26.mlp.up_proj.weight',\n",
       "              tensor([[ 1.2283e-03, -1.6632e-03, -2.5635e-03,  ...,  7.6294e-04,\n",
       "                       -9.7275e-04, -2.9449e-03],\n",
       "                      [-4.6082e-03, -3.9101e-05,  2.5024e-03,  ..., -6.0272e-04,\n",
       "                       -2.6703e-03, -2.8992e-03],\n",
       "                      [ 5.8365e-04, -2.3041e-03,  3.4180e-03,  ...,  2.9144e-03,\n",
       "                       -1.1520e-03, -7.1335e-04],\n",
       "                      ...,\n",
       "                      [ 1.3580e-03,  9.7656e-04, -2.8229e-03,  ..., -1.7395e-03,\n",
       "                        8.9645e-04,  2.0905e-03],\n",
       "                      [ 8.8692e-05,  2.0447e-03, -3.6469e-03,  ..., -1.4019e-04,\n",
       "                        7.1106e-03,  2.5635e-03],\n",
       "                      [-3.8300e-03, -1.9836e-03, -3.2349e-03,  ...,  1.8597e-04,\n",
       "                        1.3428e-03, -1.3428e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.26.mlp.down_proj.weight',\n",
       "              tensor([[ 1.9836e-03,  3.1853e-04, -2.8229e-03,  ...,  8.9645e-04,\n",
       "                        2.9144e-03, -3.7384e-04],\n",
       "                      [ 7.3624e-04, -6.8359e-03, -2.1210e-03,  ...,  1.3275e-03,\n",
       "                        2.7180e-05, -5.1880e-03],\n",
       "                      [ 5.4389e-07,  1.1301e-04, -1.0986e-03,  ..., -7.7057e-04,\n",
       "                       -5.7983e-04, -1.9836e-03],\n",
       "                      ...,\n",
       "                      [ 1.1253e-04, -2.7924e-03, -6.0730e-03,  ...,  1.2302e-04,\n",
       "                       -1.1368e-03,  6.2943e-04],\n",
       "                      [-4.2152e-04, -7.8201e-04, -1.1978e-03,  ...,  2.6398e-03,\n",
       "                        2.6093e-03,  2.7313e-03],\n",
       "                      [ 3.0975e-03, -8.2397e-04,  4.9438e-03,  ...,  1.8158e-03,\n",
       "                        6.5994e-04,  1.7166e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.26.input_layernorm.weight',\n",
       "              tensor([2.6094, 2.5938, 2.5625,  ..., 2.5469, 2.4688, 2.5469],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.26.post_attention_layernorm.weight',\n",
       "              tensor([3.7188, 3.9531, 3.7812,  ..., 3.5312, 3.6250, 3.6094],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.27.self_attn.q_proj.weight',\n",
       "              tensor([[ 2.8076e-03,  8.7357e-04,  2.4414e-03,  ..., -1.9684e-03,\n",
       "                        6.0425e-03,  4.7302e-04],\n",
       "                      [-1.6098e-03, -9.4986e-04, -5.8365e-04,  ...,  7.6675e-04,\n",
       "                       -2.8229e-03, -6.9885e-03],\n",
       "                      [ 7.6294e-04, -4.0283e-03, -2.4567e-03,  ...,  3.0975e-03,\n",
       "                       -2.5024e-03, -2.5482e-03],\n",
       "                      ...,\n",
       "                      [ 5.1575e-03,  6.6376e-04, -9.2983e-05,  ..., -2.1820e-03,\n",
       "                        5.8594e-03, -1.0834e-03],\n",
       "                      [-6.7139e-04,  1.1292e-03,  1.5945e-03,  ..., -9.5367e-04,\n",
       "                        2.3193e-03, -7.1824e-06],\n",
       "                      [ 3.3417e-03,  2.6398e-03, -1.9684e-03,  ...,  6.2256e-03,\n",
       "                        2.6894e-04, -4.7302e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.27.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.6689e-04,  1.7014e-03, -3.2196e-03,  ...,  4.2534e-04,\n",
       "                        5.2691e-05, -1.0605e-03],\n",
       "                      [ 5.1575e-03,  1.5411e-03, -1.2665e-03,  ..., -7.4158e-03,\n",
       "                        1.3199e-03, -1.8921e-03],\n",
       "                      [ 5.7983e-03, -5.7602e-04, -1.9302e-03,  ..., -2.7771e-03,\n",
       "                        1.8234e-03, -4.8828e-03],\n",
       "                      ...,\n",
       "                      [-8.2016e-04,  3.5667e-04,  1.5182e-03,  ..., -4.7684e-04,\n",
       "                       -8.4305e-04, -2.1667e-03],\n",
       "                      [-7.2861e-04, -2.6131e-04,  1.7853e-03,  ...,  1.4591e-04,\n",
       "                        2.1057e-03,  2.2278e-03],\n",
       "                      [ 3.2959e-03, -2.4719e-03, -3.3112e-03,  ...,  8.0490e-04,\n",
       "                       -1.4725e-03,  5.4626e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.27.self_attn.v_proj.weight',\n",
       "              tensor([[-1.3046e-03,  5.1575e-03, -1.8387e-03,  ..., -4.3106e-04,\n",
       "                       -4.1504e-03,  2.0752e-03],\n",
       "                      [ 2.1515e-03,  3.2043e-03, -4.8828e-03,  ..., -1.8616e-03,\n",
       "                        4.9353e-05, -2.2278e-03],\n",
       "                      [-3.2997e-04, -3.7575e-04,  2.3804e-03,  ...,  4.4556e-03,\n",
       "                        1.9073e-03, -1.6022e-03],\n",
       "                      ...,\n",
       "                      [ 1.0376e-03, -1.2970e-03, -3.5400e-03,  ...,  1.4801e-03,\n",
       "                       -6.2561e-03,  3.0975e-03],\n",
       "                      [ 6.4087e-03, -3.7994e-03,  1.1826e-03,  ...,  3.4180e-03,\n",
       "                        8.0872e-04, -3.1281e-03],\n",
       "                      [-4.4861e-03, -4.9438e-03,  1.6022e-03,  ..., -3.4485e-03,\n",
       "                       -9.4223e-04,  2.3804e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.27.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.9455e-03, -4.6692e-03, -1.8692e-03,  ...,  1.7014e-03,\n",
       "                       -3.7079e-03,  1.7700e-03],\n",
       "                      [-9.3079e-04, -4.8218e-03,  2.1362e-03,  ...,  4.6692e-03,\n",
       "                        7.6890e-06,  5.3711e-03],\n",
       "                      [ 4.4250e-03, -2.2583e-03,  4.3678e-04,  ..., -1.7776e-03,\n",
       "                       -3.4027e-03, -1.3123e-03],\n",
       "                      ...,\n",
       "                      [-2.5635e-03,  1.6632e-03, -2.8229e-04,  ...,  4.3945e-03,\n",
       "                        4.6387e-03,  1.7776e-03],\n",
       "                      [ 3.4180e-03, -4.6921e-04,  1.8120e-04,  ...,  2.8687e-03,\n",
       "                       -4.6997e-03, -1.9531e-03],\n",
       "                      [ 4.6730e-05,  6.9046e-04, -7.1335e-04,  ..., -1.6022e-03,\n",
       "                        7.3624e-04, -4.4250e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.27.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0031,  0.0010,  0.0046,  ..., -0.0039, -0.0012,  0.0079],\n",
       "                      [ 0.0055,  0.0071,  0.0015,  ..., -0.0005,  0.0006,  0.0018],\n",
       "                      [-0.0009, -0.0029, -0.0029,  ..., -0.0019,  0.0014,  0.0034],\n",
       "                      ...,\n",
       "                      [-0.0015,  0.0009,  0.0030,  ...,  0.0021, -0.0013,  0.0013],\n",
       "                      [ 0.0032, -0.0003,  0.0006,  ...,  0.0002, -0.0023,  0.0024],\n",
       "                      [-0.0022, -0.0042, -0.0068,  ...,  0.0044, -0.0055,  0.0023]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.27.mlp.up_proj.weight',\n",
       "              tensor([[-0.0047,  0.0006,  0.0007,  ...,  0.0005,  0.0012,  0.0025],\n",
       "                      [ 0.0045,  0.0039, -0.0045,  ..., -0.0025, -0.0014, -0.0005],\n",
       "                      [ 0.0052, -0.0001,  0.0026,  ...,  0.0009,  0.0013, -0.0001],\n",
       "                      ...,\n",
       "                      [-0.0019, -0.0064, -0.0006,  ...,  0.0061, -0.0010, -0.0013],\n",
       "                      [ 0.0055, -0.0054, -0.0009,  ..., -0.0018, -0.0038,  0.0018],\n",
       "                      [ 0.0064,  0.0060,  0.0040,  ...,  0.0007,  0.0025, -0.0032]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.27.mlp.down_proj.weight',\n",
       "              tensor([[-3.0518e-04, -2.4109e-03,  4.0283e-03,  ...,  1.8539e-03,\n",
       "                       -5.4016e-03, -2.9907e-03],\n",
       "                      [-1.9455e-03,  3.6011e-03,  2.1515e-03,  ...,  1.0681e-03,\n",
       "                        1.0834e-03, -5.4016e-03],\n",
       "                      [ 1.9226e-03, -2.2583e-03,  2.9373e-04,  ...,  4.5471e-03,\n",
       "                       -2.0599e-03, -4.1504e-03],\n",
       "                      ...,\n",
       "                      [-8.1787e-03, -2.3007e-05, -1.4973e-04,  ...,  5.8899e-03,\n",
       "                       -2.8381e-03,  2.0294e-03],\n",
       "                      [-4.4861e-03,  7.5912e-04, -3.7231e-03,  ..., -2.0415e-06,\n",
       "                        1.1139e-03,  4.0588e-03],\n",
       "                      [ 4.1809e-03, -1.6556e-03,  1.7624e-03,  ...,  2.8839e-03,\n",
       "                       -4.0283e-03, -3.6812e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.27.input_layernorm.weight',\n",
       "              tensor([2.6875, 2.8125, 2.7031,  ..., 2.5781, 2.5938, 2.6406],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.27.post_attention_layernorm.weight',\n",
       "              tensor([3.7656, 4.0000, 3.8281,  ..., 3.6562, 3.6719, 3.7188],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.28.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0035,  0.0011, -0.0012,  ..., -0.0054,  0.0009, -0.0005],\n",
       "                      [-0.0013,  0.0010, -0.0009,  ...,  0.0015,  0.0035, -0.0003],\n",
       "                      [-0.0010,  0.0009, -0.0032,  ..., -0.0012, -0.0003,  0.0032],\n",
       "                      ...,\n",
       "                      [-0.0060, -0.0033,  0.0002,  ...,  0.0010,  0.0020, -0.0028],\n",
       "                      [ 0.0014,  0.0042,  0.0085,  ..., -0.0005,  0.0021, -0.0003],\n",
       "                      [-0.0016, -0.0028,  0.0003,  ...,  0.0049,  0.0046, -0.0004]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.28.self_attn.k_proj.weight',\n",
       "              tensor([[-8.2016e-04,  1.0452e-03,  1.6117e-04,  ..., -1.0223e-03,\n",
       "                        6.3782e-03, -1.2436e-03],\n",
       "                      [-4.8828e-03, -2.2888e-03, -4.6921e-04,  ...,  8.3923e-04,\n",
       "                        2.3193e-03,  1.5182e-03],\n",
       "                      [-3.8452e-03,  1.7071e-04, -3.0365e-03,  ..., -9.3460e-04,\n",
       "                       -6.1035e-04, -7.1526e-05],\n",
       "                      ...,\n",
       "                      [-4.3945e-03, -1.4801e-03,  2.8076e-03,  ...,  1.4725e-03,\n",
       "                       -6.6223e-03,  2.0752e-03],\n",
       "                      [ 1.8845e-03, -2.8839e-03, -6.2180e-04,  ..., -3.8910e-03,\n",
       "                       -4.3640e-03, -3.5248e-03],\n",
       "                      [ 3.2043e-03,  6.3477e-03,  2.5787e-03,  ..., -7.0190e-03,\n",
       "                       -3.7956e-04, -1.3542e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.28.self_attn.v_proj.weight',\n",
       "              tensor([[ 5.2795e-03, -1.2894e-03,  3.5286e-04,  ..., -4.0283e-03,\n",
       "                        4.6730e-04, -4.9438e-03],\n",
       "                      [ 3.4637e-03, -1.3351e-03, -3.5248e-03,  ...,  5.2490e-03,\n",
       "                       -1.0834e-03,  4.5776e-03],\n",
       "                      [-5.3406e-03,  2.1057e-03, -4.0817e-04,  ...,  1.1384e-05,\n",
       "                        1.6785e-03,  3.9978e-03],\n",
       "                      ...,\n",
       "                      [ 1.3733e-03, -8.8501e-04,  4.2725e-04,  ..., -2.6093e-03,\n",
       "                       -3.3569e-03,  1.7929e-03],\n",
       "                      [ 3.0518e-03, -8.6594e-04, -3.4332e-03,  ...,  6.8665e-04,\n",
       "                       -5.6458e-03,  2.7008e-03],\n",
       "                      [ 1.4496e-04,  9.9945e-04, -5.2643e-04,  ...,  2.4414e-03,\n",
       "                        1.7776e-03, -4.4250e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.28.self_attn.o_proj.weight',\n",
       "              tensor([[-1.5030e-03,  4.6997e-03,  2.9907e-03,  ...,  9.9182e-04,\n",
       "                       -1.1063e-03, -6.0272e-04],\n",
       "                      [-2.0599e-03,  1.0910e-03, -3.1281e-03,  ...,  3.0212e-03,\n",
       "                        2.2736e-03,  5.7983e-03],\n",
       "                      [ 4.1504e-03,  2.0027e-04,  1.9836e-03,  ...,  2.4414e-04,\n",
       "                        1.4954e-03, -2.3651e-03],\n",
       "                      ...,\n",
       "                      [-1.0071e-03, -1.3809e-03,  3.9673e-03,  ..., -7.1716e-04,\n",
       "                       -2.0142e-03,  4.5471e-03],\n",
       "                      [-7.4387e-04,  3.3722e-03,  2.8992e-03,  ..., -3.3722e-03,\n",
       "                       -1.9836e-03, -9.0408e-04],\n",
       "                      [-1.4267e-03,  8.2016e-04,  2.2736e-03,  ..., -1.7853e-03,\n",
       "                       -7.0095e-05, -5.1880e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.28.mlp.gate_proj.weight',\n",
       "              tensor([[ 4.8828e-03,  1.6022e-03,  1.8539e-03,  ..., -1.5793e-03,\n",
       "                        5.1880e-03, -1.4114e-03],\n",
       "                      [ 1.8921e-03, -4.4823e-04, -2.3499e-03,  ..., -2.5482e-03,\n",
       "                       -4.1504e-03, -1.4725e-03],\n",
       "                      [ 6.4087e-03, -6.2180e-04,  8.8882e-04,  ..., -5.6458e-03,\n",
       "                        4.5776e-04, -3.8910e-04],\n",
       "                      ...,\n",
       "                      [-2.6855e-03,  1.2493e-04, -3.6316e-03,  ..., -1.3351e-03,\n",
       "                       -3.0365e-03, -7.2861e-04],\n",
       "                      [ 5.2795e-03, -2.1362e-03, -1.8539e-03,  ..., -1.4343e-03,\n",
       "                       -2.3603e-05,  5.1880e-03],\n",
       "                      [ 2.1210e-03,  2.9755e-03, -3.7537e-03,  ...,  6.8188e-05,\n",
       "                        5.4016e-03, -2.9449e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.28.mlp.up_proj.weight',\n",
       "              tensor([[-0.0014,  0.0022,  0.0020,  ...,  0.0058, -0.0030,  0.0087],\n",
       "                      [ 0.0036,  0.0036,  0.0012,  ..., -0.0018,  0.0017, -0.0026],\n",
       "                      [ 0.0009, -0.0025,  0.0028,  ...,  0.0023, -0.0021,  0.0042],\n",
       "                      ...,\n",
       "                      [-0.0008,  0.0038, -0.0003,  ..., -0.0001,  0.0032, -0.0002],\n",
       "                      [ 0.0030, -0.0009, -0.0037,  ...,  0.0011, -0.0001, -0.0003],\n",
       "                      [-0.0022,  0.0037,  0.0006,  ...,  0.0004, -0.0074, -0.0020]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.28.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0030,  0.0003, -0.0040,  ...,  0.0048, -0.0017,  0.0001],\n",
       "                      [ 0.0030,  0.0033, -0.0003,  ...,  0.0009,  0.0030,  0.0004],\n",
       "                      [-0.0019, -0.0077,  0.0042,  ...,  0.0065, -0.0013,  0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0021,  0.0053,  0.0027,  ..., -0.0029,  0.0020, -0.0009],\n",
       "                      [ 0.0004, -0.0067,  0.0026,  ..., -0.0029,  0.0014,  0.0006],\n",
       "                      [-0.0025,  0.0039,  0.0042,  ...,  0.0007,  0.0017,  0.0028]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.28.input_layernorm.weight',\n",
       "              tensor([2.5469, 2.6562, 2.6562,  ..., 2.4219, 2.4844, 2.4531],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.28.post_attention_layernorm.weight',\n",
       "              tensor([3.7656, 4.0312, 3.9219,  ..., 3.7656, 3.7969, 3.7656],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.29.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0015,  0.0033,  0.0008,  ..., -0.0030, -0.0025, -0.0056],\n",
       "                      [-0.0010,  0.0019, -0.0015,  ..., -0.0016,  0.0028, -0.0005],\n",
       "                      [ 0.0007, -0.0012,  0.0028,  ..., -0.0040, -0.0021, -0.0019],\n",
       "                      ...,\n",
       "                      [ 0.0083,  0.0020,  0.0004,  ..., -0.0086, -0.0001,  0.0006],\n",
       "                      [-0.0012,  0.0040, -0.0062,  ..., -0.0059, -0.0016, -0.0015],\n",
       "                      [ 0.0001, -0.0035,  0.0031,  ...,  0.0049, -0.0005, -0.0008]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.29.self_attn.k_proj.weight',\n",
       "              tensor([[-2.1667e-03, -2.5368e-04, -9.8705e-05,  ...,  3.2997e-04,\n",
       "                       -5.3406e-04,  4.6082e-03],\n",
       "                      [-1.5030e-03,  3.7537e-03,  1.2131e-03,  ..., -3.0823e-03,\n",
       "                       -1.7624e-03,  1.2436e-03],\n",
       "                      [-1.0147e-03,  6.7139e-04,  3.5477e-04,  ...,  1.6403e-03,\n",
       "                        3.7432e-05, -2.4109e-03],\n",
       "                      ...,\n",
       "                      [ 4.3640e-03, -1.1749e-03,  2.9144e-03,  ...,  3.1281e-03,\n",
       "                       -3.7537e-03,  1.3351e-05],\n",
       "                      [-9.7046e-03,  9.3460e-04,  9.7046e-03,  ..., -2.6550e-03,\n",
       "                       -4.3640e-03, -3.6011e-03],\n",
       "                      [ 5.7373e-03, -3.0975e-03, -1.1444e-03,  ..., -8.3618e-03,\n",
       "                       -1.0920e-04, -1.0071e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.29.self_attn.v_proj.weight',\n",
       "              tensor([[-6.8665e-03, -3.6049e-04, -1.4648e-03,  ..., -1.6098e-03,\n",
       "                        2.0599e-04, -4.5776e-03],\n",
       "                      [-1.2024e-02, -5.8746e-04, -4.0588e-03,  ...,  1.2451e-02,\n",
       "                        5.6152e-03, -4.5166e-03],\n",
       "                      [-5.7068e-03,  7.1716e-03, -3.4027e-03,  ..., -3.2501e-03,\n",
       "                       -3.2196e-03,  2.8534e-03],\n",
       "                      ...,\n",
       "                      [-2.0905e-03,  1.5717e-03,  3.7079e-03,  ..., -3.8300e-03,\n",
       "                       -2.8687e-03, -1.7548e-03],\n",
       "                      [-2.1515e-03, -4.4250e-03, -3.8910e-03,  ...,  2.6855e-03,\n",
       "                       -2.3693e-06,  2.1210e-03],\n",
       "                      [-9.4604e-04, -3.6240e-04,  2.2736e-03,  ...,  5.1575e-03,\n",
       "                        3.6011e-03,  3.5286e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.29.self_attn.o_proj.weight',\n",
       "              tensor([[ 2.4872e-03,  5.7983e-03,  3.1281e-03,  ...,  1.9226e-03,\n",
       "                       -7.0572e-04,  1.2360e-03],\n",
       "                      [-9.3842e-04,  1.6327e-03, -3.2196e-03,  ..., -2.6703e-03,\n",
       "                       -3.8719e-04,  1.7242e-03],\n",
       "                      [-1.2054e-03, -9.0408e-04, -5.1498e-04,  ...,  3.6774e-03,\n",
       "                       -2.7657e-04, -1.0834e-03],\n",
       "                      ...,\n",
       "                      [ 1.8005e-03, -4.4556e-03,  1.5259e-03,  ...,  1.0757e-03,\n",
       "                        5.7983e-03,  2.2278e-03],\n",
       "                      [ 1.7929e-03, -2.8076e-03,  2.1820e-03,  ..., -1.8005e-03,\n",
       "                       -2.7466e-03,  5.6076e-04],\n",
       "                      [ 1.4343e-03,  1.4267e-03,  2.1057e-03,  ...,  3.1281e-04,\n",
       "                        3.4332e-04,  5.9128e-05]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.29.mlp.gate_proj.weight',\n",
       "              tensor([[-6.8970e-03, -2.3365e-04, -7.2632e-03,  ..., -3.5858e-03,\n",
       "                       -4.9770e-06,  3.7994e-03],\n",
       "                      [-2.9945e-04, -1.6937e-03,  4.7913e-03,  ...,  4.7607e-03,\n",
       "                        1.0986e-03,  3.2501e-03],\n",
       "                      [ 9.8267e-03,  1.9150e-03, -1.0910e-03,  ..., -5.4016e-03,\n",
       "                       -1.1597e-03,  2.2030e-04],\n",
       "                      ...,\n",
       "                      [-3.5706e-03,  5.4762e-07,  1.9455e-03,  ...,  2.3499e-03,\n",
       "                       -1.7242e-03, -1.3542e-04],\n",
       "                      [ 9.4986e-04,  1.8845e-03,  2.9297e-03,  ..., -4.6997e-03,\n",
       "                       -1.6022e-03,  1.5717e-03],\n",
       "                      [ 2.0599e-03,  3.2043e-04, -3.9368e-03,  ...,  5.1270e-03,\n",
       "                        7.7438e-04,  1.3885e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.29.mlp.up_proj.weight',\n",
       "              tensor([[ 1.9455e-03,  1.6708e-03, -5.2185e-03,  ..., -1.9684e-03,\n",
       "                       -1.6785e-03,  1.3733e-03],\n",
       "                      [ 9.1553e-03, -5.4626e-03,  7.4387e-04,  ...,  2.2888e-03,\n",
       "                        3.0975e-03,  2.7466e-03],\n",
       "                      [-4.1504e-03, -4.9133e-03,  1.2589e-03,  ...,  3.2616e-04,\n",
       "                        6.7711e-05, -2.1820e-03],\n",
       "                      ...,\n",
       "                      [-3.9673e-04, -2.7008e-03, -5.0049e-03,  ..., -2.1973e-03,\n",
       "                        1.1749e-03,  4.0283e-03],\n",
       "                      [ 4.2114e-03,  7.2937e-03, -3.8452e-03,  ..., -2.8534e-03,\n",
       "                       -2.8534e-03,  1.8234e-03],\n",
       "                      [ 6.3705e-04, -6.2180e-04, -2.3956e-03,  ..., -3.1281e-03,\n",
       "                        8.1253e-04,  2.1820e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.29.mlp.down_proj.weight',\n",
       "              tensor([[-1.2970e-03, -1.4343e-03,  1.2589e-03,  ...,  2.0294e-03,\n",
       "                       -4.7302e-03,  2.2583e-03],\n",
       "                      [-2.3193e-03, -7.4768e-04,  8.6060e-03,  ..., -4.2534e-04,\n",
       "                       -2.3651e-03, -1.3809e-03],\n",
       "                      [ 6.7902e-04,  5.4932e-03, -4.7493e-04,  ...,  2.6703e-03,\n",
       "                       -1.8921e-03, -2.2430e-03],\n",
       "                      ...,\n",
       "                      [ 2.9602e-03, -2.5635e-03, -6.8359e-03,  ...,  2.0447e-03,\n",
       "                       -5.8174e-05, -4.4060e-04],\n",
       "                      [ 1.6251e-03, -7.6294e-05,  2.4872e-03,  ...,  5.8289e-03,\n",
       "                        4.8218e-03,  2.1515e-03],\n",
       "                      [-3.3112e-03, -3.3417e-03,  1.0147e-03,  ..., -1.7090e-03,\n",
       "                        3.2806e-03,  1.0300e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.29.input_layernorm.weight',\n",
       "              tensor([2.8906, 3.0781, 2.9375,  ..., 2.5938, 2.9062, 2.8438],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.29.post_attention_layernorm.weight',\n",
       "              tensor([3.8281, 3.9375, 3.8438,  ..., 3.7500, 3.7188, 3.6875],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.30.self_attn.q_proj.weight',\n",
       "              tensor([[ 7.9956e-03,  2.8229e-03,  1.0669e-05,  ...,  1.5182e-03,\n",
       "                       -4.7607e-03, -4.5967e-04],\n",
       "                      [-2.9755e-04,  1.6937e-03, -3.5248e-03,  ...,  4.2915e-04,\n",
       "                        7.2327e-03,  5.9509e-03],\n",
       "                      [-3.5095e-03,  1.2283e-03,  5.0659e-03,  ..., -2.5368e-04,\n",
       "                        5.1880e-03,  8.0490e-04],\n",
       "                      ...,\n",
       "                      [-2.7771e-03, -2.2736e-03, -2.8381e-03,  ..., -6.3477e-03,\n",
       "                       -2.4872e-03, -3.3722e-03],\n",
       "                      [ 2.2430e-03,  4.8828e-03, -2.3193e-03,  ...,  1.7929e-04,\n",
       "                       -5.7983e-03, -1.2360e-03],\n",
       "                      [-2.5635e-03,  1.9684e-03,  3.7956e-04,  ...,  2.6093e-03,\n",
       "                        2.1667e-03,  1.3428e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.30.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0002, -0.0002,  0.0004,  ..., -0.0024,  0.0014, -0.0005],\n",
       "                      [-0.0006, -0.0019, -0.0011,  ..., -0.0003,  0.0007, -0.0008],\n",
       "                      [ 0.0006, -0.0004,  0.0009,  ..., -0.0013, -0.0022,  0.0012],\n",
       "                      ...,\n",
       "                      [-0.0019, -0.0002, -0.0004,  ...,  0.0052, -0.0003,  0.0007],\n",
       "                      [ 0.0030, -0.0012, -0.0008,  ...,  0.0006, -0.0020, -0.0026],\n",
       "                      [-0.0026, -0.0052,  0.0032,  ...,  0.0007,  0.0010,  0.0020]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.30.self_attn.v_proj.weight',\n",
       "              tensor([[-4.4823e-05, -2.7161e-03,  2.9907e-03,  ...,  7.5912e-04,\n",
       "                       -1.6556e-03,  3.9062e-03],\n",
       "                      [-8.3008e-03,  2.2736e-03, -3.8910e-04,  ...,  3.7689e-03,\n",
       "                       -4.1199e-03, -9.5749e-04],\n",
       "                      [-1.0193e-02, -5.3101e-03,  2.3651e-03,  ..., -3.0212e-03,\n",
       "                       -3.6163e-03, -1.1139e-03],\n",
       "                      ...,\n",
       "                      [-1.9989e-03,  3.3569e-03, -9.2773e-03,  ...,  6.9427e-04,\n",
       "                       -4.1199e-03,  3.8605e-03],\n",
       "                      [ 8.6060e-03,  1.3657e-03, -5.0964e-03,  ...,  1.9531e-03,\n",
       "                        6.3171e-03, -8.3618e-03],\n",
       "                      [-2.0142e-03,  8.2016e-04, -3.5706e-03,  ..., -2.2736e-03,\n",
       "                       -5.2643e-04, -4.2114e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.30.self_attn.o_proj.weight',\n",
       "              tensor([[-1.6022e-03, -2.2278e-03, -2.5940e-03,  ..., -6.0272e-04,\n",
       "                       -4.8065e-04,  5.0659e-03],\n",
       "                      [-4.3335e-03, -1.9073e-03, -1.1826e-03,  ...,  5.4550e-04,\n",
       "                       -6.5613e-03,  4.4441e-04],\n",
       "                      [ 1.0147e-03,  4.3945e-03,  5.2185e-03,  ..., -2.1515e-03,\n",
       "                       -3.6240e-04, -3.5400e-03],\n",
       "                      ...,\n",
       "                      [-2.6550e-03,  5.0354e-04,  5.6152e-03,  ...,  4.1580e-04,\n",
       "                        2.2278e-03,  1.8501e-04],\n",
       "                      [-4.7112e-04, -2.5024e-03,  1.4725e-03,  ..., -4.2725e-03,\n",
       "                        2.4719e-03,  8.4400e-05],\n",
       "                      [-1.8005e-03,  7.0190e-04,  6.5308e-03,  ..., -3.6812e-04,\n",
       "                       -6.7139e-03,  9.7656e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.30.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0028,  0.0009, -0.0029,  ..., -0.0045,  0.0026, -0.0060],\n",
       "                      [-0.0008, -0.0009,  0.0028,  ..., -0.0006, -0.0015,  0.0022],\n",
       "                      [ 0.0012,  0.0031, -0.0010,  ...,  0.0032,  0.0027, -0.0004],\n",
       "                      ...,\n",
       "                      [ 0.0005, -0.0034, -0.0006,  ...,  0.0057,  0.0012, -0.0005],\n",
       "                      [ 0.0015,  0.0027, -0.0008,  ..., -0.0002, -0.0038, -0.0058],\n",
       "                      [-0.0011, -0.0006, -0.0014,  ...,  0.0030, -0.0008, -0.0008]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.30.mlp.up_proj.weight',\n",
       "              tensor([[-0.0006, -0.0025, -0.0017,  ...,  0.0014,  0.0007,  0.0062],\n",
       "                      [-0.0044,  0.0060,  0.0027,  ...,  0.0012,  0.0028, -0.0036],\n",
       "                      [-0.0014, -0.0075, -0.0015,  ...,  0.0003,  0.0055, -0.0015],\n",
       "                      ...,\n",
       "                      [-0.0003, -0.0012,  0.0006,  ...,  0.0011,  0.0032, -0.0003],\n",
       "                      [-0.0018, -0.0021, -0.0046,  ...,  0.0015, -0.0052,  0.0010],\n",
       "                      [ 0.0018, -0.0011,  0.0045,  ..., -0.0020, -0.0022, -0.0005]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.30.mlp.down_proj.weight',\n",
       "              tensor([[ 4.2114e-03, -5.8365e-04,  1.8768e-03,  ..., -9.3842e-04,\n",
       "                       -1.0223e-03,  8.1635e-04],\n",
       "                      [-9.5749e-04, -5.0354e-03, -2.6093e-03,  ...,  1.9226e-03,\n",
       "                       -2.1820e-03,  4.5967e-04],\n",
       "                      [-1.6785e-03,  2.6855e-03,  3.7689e-03,  ...,  2.3804e-03,\n",
       "                        3.2196e-03, -1.9836e-03],\n",
       "                      ...,\n",
       "                      [ 1.7834e-04,  2.8839e-03, -3.2806e-04,  ..., -7.7438e-04,\n",
       "                       -4.2534e-04,  1.1368e-03],\n",
       "                      [ 1.0376e-03, -1.5450e-04, -9.6130e-04,  ..., -2.2888e-03,\n",
       "                        1.2589e-03,  5.1270e-03],\n",
       "                      [-1.6708e-03, -9.9659e-05,  1.3046e-03,  ...,  2.3804e-03,\n",
       "                        9.1553e-04, -1.4725e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.30.input_layernorm.weight',\n",
       "              tensor([2.7188, 2.6094, 2.6562,  ..., 2.4219, 2.4219, 2.6250],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.30.post_attention_layernorm.weight',\n",
       "              tensor([3.7500, 3.9844, 3.8906,  ..., 3.8125, 3.7344, 3.8594],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.31.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0014, -0.0017, -0.0011,  ..., -0.0034,  0.0037,  0.0004],\n",
       "                      [ 0.0030, -0.0023, -0.0010,  ..., -0.0006, -0.0002, -0.0033],\n",
       "                      [-0.0006,  0.0021,  0.0024,  ...,  0.0025, -0.0011,  0.0051],\n",
       "                      ...,\n",
       "                      [-0.0003, -0.0012,  0.0027,  ..., -0.0003, -0.0020,  0.0001],\n",
       "                      [ 0.0078, -0.0008,  0.0002,  ..., -0.0029,  0.0005, -0.0005],\n",
       "                      [-0.0025, -0.0002,  0.0048,  ...,  0.0026,  0.0010, -0.0015]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.31.self_attn.k_proj.weight',\n",
       "              tensor([[ 2.3804e-03, -4.3297e-04, -1.5640e-03,  ...,  7.9346e-04,\n",
       "                       -6.5231e-04,  1.2207e-03],\n",
       "                      [ 2.5024e-03,  1.1520e-03, -6.9046e-04,  ...,  1.7929e-03,\n",
       "                        2.2278e-03, -2.0142e-03],\n",
       "                      [ 2.3270e-04, -1.7471e-03,  3.1433e-03,  ...,  2.9755e-03,\n",
       "                       -4.7874e-04,  1.4420e-03],\n",
       "                      ...,\n",
       "                      [ 1.1292e-03,  1.1368e-03,  1.8082e-03,  ..., -1.8845e-03,\n",
       "                       -2.2583e-03, -3.2425e-04],\n",
       "                      [ 9.8419e-04, -2.9449e-03, -9.6798e-05,  ..., -3.7079e-03,\n",
       "                        7.5912e-04, -2.3603e-05],\n",
       "                      [-4.8218e-03,  5.5542e-03, -5.6458e-03,  ..., -8.0109e-04,\n",
       "                        3.0365e-03,  1.1683e-04]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.31.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0007, -0.0017,  0.0004,  ..., -0.0062, -0.0005,  0.0006],\n",
       "                      [ 0.0011,  0.0028, -0.0036,  ..., -0.0044, -0.0059, -0.0038],\n",
       "                      [ 0.0062,  0.0016, -0.0003,  ...,  0.0017, -0.0005,  0.0019],\n",
       "                      ...,\n",
       "                      [ 0.0036, -0.0026, -0.0039,  ..., -0.0053,  0.0002, -0.0032],\n",
       "                      [ 0.0047,  0.0059, -0.0001,  ..., -0.0086, -0.0004,  0.0066],\n",
       "                      [-0.0118,  0.0002, -0.0063,  ...,  0.0084,  0.0106, -0.0004]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.31.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0014, -0.0012, -0.0010,  ...,  0.0020,  0.0076, -0.0014],\n",
       "                      [ 0.0018, -0.0025,  0.0018,  ..., -0.0026, -0.0035,  0.0019],\n",
       "                      [-0.0003, -0.0041,  0.0009,  ..., -0.0023, -0.0072, -0.0028],\n",
       "                      ...,\n",
       "                      [ 0.0027,  0.0015,  0.0027,  ...,  0.0028,  0.0003,  0.0020],\n",
       "                      [ 0.0024,  0.0021, -0.0011,  ...,  0.0032, -0.0041,  0.0006],\n",
       "                      [-0.0039,  0.0007,  0.0010,  ...,  0.0025, -0.0008, -0.0022]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.31.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0003,  0.0012,  0.0039,  ..., -0.0008,  0.0024,  0.0025],\n",
       "                      [ 0.0068,  0.0033,  0.0019,  ...,  0.0068, -0.0010,  0.0023],\n",
       "                      [ 0.0013,  0.0026, -0.0045,  ..., -0.0012, -0.0016, -0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0030, -0.0053, -0.0045,  ..., -0.0036, -0.0023, -0.0045],\n",
       "                      [ 0.0026, -0.0045,  0.0070,  ..., -0.0027,  0.0037, -0.0002],\n",
       "                      [-0.0068,  0.0006,  0.0017,  ..., -0.0011, -0.0031, -0.0007]],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.31.mlp.up_proj.weight',\n",
       "              tensor([[ 2.1362e-03, -6.0120e-03,  5.0964e-03,  ...,  4.9133e-03,\n",
       "                       -1.4725e-03,  4.5013e-04],\n",
       "                      [-9.2506e-05,  2.0142e-03, -6.1646e-03,  ...,  1.7090e-03,\n",
       "                        1.9455e-03, -5.4321e-03],\n",
       "                      [-1.3351e-03, -2.9755e-03, -5.8746e-04,  ..., -2.2278e-03,\n",
       "                       -2.1973e-03,  2.9297e-03],\n",
       "                      ...,\n",
       "                      [-3.2806e-03,  1.5182e-03,  6.9046e-04,  ...,  2.5482e-03,\n",
       "                        2.7008e-03,  6.2561e-03],\n",
       "                      [ 5.4169e-04,  2.4109e-03,  1.6479e-03,  ..., -1.2817e-03,\n",
       "                       -2.6703e-04,  1.1368e-03],\n",
       "                      [-3.0365e-03, -9.5749e-04,  1.4038e-03,  ..., -5.0354e-03,\n",
       "                        2.7008e-03,  1.9150e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.31.mlp.down_proj.weight',\n",
       "              tensor([[ 1.7700e-03, -2.4414e-03,  1.8463e-03,  ...,  6.5002e-03,\n",
       "                        2.9602e-03,  9.7275e-04],\n",
       "                      [ 2.3193e-03,  6.4087e-04,  2.5330e-03,  ..., -2.4872e-03,\n",
       "                        3.0518e-04, -2.0294e-03],\n",
       "                      [ 4.1723e-06,  2.6855e-03, -2.2736e-03,  ..., -7.0190e-04,\n",
       "                        1.9226e-03, -3.0823e-03],\n",
       "                      ...,\n",
       "                      [-5.1270e-03, -3.0670e-03,  1.8463e-03,  ..., -1.2436e-03,\n",
       "                       -1.8005e-03,  1.9073e-03],\n",
       "                      [ 5.1880e-04, -6.7139e-03,  1.0757e-03,  ..., -2.7008e-03,\n",
       "                        5.3406e-03,  9.0942e-03],\n",
       "                      [-3.6926e-03,  3.5706e-03, -4.7913e-03,  ..., -4.8218e-03,\n",
       "                       -3.9673e-03,  2.0905e-03]], dtype=torch.bfloat16)),\n",
       "             ('model.layers.31.input_layernorm.weight',\n",
       "              tensor([2.5312, 2.5938, 2.5781,  ..., 2.6094, 2.4219, 2.6719],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.layers.31.post_attention_layernorm.weight',\n",
       "              tensor([3.7031, 3.7969, 3.7969,  ..., 3.5312, 3.5312, 3.7188],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('model.norm.weight',\n",
       "              tensor([5.3750, 5.5625, 5.4375,  ..., 5.3125, 5.5938, 5.3125],\n",
       "                     dtype=torch.bfloat16)),\n",
       "             ('lm_head.weight',\n",
       "              tensor([[ 9.4727e-02, -1.6602e-01, -1.2939e-02,  ...,  8.5938e-02,\n",
       "                        1.4062e-01, -1.9727e-01],\n",
       "                      [-2.8076e-03,  5.1856e-06, -2.3804e-03,  ...,  5.0735e-04,\n",
       "                        5.1880e-03,  2.3956e-03],\n",
       "                      [ 3.4943e-03,  4.6082e-03,  2.6550e-03,  ..., -3.0518e-03,\n",
       "                       -3.4027e-03,  2.2736e-03],\n",
       "                      ...,\n",
       "                      [-2.9602e-03,  8.6594e-04, -4.7913e-03,  ...,  3.3112e-03,\n",
       "                        2.4109e-03, -3.1281e-03],\n",
       "                      [-6.4468e-04, -6.0730e-03, -5.6458e-03,  ..., -1.1139e-03,\n",
       "                        4.9133e-03,  1.4191e-03],\n",
       "                      [ 4.5471e-03, -5.2185e-03,  4.5471e-03,  ..., -5.7373e-03,\n",
       "                        2.6550e-03, -1.0452e-03]], dtype=torch.bfloat16))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict= model.state_dict\n",
    "state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model/raftv2\", state_dict=state_dict())\n",
    "#tokenizer.save_pretrained(\"unload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/raft/tokenizer_config.json',\n",
       " 'model/raft/special_tokens_map.json',\n",
       " 'model/raft/tokenizer.model',\n",
       " 'model/raft/added_tokens.json',\n",
       " 'model/raft/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"model/raftv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='checkpoints/raft-v2/checkpoint-250', vocab_size=32768, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[/INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[TOOL_CALLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"[AVAILABLE_TOOLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"[/AVAILABLE_TOOLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"[TOOL_RESULTS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"[/TOOL_RESULTS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"[control_8]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t11: AddedToken(\"[control_9]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t12: AddedToken(\"[control_10]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t13: AddedToken(\"[control_11]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t14: AddedToken(\"[control_12]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t15: AddedToken(\"[control_13]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t16: AddedToken(\"[control_14]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t17: AddedToken(\"[control_15]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t18: AddedToken(\"[control_16]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t19: AddedToken(\"[control_17]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20: AddedToken(\"[control_18]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21: AddedToken(\"[control_19]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t22: AddedToken(\"[control_20]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t23: AddedToken(\"[control_21]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t24: AddedToken(\"[control_22]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t25: AddedToken(\"[control_23]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t26: AddedToken(\"[control_24]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t27: AddedToken(\"[control_25]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t28: AddedToken(\"[control_26]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t29: AddedToken(\"[control_27]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30: AddedToken(\"[control_28]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t31: AddedToken(\"[control_29]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32: AddedToken(\"[control_30]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t33: AddedToken(\"[control_31]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t34: AddedToken(\"[control_32]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t35: AddedToken(\"[control_33]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t36: AddedToken(\"[control_34]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t37: AddedToken(\"[control_35]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t38: AddedToken(\"[control_36]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t39: AddedToken(\"[control_37]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t40: AddedToken(\"[control_38]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t41: AddedToken(\"[control_39]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t42: AddedToken(\"[control_40]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t43: AddedToken(\"[control_41]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t44: AddedToken(\"[control_42]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t45: AddedToken(\"[control_43]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t46: AddedToken(\"[control_44]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t47: AddedToken(\"[control_45]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t48: AddedToken(\"[control_46]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t49: AddedToken(\"[control_47]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50: AddedToken(\"[control_48]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51: AddedToken(\"[control_49]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t52: AddedToken(\"[control_50]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t53: AddedToken(\"[control_51]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t54: AddedToken(\"[control_52]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t55: AddedToken(\"[control_53]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t56: AddedToken(\"[control_54]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57: AddedToken(\"[control_55]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t58: AddedToken(\"[control_56]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59: AddedToken(\"[control_57]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t60: AddedToken(\"[control_58]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t61: AddedToken(\"[control_59]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t62: AddedToken(\"[control_60]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t63: AddedToken(\"[control_61]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t64: AddedToken(\"[control_62]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t65: AddedToken(\"[control_63]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t66: AddedToken(\"[control_64]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t67: AddedToken(\"[control_65]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t68: AddedToken(\"[control_66]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t69: AddedToken(\"[control_67]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t70: AddedToken(\"[control_68]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t71: AddedToken(\"[control_69]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t72: AddedToken(\"[control_70]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t73: AddedToken(\"[control_71]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t74: AddedToken(\"[control_72]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t75: AddedToken(\"[control_73]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t76: AddedToken(\"[control_74]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t77: AddedToken(\"[control_75]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t78: AddedToken(\"[control_76]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t79: AddedToken(\"[control_77]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t80: AddedToken(\"[control_78]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t81: AddedToken(\"[control_79]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t82: AddedToken(\"[control_80]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t83: AddedToken(\"[control_81]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t84: AddedToken(\"[control_82]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t85: AddedToken(\"[control_83]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t86: AddedToken(\"[control_84]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t87: AddedToken(\"[control_85]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t88: AddedToken(\"[control_86]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t89: AddedToken(\"[control_87]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t90: AddedToken(\"[control_88]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t91: AddedToken(\"[control_89]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92: AddedToken(\"[control_90]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t93: AddedToken(\"[control_91]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t94: AddedToken(\"[control_92]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t95: AddedToken(\"[control_93]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t96: AddedToken(\"[control_94]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t97: AddedToken(\"[control_95]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t98: AddedToken(\"[control_96]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t99: AddedToken(\"[control_97]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[control_98]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[control_99]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[control_100]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[control_101]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t104: AddedToken(\"[control_102]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t105: AddedToken(\"[control_103]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t106: AddedToken(\"[control_104]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t107: AddedToken(\"[control_105]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t108: AddedToken(\"[control_106]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t109: AddedToken(\"[control_107]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t110: AddedToken(\"[control_108]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t111: AddedToken(\"[control_109]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t112: AddedToken(\"[control_110]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t113: AddedToken(\"[control_111]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t114: AddedToken(\"[control_112]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t115: AddedToken(\"[control_113]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t116: AddedToken(\"[control_114]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t117: AddedToken(\"[control_115]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t118: AddedToken(\"[control_116]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t119: AddedToken(\"[control_117]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t120: AddedToken(\"[control_118]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t121: AddedToken(\"[control_119]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t122: AddedToken(\"[control_120]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t123: AddedToken(\"[control_121]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t124: AddedToken(\"[control_122]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t125: AddedToken(\"[control_123]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t126: AddedToken(\"[control_124]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t127: AddedToken(\"[control_125]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128: AddedToken(\"[control_126]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t129: AddedToken(\"[control_127]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t130: AddedToken(\"[control_128]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t131: AddedToken(\"[control_129]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t132: AddedToken(\"[control_130]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t133: AddedToken(\"[control_131]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t134: AddedToken(\"[control_132]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t135: AddedToken(\"[control_133]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t136: AddedToken(\"[control_134]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t137: AddedToken(\"[control_135]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t138: AddedToken(\"[control_136]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t139: AddedToken(\"[control_137]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t140: AddedToken(\"[control_138]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t141: AddedToken(\"[control_139]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t142: AddedToken(\"[control_140]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t143: AddedToken(\"[control_141]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t144: AddedToken(\"[control_142]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t145: AddedToken(\"[control_143]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t146: AddedToken(\"[control_144]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t147: AddedToken(\"[control_145]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t148: AddedToken(\"[control_146]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t149: AddedToken(\"[control_147]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t150: AddedToken(\"[control_148]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151: AddedToken(\"[control_149]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t152: AddedToken(\"[control_150]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t153: AddedToken(\"[control_151]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t154: AddedToken(\"[control_152]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t155: AddedToken(\"[control_153]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t156: AddedToken(\"[control_154]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t157: AddedToken(\"[control_155]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t158: AddedToken(\"[control_156]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t159: AddedToken(\"[control_157]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t160: AddedToken(\"[control_158]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t161: AddedToken(\"[control_159]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t162: AddedToken(\"[control_160]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t163: AddedToken(\"[control_161]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t164: AddedToken(\"[control_162]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t165: AddedToken(\"[control_163]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t166: AddedToken(\"[control_164]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t167: AddedToken(\"[control_165]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t168: AddedToken(\"[control_166]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t169: AddedToken(\"[control_167]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t170: AddedToken(\"[control_168]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t171: AddedToken(\"[control_169]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t172: AddedToken(\"[control_170]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t173: AddedToken(\"[control_171]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t174: AddedToken(\"[control_172]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t175: AddedToken(\"[control_173]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t176: AddedToken(\"[control_174]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t177: AddedToken(\"[control_175]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t178: AddedToken(\"[control_176]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t179: AddedToken(\"[control_177]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t180: AddedToken(\"[control_178]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t181: AddedToken(\"[control_179]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t182: AddedToken(\"[control_180]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t183: AddedToken(\"[control_181]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t184: AddedToken(\"[control_182]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t185: AddedToken(\"[control_183]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t186: AddedToken(\"[control_184]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t187: AddedToken(\"[control_185]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t188: AddedToken(\"[control_186]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t189: AddedToken(\"[control_187]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t190: AddedToken(\"[control_188]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t191: AddedToken(\"[control_189]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t192: AddedToken(\"[control_190]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t193: AddedToken(\"[control_191]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t194: AddedToken(\"[control_192]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t195: AddedToken(\"[control_193]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t196: AddedToken(\"[control_194]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t197: AddedToken(\"[control_195]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t198: AddedToken(\"[control_196]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t199: AddedToken(\"[control_197]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200: AddedToken(\"[control_198]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t201: AddedToken(\"[control_199]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t202: AddedToken(\"[control_200]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t203: AddedToken(\"[control_201]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t204: AddedToken(\"[control_202]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t205: AddedToken(\"[control_203]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t206: AddedToken(\"[control_204]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t207: AddedToken(\"[control_205]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t208: AddedToken(\"[control_206]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t209: AddedToken(\"[control_207]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t210: AddedToken(\"[control_208]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t211: AddedToken(\"[control_209]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t212: AddedToken(\"[control_210]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t213: AddedToken(\"[control_211]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t214: AddedToken(\"[control_212]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t215: AddedToken(\"[control_213]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t216: AddedToken(\"[control_214]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t217: AddedToken(\"[control_215]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t218: AddedToken(\"[control_216]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t219: AddedToken(\"[control_217]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t220: AddedToken(\"[control_218]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t221: AddedToken(\"[control_219]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t222: AddedToken(\"[control_220]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t223: AddedToken(\"[control_221]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t224: AddedToken(\"[control_222]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t225: AddedToken(\"[control_223]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t226: AddedToken(\"[control_224]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t227: AddedToken(\"[control_225]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t228: AddedToken(\"[control_226]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t229: AddedToken(\"[control_227]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t230: AddedToken(\"[control_228]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t231: AddedToken(\"[control_229]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t232: AddedToken(\"[control_230]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t233: AddedToken(\"[control_231]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t234: AddedToken(\"[control_232]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t235: AddedToken(\"[control_233]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t236: AddedToken(\"[control_234]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t237: AddedToken(\"[control_235]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t238: AddedToken(\"[control_236]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t239: AddedToken(\"[control_237]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t240: AddedToken(\"[control_238]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t241: AddedToken(\"[control_239]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t242: AddedToken(\"[control_240]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t243: AddedToken(\"[control_241]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t244: AddedToken(\"[control_242]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t245: AddedToken(\"[control_243]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t246: AddedToken(\"[control_244]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t247: AddedToken(\"[control_245]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t248: AddedToken(\"[control_246]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t249: AddedToken(\"[control_247]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t250: AddedToken(\"[control_248]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t251: AddedToken(\"[control_249]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t252: AddedToken(\"[control_250]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t253: AddedToken(\"[control_251]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t254: AddedToken(\"[control_252]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t255: AddedToken(\"[control_253]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256: AddedToken(\"[control_254]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t257: AddedToken(\"[control_255]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t258: AddedToken(\"[control_256]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t259: AddedToken(\"[control_257]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t260: AddedToken(\"[control_258]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t261: AddedToken(\"[control_259]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t262: AddedToken(\"[control_260]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t263: AddedToken(\"[control_261]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t264: AddedToken(\"[control_262]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t265: AddedToken(\"[control_263]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t266: AddedToken(\"[control_264]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t267: AddedToken(\"[control_265]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t268: AddedToken(\"[control_266]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t269: AddedToken(\"[control_267]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t270: AddedToken(\"[control_268]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t271: AddedToken(\"[control_269]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t272: AddedToken(\"[control_270]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t273: AddedToken(\"[control_271]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t274: AddedToken(\"[control_272]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t275: AddedToken(\"[control_273]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t276: AddedToken(\"[control_274]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t277: AddedToken(\"[control_275]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t278: AddedToken(\"[control_276]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t279: AddedToken(\"[control_277]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t280: AddedToken(\"[control_278]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t281: AddedToken(\"[control_279]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t282: AddedToken(\"[control_280]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t283: AddedToken(\"[control_281]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t284: AddedToken(\"[control_282]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t285: AddedToken(\"[control_283]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t286: AddedToken(\"[control_284]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t287: AddedToken(\"[control_285]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t288: AddedToken(\"[control_286]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t289: AddedToken(\"[control_287]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t290: AddedToken(\"[control_288]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t291: AddedToken(\"[control_289]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t292: AddedToken(\"[control_290]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t293: AddedToken(\"[control_291]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t294: AddedToken(\"[control_292]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t295: AddedToken(\"[control_293]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t296: AddedToken(\"[control_294]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t297: AddedToken(\"[control_295]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t298: AddedToken(\"[control_296]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t299: AddedToken(\"[control_297]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t300: AddedToken(\"[control_298]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t301: AddedToken(\"[control_299]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t302: AddedToken(\"[control_300]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t303: AddedToken(\"[control_301]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t304: AddedToken(\"[control_302]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t305: AddedToken(\"[control_303]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t306: AddedToken(\"[control_304]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t307: AddedToken(\"[control_305]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t308: AddedToken(\"[control_306]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t309: AddedToken(\"[control_307]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t310: AddedToken(\"[control_308]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t311: AddedToken(\"[control_309]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t312: AddedToken(\"[control_310]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t313: AddedToken(\"[control_311]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t314: AddedToken(\"[control_312]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t315: AddedToken(\"[control_313]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t316: AddedToken(\"[control_314]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t317: AddedToken(\"[control_315]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t318: AddedToken(\"[control_316]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t319: AddedToken(\"[control_317]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t320: AddedToken(\"[control_318]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t321: AddedToken(\"[control_319]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t322: AddedToken(\"[control_320]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t323: AddedToken(\"[control_321]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t324: AddedToken(\"[control_322]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t325: AddedToken(\"[control_323]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t326: AddedToken(\"[control_324]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t327: AddedToken(\"[control_325]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t328: AddedToken(\"[control_326]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t329: AddedToken(\"[control_327]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t330: AddedToken(\"[control_328]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t331: AddedToken(\"[control_329]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t332: AddedToken(\"[control_330]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t333: AddedToken(\"[control_331]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t334: AddedToken(\"[control_332]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t335: AddedToken(\"[control_333]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t336: AddedToken(\"[control_334]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t337: AddedToken(\"[control_335]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t338: AddedToken(\"[control_336]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t339: AddedToken(\"[control_337]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t340: AddedToken(\"[control_338]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t341: AddedToken(\"[control_339]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t342: AddedToken(\"[control_340]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t343: AddedToken(\"[control_341]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t344: AddedToken(\"[control_342]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t345: AddedToken(\"[control_343]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t346: AddedToken(\"[control_344]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t347: AddedToken(\"[control_345]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t348: AddedToken(\"[control_346]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t349: AddedToken(\"[control_347]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t350: AddedToken(\"[control_348]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t351: AddedToken(\"[control_349]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t352: AddedToken(\"[control_350]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t353: AddedToken(\"[control_351]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t354: AddedToken(\"[control_352]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t355: AddedToken(\"[control_353]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t356: AddedToken(\"[control_354]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t357: AddedToken(\"[control_355]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t358: AddedToken(\"[control_356]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t359: AddedToken(\"[control_357]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t360: AddedToken(\"[control_358]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t361: AddedToken(\"[control_359]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t362: AddedToken(\"[control_360]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t363: AddedToken(\"[control_361]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t364: AddedToken(\"[control_362]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t365: AddedToken(\"[control_363]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t366: AddedToken(\"[control_364]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t367: AddedToken(\"[control_365]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t368: AddedToken(\"[control_366]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t369: AddedToken(\"[control_367]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t370: AddedToken(\"[control_368]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t371: AddedToken(\"[control_369]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t372: AddedToken(\"[control_370]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t373: AddedToken(\"[control_371]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t374: AddedToken(\"[control_372]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t375: AddedToken(\"[control_373]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t376: AddedToken(\"[control_374]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t377: AddedToken(\"[control_375]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t378: AddedToken(\"[control_376]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t379: AddedToken(\"[control_377]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t380: AddedToken(\"[control_378]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t381: AddedToken(\"[control_379]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t382: AddedToken(\"[control_380]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t383: AddedToken(\"[control_381]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t384: AddedToken(\"[control_382]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t385: AddedToken(\"[control_383]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t386: AddedToken(\"[control_384]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t387: AddedToken(\"[control_385]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t388: AddedToken(\"[control_386]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t389: AddedToken(\"[control_387]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t390: AddedToken(\"[control_388]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t391: AddedToken(\"[control_389]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t392: AddedToken(\"[control_390]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t393: AddedToken(\"[control_391]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t394: AddedToken(\"[control_392]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t395: AddedToken(\"[control_393]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t396: AddedToken(\"[control_394]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t397: AddedToken(\"[control_395]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t398: AddedToken(\"[control_396]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t399: AddedToken(\"[control_397]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t400: AddedToken(\"[control_398]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t401: AddedToken(\"[control_399]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t402: AddedToken(\"[control_400]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t403: AddedToken(\"[control_401]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t404: AddedToken(\"[control_402]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t405: AddedToken(\"[control_403]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t406: AddedToken(\"[control_404]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t407: AddedToken(\"[control_405]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t408: AddedToken(\"[control_406]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t409: AddedToken(\"[control_407]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t410: AddedToken(\"[control_408]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t411: AddedToken(\"[control_409]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t412: AddedToken(\"[control_410]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t413: AddedToken(\"[control_411]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t414: AddedToken(\"[control_412]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t415: AddedToken(\"[control_413]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t416: AddedToken(\"[control_414]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t417: AddedToken(\"[control_415]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t418: AddedToken(\"[control_416]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t419: AddedToken(\"[control_417]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t420: AddedToken(\"[control_418]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t421: AddedToken(\"[control_419]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t422: AddedToken(\"[control_420]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t423: AddedToken(\"[control_421]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t424: AddedToken(\"[control_422]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t425: AddedToken(\"[control_423]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t426: AddedToken(\"[control_424]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t427: AddedToken(\"[control_425]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t428: AddedToken(\"[control_426]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t429: AddedToken(\"[control_427]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t430: AddedToken(\"[control_428]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t431: AddedToken(\"[control_429]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t432: AddedToken(\"[control_430]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t433: AddedToken(\"[control_431]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t434: AddedToken(\"[control_432]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t435: AddedToken(\"[control_433]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t436: AddedToken(\"[control_434]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t437: AddedToken(\"[control_435]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t438: AddedToken(\"[control_436]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t439: AddedToken(\"[control_437]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t440: AddedToken(\"[control_438]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t441: AddedToken(\"[control_439]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t442: AddedToken(\"[control_440]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t443: AddedToken(\"[control_441]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t444: AddedToken(\"[control_442]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t445: AddedToken(\"[control_443]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t446: AddedToken(\"[control_444]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t447: AddedToken(\"[control_445]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t448: AddedToken(\"[control_446]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t449: AddedToken(\"[control_447]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t450: AddedToken(\"[control_448]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t451: AddedToken(\"[control_449]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t452: AddedToken(\"[control_450]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t453: AddedToken(\"[control_451]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t454: AddedToken(\"[control_452]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t455: AddedToken(\"[control_453]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t456: AddedToken(\"[control_454]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t457: AddedToken(\"[control_455]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t458: AddedToken(\"[control_456]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t459: AddedToken(\"[control_457]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t460: AddedToken(\"[control_458]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t461: AddedToken(\"[control_459]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t462: AddedToken(\"[control_460]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t463: AddedToken(\"[control_461]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t464: AddedToken(\"[control_462]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t465: AddedToken(\"[control_463]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t466: AddedToken(\"[control_464]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t467: AddedToken(\"[control_465]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t468: AddedToken(\"[control_466]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t469: AddedToken(\"[control_467]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t470: AddedToken(\"[control_468]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t471: AddedToken(\"[control_469]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t472: AddedToken(\"[control_470]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t473: AddedToken(\"[control_471]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t474: AddedToken(\"[control_472]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t475: AddedToken(\"[control_473]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t476: AddedToken(\"[control_474]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t477: AddedToken(\"[control_475]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t478: AddedToken(\"[control_476]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t479: AddedToken(\"[control_477]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t480: AddedToken(\"[control_478]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t481: AddedToken(\"[control_479]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t482: AddedToken(\"[control_480]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t483: AddedToken(\"[control_481]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t484: AddedToken(\"[control_482]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t485: AddedToken(\"[control_483]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t486: AddedToken(\"[control_484]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t487: AddedToken(\"[control_485]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t488: AddedToken(\"[control_486]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t489: AddedToken(\"[control_487]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t490: AddedToken(\"[control_488]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t491: AddedToken(\"[control_489]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t492: AddedToken(\"[control_490]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t493: AddedToken(\"[control_491]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t494: AddedToken(\"[control_492]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t495: AddedToken(\"[control_493]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t496: AddedToken(\"[control_494]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t497: AddedToken(\"[control_495]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t498: AddedToken(\"[control_496]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t499: AddedToken(\"[control_497]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t500: AddedToken(\"[control_498]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t501: AddedToken(\"[control_499]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t502: AddedToken(\"[control_500]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t503: AddedToken(\"[control_501]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t504: AddedToken(\"[control_502]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t505: AddedToken(\"[control_503]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t506: AddedToken(\"[control_504]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t507: AddedToken(\"[control_505]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t508: AddedToken(\"[control_506]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t509: AddedToken(\"[control_507]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t510: AddedToken(\"[control_508]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t511: AddedToken(\"[control_509]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t512: AddedToken(\"[control_510]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t513: AddedToken(\"[control_511]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t514: AddedToken(\"[control_512]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t515: AddedToken(\"[control_513]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t516: AddedToken(\"[control_514]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t517: AddedToken(\"[control_515]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t518: AddedToken(\"[control_516]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t519: AddedToken(\"[control_517]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t520: AddedToken(\"[control_518]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t521: AddedToken(\"[control_519]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t522: AddedToken(\"[control_520]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t523: AddedToken(\"[control_521]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t524: AddedToken(\"[control_522]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t525: AddedToken(\"[control_523]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t526: AddedToken(\"[control_524]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t527: AddedToken(\"[control_525]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t528: AddedToken(\"[control_526]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t529: AddedToken(\"[control_527]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t530: AddedToken(\"[control_528]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t531: AddedToken(\"[control_529]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t532: AddedToken(\"[control_530]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t533: AddedToken(\"[control_531]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t534: AddedToken(\"[control_532]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t535: AddedToken(\"[control_533]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t536: AddedToken(\"[control_534]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t537: AddedToken(\"[control_535]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t538: AddedToken(\"[control_536]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t539: AddedToken(\"[control_537]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t540: AddedToken(\"[control_538]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t541: AddedToken(\"[control_539]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t542: AddedToken(\"[control_540]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t543: AddedToken(\"[control_541]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t544: AddedToken(\"[control_542]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t545: AddedToken(\"[control_543]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t546: AddedToken(\"[control_544]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t547: AddedToken(\"[control_545]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t548: AddedToken(\"[control_546]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t549: AddedToken(\"[control_547]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t550: AddedToken(\"[control_548]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t551: AddedToken(\"[control_549]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t552: AddedToken(\"[control_550]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t553: AddedToken(\"[control_551]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t554: AddedToken(\"[control_552]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t555: AddedToken(\"[control_553]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t556: AddedToken(\"[control_554]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t557: AddedToken(\"[control_555]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t558: AddedToken(\"[control_556]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t559: AddedToken(\"[control_557]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t560: AddedToken(\"[control_558]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t561: AddedToken(\"[control_559]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t562: AddedToken(\"[control_560]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t563: AddedToken(\"[control_561]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t564: AddedToken(\"[control_562]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t565: AddedToken(\"[control_563]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t566: AddedToken(\"[control_564]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t567: AddedToken(\"[control_565]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t568: AddedToken(\"[control_566]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t569: AddedToken(\"[control_567]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t570: AddedToken(\"[control_568]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t571: AddedToken(\"[control_569]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t572: AddedToken(\"[control_570]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t573: AddedToken(\"[control_571]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t574: AddedToken(\"[control_572]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t575: AddedToken(\"[control_573]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t576: AddedToken(\"[control_574]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t577: AddedToken(\"[control_575]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t578: AddedToken(\"[control_576]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t579: AddedToken(\"[control_577]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t580: AddedToken(\"[control_578]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t581: AddedToken(\"[control_579]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t582: AddedToken(\"[control_580]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t583: AddedToken(\"[control_581]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t584: AddedToken(\"[control_582]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t585: AddedToken(\"[control_583]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t586: AddedToken(\"[control_584]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t587: AddedToken(\"[control_585]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t588: AddedToken(\"[control_586]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t589: AddedToken(\"[control_587]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t590: AddedToken(\"[control_588]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t591: AddedToken(\"[control_589]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t592: AddedToken(\"[control_590]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t593: AddedToken(\"[control_591]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t594: AddedToken(\"[control_592]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t595: AddedToken(\"[control_593]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t596: AddedToken(\"[control_594]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t597: AddedToken(\"[control_595]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t598: AddedToken(\"[control_596]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t599: AddedToken(\"[control_597]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t600: AddedToken(\"[control_598]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t601: AddedToken(\"[control_599]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t602: AddedToken(\"[control_600]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t603: AddedToken(\"[control_601]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t604: AddedToken(\"[control_602]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t605: AddedToken(\"[control_603]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t606: AddedToken(\"[control_604]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t607: AddedToken(\"[control_605]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t608: AddedToken(\"[control_606]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t609: AddedToken(\"[control_607]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t610: AddedToken(\"[control_608]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t611: AddedToken(\"[control_609]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t612: AddedToken(\"[control_610]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t613: AddedToken(\"[control_611]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t614: AddedToken(\"[control_612]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t615: AddedToken(\"[control_613]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t616: AddedToken(\"[control_614]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t617: AddedToken(\"[control_615]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t618: AddedToken(\"[control_616]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t619: AddedToken(\"[control_617]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t620: AddedToken(\"[control_618]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t621: AddedToken(\"[control_619]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t622: AddedToken(\"[control_620]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t623: AddedToken(\"[control_621]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t624: AddedToken(\"[control_622]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t625: AddedToken(\"[control_623]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t626: AddedToken(\"[control_624]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t627: AddedToken(\"[control_625]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t628: AddedToken(\"[control_626]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t629: AddedToken(\"[control_627]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t630: AddedToken(\"[control_628]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t631: AddedToken(\"[control_629]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t632: AddedToken(\"[control_630]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t633: AddedToken(\"[control_631]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t634: AddedToken(\"[control_632]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t635: AddedToken(\"[control_633]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t636: AddedToken(\"[control_634]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t637: AddedToken(\"[control_635]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t638: AddedToken(\"[control_636]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t639: AddedToken(\"[control_637]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t640: AddedToken(\"[control_638]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t641: AddedToken(\"[control_639]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t642: AddedToken(\"[control_640]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t643: AddedToken(\"[control_641]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t644: AddedToken(\"[control_642]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t645: AddedToken(\"[control_643]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t646: AddedToken(\"[control_644]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t647: AddedToken(\"[control_645]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t648: AddedToken(\"[control_646]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t649: AddedToken(\"[control_647]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t650: AddedToken(\"[control_648]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t651: AddedToken(\"[control_649]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t652: AddedToken(\"[control_650]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t653: AddedToken(\"[control_651]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t654: AddedToken(\"[control_652]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t655: AddedToken(\"[control_653]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t656: AddedToken(\"[control_654]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t657: AddedToken(\"[control_655]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t658: AddedToken(\"[control_656]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t659: AddedToken(\"[control_657]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t660: AddedToken(\"[control_658]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t661: AddedToken(\"[control_659]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t662: AddedToken(\"[control_660]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t663: AddedToken(\"[control_661]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t664: AddedToken(\"[control_662]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t665: AddedToken(\"[control_663]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t666: AddedToken(\"[control_664]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t667: AddedToken(\"[control_665]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t668: AddedToken(\"[control_666]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t669: AddedToken(\"[control_667]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t670: AddedToken(\"[control_668]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t671: AddedToken(\"[control_669]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t672: AddedToken(\"[control_670]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t673: AddedToken(\"[control_671]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t674: AddedToken(\"[control_672]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t675: AddedToken(\"[control_673]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t676: AddedToken(\"[control_674]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t677: AddedToken(\"[control_675]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t678: AddedToken(\"[control_676]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t679: AddedToken(\"[control_677]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t680: AddedToken(\"[control_678]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t681: AddedToken(\"[control_679]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t682: AddedToken(\"[control_680]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t683: AddedToken(\"[control_681]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t684: AddedToken(\"[control_682]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t685: AddedToken(\"[control_683]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t686: AddedToken(\"[control_684]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t687: AddedToken(\"[control_685]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t688: AddedToken(\"[control_686]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t689: AddedToken(\"[control_687]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t690: AddedToken(\"[control_688]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t691: AddedToken(\"[control_689]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t692: AddedToken(\"[control_690]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t693: AddedToken(\"[control_691]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t694: AddedToken(\"[control_692]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t695: AddedToken(\"[control_693]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t696: AddedToken(\"[control_694]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t697: AddedToken(\"[control_695]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t698: AddedToken(\"[control_696]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t699: AddedToken(\"[control_697]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t700: AddedToken(\"[control_698]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t701: AddedToken(\"[control_699]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t702: AddedToken(\"[control_700]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t703: AddedToken(\"[control_701]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t704: AddedToken(\"[control_702]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t705: AddedToken(\"[control_703]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t706: AddedToken(\"[control_704]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t707: AddedToken(\"[control_705]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t708: AddedToken(\"[control_706]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t709: AddedToken(\"[control_707]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t710: AddedToken(\"[control_708]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t711: AddedToken(\"[control_709]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t712: AddedToken(\"[control_710]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t713: AddedToken(\"[control_711]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t714: AddedToken(\"[control_712]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t715: AddedToken(\"[control_713]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t716: AddedToken(\"[control_714]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t717: AddedToken(\"[control_715]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t718: AddedToken(\"[control_716]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t719: AddedToken(\"[control_717]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t720: AddedToken(\"[control_718]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t721: AddedToken(\"[control_719]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t722: AddedToken(\"[control_720]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t723: AddedToken(\"[control_721]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t724: AddedToken(\"[control_722]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t725: AddedToken(\"[control_723]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t726: AddedToken(\"[control_724]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t727: AddedToken(\"[control_725]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t728: AddedToken(\"[control_726]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t729: AddedToken(\"[control_727]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t730: AddedToken(\"[control_728]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t731: AddedToken(\"[control_729]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t732: AddedToken(\"[control_730]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t733: AddedToken(\"[control_731]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t734: AddedToken(\"[control_732]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t735: AddedToken(\"[control_733]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t736: AddedToken(\"[control_734]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t737: AddedToken(\"[control_735]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t738: AddedToken(\"[control_736]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t739: AddedToken(\"[control_737]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t740: AddedToken(\"[control_738]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t741: AddedToken(\"[control_739]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t742: AddedToken(\"[control_740]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t743: AddedToken(\"[control_741]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t744: AddedToken(\"[control_742]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t745: AddedToken(\"[control_743]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t746: AddedToken(\"[control_744]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t747: AddedToken(\"[control_745]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t748: AddedToken(\"[control_746]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t749: AddedToken(\"[control_747]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t750: AddedToken(\"[control_748]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t751: AddedToken(\"[control_749]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t752: AddedToken(\"[control_750]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t753: AddedToken(\"[control_751]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t754: AddedToken(\"[control_752]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t755: AddedToken(\"[control_753]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t756: AddedToken(\"[control_754]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t757: AddedToken(\"[control_755]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t758: AddedToken(\"[control_756]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t759: AddedToken(\"[control_757]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t760: AddedToken(\"[control_758]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t761: AddedToken(\"[control_759]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t762: AddedToken(\"[control_760]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t763: AddedToken(\"[control_761]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t764: AddedToken(\"[control_762]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t765: AddedToken(\"[control_763]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t766: AddedToken(\"[control_764]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t767: AddedToken(\"[control_765]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t768: AddedToken(\"[control_766]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t769: AddedToken(\"[control_767]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t770: AddedToken(\"[control_768]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoints/raft/checkpoint-425\n",
    "# checkpoints/raft-v2/checkpoint-250\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"checkpoints/raft-v2/checkpoint-250\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/raftv2/tokenizer_config.json',\n",
       " 'model/raftv2/special_tokens_map.json',\n",
       " 'model/raftv2/tokenizer.model',\n",
       " 'model/raftv2/added_tokens.json',\n",
       " 'model/raftv2/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"model/raftv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.3', vocab_size=32768, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[/INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[TOOL_CALLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"[AVAILABLE_TOOLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"[/AVAILABLE_TOOLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"[TOOL_RESULTS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"[/TOOL_RESULTS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"[control_8]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t11: AddedToken(\"[control_9]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t12: AddedToken(\"[control_10]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t13: AddedToken(\"[control_11]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t14: AddedToken(\"[control_12]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t15: AddedToken(\"[control_13]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t16: AddedToken(\"[control_14]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t17: AddedToken(\"[control_15]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t18: AddedToken(\"[control_16]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t19: AddedToken(\"[control_17]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20: AddedToken(\"[control_18]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21: AddedToken(\"[control_19]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t22: AddedToken(\"[control_20]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t23: AddedToken(\"[control_21]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t24: AddedToken(\"[control_22]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t25: AddedToken(\"[control_23]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t26: AddedToken(\"[control_24]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t27: AddedToken(\"[control_25]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t28: AddedToken(\"[control_26]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t29: AddedToken(\"[control_27]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30: AddedToken(\"[control_28]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t31: AddedToken(\"[control_29]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32: AddedToken(\"[control_30]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t33: AddedToken(\"[control_31]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t34: AddedToken(\"[control_32]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t35: AddedToken(\"[control_33]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t36: AddedToken(\"[control_34]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t37: AddedToken(\"[control_35]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t38: AddedToken(\"[control_36]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t39: AddedToken(\"[control_37]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t40: AddedToken(\"[control_38]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t41: AddedToken(\"[control_39]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t42: AddedToken(\"[control_40]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t43: AddedToken(\"[control_41]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t44: AddedToken(\"[control_42]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t45: AddedToken(\"[control_43]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t46: AddedToken(\"[control_44]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t47: AddedToken(\"[control_45]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t48: AddedToken(\"[control_46]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t49: AddedToken(\"[control_47]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50: AddedToken(\"[control_48]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51: AddedToken(\"[control_49]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t52: AddedToken(\"[control_50]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t53: AddedToken(\"[control_51]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t54: AddedToken(\"[control_52]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t55: AddedToken(\"[control_53]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t56: AddedToken(\"[control_54]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57: AddedToken(\"[control_55]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t58: AddedToken(\"[control_56]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59: AddedToken(\"[control_57]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t60: AddedToken(\"[control_58]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t61: AddedToken(\"[control_59]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t62: AddedToken(\"[control_60]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t63: AddedToken(\"[control_61]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t64: AddedToken(\"[control_62]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t65: AddedToken(\"[control_63]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t66: AddedToken(\"[control_64]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t67: AddedToken(\"[control_65]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t68: AddedToken(\"[control_66]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t69: AddedToken(\"[control_67]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t70: AddedToken(\"[control_68]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t71: AddedToken(\"[control_69]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t72: AddedToken(\"[control_70]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t73: AddedToken(\"[control_71]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t74: AddedToken(\"[control_72]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t75: AddedToken(\"[control_73]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t76: AddedToken(\"[control_74]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t77: AddedToken(\"[control_75]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t78: AddedToken(\"[control_76]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t79: AddedToken(\"[control_77]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t80: AddedToken(\"[control_78]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t81: AddedToken(\"[control_79]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t82: AddedToken(\"[control_80]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t83: AddedToken(\"[control_81]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t84: AddedToken(\"[control_82]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t85: AddedToken(\"[control_83]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t86: AddedToken(\"[control_84]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t87: AddedToken(\"[control_85]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t88: AddedToken(\"[control_86]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t89: AddedToken(\"[control_87]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t90: AddedToken(\"[control_88]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t91: AddedToken(\"[control_89]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92: AddedToken(\"[control_90]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t93: AddedToken(\"[control_91]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t94: AddedToken(\"[control_92]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t95: AddedToken(\"[control_93]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t96: AddedToken(\"[control_94]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t97: AddedToken(\"[control_95]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t98: AddedToken(\"[control_96]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t99: AddedToken(\"[control_97]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[control_98]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[control_99]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[control_100]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[control_101]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t104: AddedToken(\"[control_102]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t105: AddedToken(\"[control_103]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t106: AddedToken(\"[control_104]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t107: AddedToken(\"[control_105]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t108: AddedToken(\"[control_106]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t109: AddedToken(\"[control_107]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t110: AddedToken(\"[control_108]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t111: AddedToken(\"[control_109]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t112: AddedToken(\"[control_110]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t113: AddedToken(\"[control_111]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t114: AddedToken(\"[control_112]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t115: AddedToken(\"[control_113]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t116: AddedToken(\"[control_114]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t117: AddedToken(\"[control_115]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t118: AddedToken(\"[control_116]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t119: AddedToken(\"[control_117]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t120: AddedToken(\"[control_118]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t121: AddedToken(\"[control_119]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t122: AddedToken(\"[control_120]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t123: AddedToken(\"[control_121]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t124: AddedToken(\"[control_122]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t125: AddedToken(\"[control_123]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t126: AddedToken(\"[control_124]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t127: AddedToken(\"[control_125]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128: AddedToken(\"[control_126]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t129: AddedToken(\"[control_127]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t130: AddedToken(\"[control_128]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t131: AddedToken(\"[control_129]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t132: AddedToken(\"[control_130]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t133: AddedToken(\"[control_131]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t134: AddedToken(\"[control_132]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t135: AddedToken(\"[control_133]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t136: AddedToken(\"[control_134]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t137: AddedToken(\"[control_135]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t138: AddedToken(\"[control_136]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t139: AddedToken(\"[control_137]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t140: AddedToken(\"[control_138]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t141: AddedToken(\"[control_139]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t142: AddedToken(\"[control_140]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t143: AddedToken(\"[control_141]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t144: AddedToken(\"[control_142]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t145: AddedToken(\"[control_143]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t146: AddedToken(\"[control_144]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t147: AddedToken(\"[control_145]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t148: AddedToken(\"[control_146]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t149: AddedToken(\"[control_147]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t150: AddedToken(\"[control_148]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151: AddedToken(\"[control_149]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t152: AddedToken(\"[control_150]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t153: AddedToken(\"[control_151]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t154: AddedToken(\"[control_152]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t155: AddedToken(\"[control_153]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t156: AddedToken(\"[control_154]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t157: AddedToken(\"[control_155]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t158: AddedToken(\"[control_156]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t159: AddedToken(\"[control_157]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t160: AddedToken(\"[control_158]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t161: AddedToken(\"[control_159]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t162: AddedToken(\"[control_160]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t163: AddedToken(\"[control_161]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t164: AddedToken(\"[control_162]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t165: AddedToken(\"[control_163]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t166: AddedToken(\"[control_164]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t167: AddedToken(\"[control_165]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t168: AddedToken(\"[control_166]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t169: AddedToken(\"[control_167]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t170: AddedToken(\"[control_168]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t171: AddedToken(\"[control_169]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t172: AddedToken(\"[control_170]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t173: AddedToken(\"[control_171]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t174: AddedToken(\"[control_172]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t175: AddedToken(\"[control_173]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t176: AddedToken(\"[control_174]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t177: AddedToken(\"[control_175]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t178: AddedToken(\"[control_176]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t179: AddedToken(\"[control_177]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t180: AddedToken(\"[control_178]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t181: AddedToken(\"[control_179]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t182: AddedToken(\"[control_180]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t183: AddedToken(\"[control_181]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t184: AddedToken(\"[control_182]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t185: AddedToken(\"[control_183]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t186: AddedToken(\"[control_184]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t187: AddedToken(\"[control_185]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t188: AddedToken(\"[control_186]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t189: AddedToken(\"[control_187]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t190: AddedToken(\"[control_188]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t191: AddedToken(\"[control_189]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t192: AddedToken(\"[control_190]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t193: AddedToken(\"[control_191]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t194: AddedToken(\"[control_192]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t195: AddedToken(\"[control_193]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t196: AddedToken(\"[control_194]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t197: AddedToken(\"[control_195]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t198: AddedToken(\"[control_196]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t199: AddedToken(\"[control_197]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200: AddedToken(\"[control_198]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t201: AddedToken(\"[control_199]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t202: AddedToken(\"[control_200]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t203: AddedToken(\"[control_201]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t204: AddedToken(\"[control_202]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t205: AddedToken(\"[control_203]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t206: AddedToken(\"[control_204]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t207: AddedToken(\"[control_205]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t208: AddedToken(\"[control_206]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t209: AddedToken(\"[control_207]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t210: AddedToken(\"[control_208]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t211: AddedToken(\"[control_209]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t212: AddedToken(\"[control_210]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t213: AddedToken(\"[control_211]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t214: AddedToken(\"[control_212]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t215: AddedToken(\"[control_213]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t216: AddedToken(\"[control_214]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t217: AddedToken(\"[control_215]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t218: AddedToken(\"[control_216]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t219: AddedToken(\"[control_217]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t220: AddedToken(\"[control_218]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t221: AddedToken(\"[control_219]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t222: AddedToken(\"[control_220]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t223: AddedToken(\"[control_221]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t224: AddedToken(\"[control_222]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t225: AddedToken(\"[control_223]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t226: AddedToken(\"[control_224]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t227: AddedToken(\"[control_225]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t228: AddedToken(\"[control_226]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t229: AddedToken(\"[control_227]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t230: AddedToken(\"[control_228]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t231: AddedToken(\"[control_229]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t232: AddedToken(\"[control_230]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t233: AddedToken(\"[control_231]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t234: AddedToken(\"[control_232]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t235: AddedToken(\"[control_233]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t236: AddedToken(\"[control_234]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t237: AddedToken(\"[control_235]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t238: AddedToken(\"[control_236]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t239: AddedToken(\"[control_237]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t240: AddedToken(\"[control_238]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t241: AddedToken(\"[control_239]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t242: AddedToken(\"[control_240]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t243: AddedToken(\"[control_241]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t244: AddedToken(\"[control_242]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t245: AddedToken(\"[control_243]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t246: AddedToken(\"[control_244]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t247: AddedToken(\"[control_245]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t248: AddedToken(\"[control_246]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t249: AddedToken(\"[control_247]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t250: AddedToken(\"[control_248]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t251: AddedToken(\"[control_249]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t252: AddedToken(\"[control_250]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t253: AddedToken(\"[control_251]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t254: AddedToken(\"[control_252]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t255: AddedToken(\"[control_253]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256: AddedToken(\"[control_254]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t257: AddedToken(\"[control_255]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t258: AddedToken(\"[control_256]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t259: AddedToken(\"[control_257]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t260: AddedToken(\"[control_258]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t261: AddedToken(\"[control_259]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t262: AddedToken(\"[control_260]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t263: AddedToken(\"[control_261]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t264: AddedToken(\"[control_262]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t265: AddedToken(\"[control_263]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t266: AddedToken(\"[control_264]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t267: AddedToken(\"[control_265]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t268: AddedToken(\"[control_266]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t269: AddedToken(\"[control_267]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t270: AddedToken(\"[control_268]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t271: AddedToken(\"[control_269]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t272: AddedToken(\"[control_270]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t273: AddedToken(\"[control_271]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t274: AddedToken(\"[control_272]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t275: AddedToken(\"[control_273]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t276: AddedToken(\"[control_274]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t277: AddedToken(\"[control_275]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t278: AddedToken(\"[control_276]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t279: AddedToken(\"[control_277]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t280: AddedToken(\"[control_278]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t281: AddedToken(\"[control_279]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t282: AddedToken(\"[control_280]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t283: AddedToken(\"[control_281]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t284: AddedToken(\"[control_282]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t285: AddedToken(\"[control_283]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t286: AddedToken(\"[control_284]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t287: AddedToken(\"[control_285]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t288: AddedToken(\"[control_286]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t289: AddedToken(\"[control_287]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t290: AddedToken(\"[control_288]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t291: AddedToken(\"[control_289]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t292: AddedToken(\"[control_290]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t293: AddedToken(\"[control_291]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t294: AddedToken(\"[control_292]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t295: AddedToken(\"[control_293]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t296: AddedToken(\"[control_294]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t297: AddedToken(\"[control_295]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t298: AddedToken(\"[control_296]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t299: AddedToken(\"[control_297]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t300: AddedToken(\"[control_298]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t301: AddedToken(\"[control_299]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t302: AddedToken(\"[control_300]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t303: AddedToken(\"[control_301]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t304: AddedToken(\"[control_302]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t305: AddedToken(\"[control_303]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t306: AddedToken(\"[control_304]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t307: AddedToken(\"[control_305]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t308: AddedToken(\"[control_306]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t309: AddedToken(\"[control_307]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t310: AddedToken(\"[control_308]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t311: AddedToken(\"[control_309]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t312: AddedToken(\"[control_310]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t313: AddedToken(\"[control_311]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t314: AddedToken(\"[control_312]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t315: AddedToken(\"[control_313]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t316: AddedToken(\"[control_314]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t317: AddedToken(\"[control_315]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t318: AddedToken(\"[control_316]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t319: AddedToken(\"[control_317]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t320: AddedToken(\"[control_318]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t321: AddedToken(\"[control_319]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t322: AddedToken(\"[control_320]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t323: AddedToken(\"[control_321]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t324: AddedToken(\"[control_322]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t325: AddedToken(\"[control_323]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t326: AddedToken(\"[control_324]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t327: AddedToken(\"[control_325]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t328: AddedToken(\"[control_326]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t329: AddedToken(\"[control_327]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t330: AddedToken(\"[control_328]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t331: AddedToken(\"[control_329]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t332: AddedToken(\"[control_330]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t333: AddedToken(\"[control_331]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t334: AddedToken(\"[control_332]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t335: AddedToken(\"[control_333]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t336: AddedToken(\"[control_334]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t337: AddedToken(\"[control_335]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t338: AddedToken(\"[control_336]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t339: AddedToken(\"[control_337]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t340: AddedToken(\"[control_338]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t341: AddedToken(\"[control_339]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t342: AddedToken(\"[control_340]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t343: AddedToken(\"[control_341]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t344: AddedToken(\"[control_342]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t345: AddedToken(\"[control_343]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t346: AddedToken(\"[control_344]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t347: AddedToken(\"[control_345]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t348: AddedToken(\"[control_346]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t349: AddedToken(\"[control_347]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t350: AddedToken(\"[control_348]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t351: AddedToken(\"[control_349]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t352: AddedToken(\"[control_350]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t353: AddedToken(\"[control_351]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t354: AddedToken(\"[control_352]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t355: AddedToken(\"[control_353]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t356: AddedToken(\"[control_354]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t357: AddedToken(\"[control_355]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t358: AddedToken(\"[control_356]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t359: AddedToken(\"[control_357]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t360: AddedToken(\"[control_358]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t361: AddedToken(\"[control_359]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t362: AddedToken(\"[control_360]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t363: AddedToken(\"[control_361]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t364: AddedToken(\"[control_362]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t365: AddedToken(\"[control_363]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t366: AddedToken(\"[control_364]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t367: AddedToken(\"[control_365]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t368: AddedToken(\"[control_366]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t369: AddedToken(\"[control_367]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t370: AddedToken(\"[control_368]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t371: AddedToken(\"[control_369]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t372: AddedToken(\"[control_370]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t373: AddedToken(\"[control_371]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t374: AddedToken(\"[control_372]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t375: AddedToken(\"[control_373]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t376: AddedToken(\"[control_374]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t377: AddedToken(\"[control_375]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t378: AddedToken(\"[control_376]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t379: AddedToken(\"[control_377]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t380: AddedToken(\"[control_378]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t381: AddedToken(\"[control_379]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t382: AddedToken(\"[control_380]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t383: AddedToken(\"[control_381]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t384: AddedToken(\"[control_382]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t385: AddedToken(\"[control_383]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t386: AddedToken(\"[control_384]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t387: AddedToken(\"[control_385]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t388: AddedToken(\"[control_386]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t389: AddedToken(\"[control_387]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t390: AddedToken(\"[control_388]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t391: AddedToken(\"[control_389]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t392: AddedToken(\"[control_390]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t393: AddedToken(\"[control_391]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t394: AddedToken(\"[control_392]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t395: AddedToken(\"[control_393]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t396: AddedToken(\"[control_394]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t397: AddedToken(\"[control_395]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t398: AddedToken(\"[control_396]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t399: AddedToken(\"[control_397]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t400: AddedToken(\"[control_398]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t401: AddedToken(\"[control_399]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t402: AddedToken(\"[control_400]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t403: AddedToken(\"[control_401]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t404: AddedToken(\"[control_402]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t405: AddedToken(\"[control_403]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t406: AddedToken(\"[control_404]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t407: AddedToken(\"[control_405]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t408: AddedToken(\"[control_406]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t409: AddedToken(\"[control_407]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t410: AddedToken(\"[control_408]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t411: AddedToken(\"[control_409]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t412: AddedToken(\"[control_410]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t413: AddedToken(\"[control_411]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t414: AddedToken(\"[control_412]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t415: AddedToken(\"[control_413]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t416: AddedToken(\"[control_414]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t417: AddedToken(\"[control_415]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t418: AddedToken(\"[control_416]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t419: AddedToken(\"[control_417]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t420: AddedToken(\"[control_418]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t421: AddedToken(\"[control_419]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t422: AddedToken(\"[control_420]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t423: AddedToken(\"[control_421]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t424: AddedToken(\"[control_422]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t425: AddedToken(\"[control_423]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t426: AddedToken(\"[control_424]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t427: AddedToken(\"[control_425]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t428: AddedToken(\"[control_426]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t429: AddedToken(\"[control_427]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t430: AddedToken(\"[control_428]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t431: AddedToken(\"[control_429]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t432: AddedToken(\"[control_430]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t433: AddedToken(\"[control_431]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t434: AddedToken(\"[control_432]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t435: AddedToken(\"[control_433]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t436: AddedToken(\"[control_434]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t437: AddedToken(\"[control_435]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t438: AddedToken(\"[control_436]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t439: AddedToken(\"[control_437]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t440: AddedToken(\"[control_438]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t441: AddedToken(\"[control_439]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t442: AddedToken(\"[control_440]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t443: AddedToken(\"[control_441]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t444: AddedToken(\"[control_442]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t445: AddedToken(\"[control_443]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t446: AddedToken(\"[control_444]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t447: AddedToken(\"[control_445]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t448: AddedToken(\"[control_446]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t449: AddedToken(\"[control_447]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t450: AddedToken(\"[control_448]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t451: AddedToken(\"[control_449]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t452: AddedToken(\"[control_450]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t453: AddedToken(\"[control_451]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t454: AddedToken(\"[control_452]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t455: AddedToken(\"[control_453]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t456: AddedToken(\"[control_454]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t457: AddedToken(\"[control_455]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t458: AddedToken(\"[control_456]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t459: AddedToken(\"[control_457]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t460: AddedToken(\"[control_458]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t461: AddedToken(\"[control_459]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t462: AddedToken(\"[control_460]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t463: AddedToken(\"[control_461]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t464: AddedToken(\"[control_462]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t465: AddedToken(\"[control_463]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t466: AddedToken(\"[control_464]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t467: AddedToken(\"[control_465]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t468: AddedToken(\"[control_466]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t469: AddedToken(\"[control_467]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t470: AddedToken(\"[control_468]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t471: AddedToken(\"[control_469]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t472: AddedToken(\"[control_470]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t473: AddedToken(\"[control_471]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t474: AddedToken(\"[control_472]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t475: AddedToken(\"[control_473]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t476: AddedToken(\"[control_474]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t477: AddedToken(\"[control_475]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t478: AddedToken(\"[control_476]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t479: AddedToken(\"[control_477]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t480: AddedToken(\"[control_478]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t481: AddedToken(\"[control_479]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t482: AddedToken(\"[control_480]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t483: AddedToken(\"[control_481]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t484: AddedToken(\"[control_482]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t485: AddedToken(\"[control_483]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t486: AddedToken(\"[control_484]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t487: AddedToken(\"[control_485]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t488: AddedToken(\"[control_486]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t489: AddedToken(\"[control_487]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t490: AddedToken(\"[control_488]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t491: AddedToken(\"[control_489]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t492: AddedToken(\"[control_490]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t493: AddedToken(\"[control_491]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t494: AddedToken(\"[control_492]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t495: AddedToken(\"[control_493]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t496: AddedToken(\"[control_494]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t497: AddedToken(\"[control_495]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t498: AddedToken(\"[control_496]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t499: AddedToken(\"[control_497]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t500: AddedToken(\"[control_498]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t501: AddedToken(\"[control_499]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t502: AddedToken(\"[control_500]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t503: AddedToken(\"[control_501]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t504: AddedToken(\"[control_502]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t505: AddedToken(\"[control_503]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t506: AddedToken(\"[control_504]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t507: AddedToken(\"[control_505]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t508: AddedToken(\"[control_506]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t509: AddedToken(\"[control_507]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t510: AddedToken(\"[control_508]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t511: AddedToken(\"[control_509]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t512: AddedToken(\"[control_510]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t513: AddedToken(\"[control_511]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t514: AddedToken(\"[control_512]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t515: AddedToken(\"[control_513]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t516: AddedToken(\"[control_514]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t517: AddedToken(\"[control_515]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t518: AddedToken(\"[control_516]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t519: AddedToken(\"[control_517]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t520: AddedToken(\"[control_518]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t521: AddedToken(\"[control_519]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t522: AddedToken(\"[control_520]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t523: AddedToken(\"[control_521]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t524: AddedToken(\"[control_522]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t525: AddedToken(\"[control_523]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t526: AddedToken(\"[control_524]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t527: AddedToken(\"[control_525]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t528: AddedToken(\"[control_526]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t529: AddedToken(\"[control_527]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t530: AddedToken(\"[control_528]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t531: AddedToken(\"[control_529]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t532: AddedToken(\"[control_530]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t533: AddedToken(\"[control_531]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t534: AddedToken(\"[control_532]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t535: AddedToken(\"[control_533]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t536: AddedToken(\"[control_534]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t537: AddedToken(\"[control_535]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t538: AddedToken(\"[control_536]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t539: AddedToken(\"[control_537]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t540: AddedToken(\"[control_538]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t541: AddedToken(\"[control_539]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t542: AddedToken(\"[control_540]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t543: AddedToken(\"[control_541]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t544: AddedToken(\"[control_542]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t545: AddedToken(\"[control_543]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t546: AddedToken(\"[control_544]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t547: AddedToken(\"[control_545]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t548: AddedToken(\"[control_546]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t549: AddedToken(\"[control_547]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t550: AddedToken(\"[control_548]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t551: AddedToken(\"[control_549]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t552: AddedToken(\"[control_550]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t553: AddedToken(\"[control_551]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t554: AddedToken(\"[control_552]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t555: AddedToken(\"[control_553]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t556: AddedToken(\"[control_554]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t557: AddedToken(\"[control_555]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t558: AddedToken(\"[control_556]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t559: AddedToken(\"[control_557]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t560: AddedToken(\"[control_558]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t561: AddedToken(\"[control_559]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t562: AddedToken(\"[control_560]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t563: AddedToken(\"[control_561]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t564: AddedToken(\"[control_562]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t565: AddedToken(\"[control_563]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t566: AddedToken(\"[control_564]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t567: AddedToken(\"[control_565]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t568: AddedToken(\"[control_566]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t569: AddedToken(\"[control_567]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t570: AddedToken(\"[control_568]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t571: AddedToken(\"[control_569]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t572: AddedToken(\"[control_570]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t573: AddedToken(\"[control_571]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t574: AddedToken(\"[control_572]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t575: AddedToken(\"[control_573]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t576: AddedToken(\"[control_574]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t577: AddedToken(\"[control_575]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t578: AddedToken(\"[control_576]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t579: AddedToken(\"[control_577]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t580: AddedToken(\"[control_578]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t581: AddedToken(\"[control_579]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t582: AddedToken(\"[control_580]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t583: AddedToken(\"[control_581]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t584: AddedToken(\"[control_582]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t585: AddedToken(\"[control_583]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t586: AddedToken(\"[control_584]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t587: AddedToken(\"[control_585]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t588: AddedToken(\"[control_586]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t589: AddedToken(\"[control_587]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t590: AddedToken(\"[control_588]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t591: AddedToken(\"[control_589]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t592: AddedToken(\"[control_590]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t593: AddedToken(\"[control_591]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t594: AddedToken(\"[control_592]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t595: AddedToken(\"[control_593]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t596: AddedToken(\"[control_594]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t597: AddedToken(\"[control_595]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t598: AddedToken(\"[control_596]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t599: AddedToken(\"[control_597]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t600: AddedToken(\"[control_598]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t601: AddedToken(\"[control_599]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t602: AddedToken(\"[control_600]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t603: AddedToken(\"[control_601]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t604: AddedToken(\"[control_602]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t605: AddedToken(\"[control_603]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t606: AddedToken(\"[control_604]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t607: AddedToken(\"[control_605]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t608: AddedToken(\"[control_606]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t609: AddedToken(\"[control_607]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t610: AddedToken(\"[control_608]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t611: AddedToken(\"[control_609]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t612: AddedToken(\"[control_610]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t613: AddedToken(\"[control_611]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t614: AddedToken(\"[control_612]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t615: AddedToken(\"[control_613]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t616: AddedToken(\"[control_614]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t617: AddedToken(\"[control_615]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t618: AddedToken(\"[control_616]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t619: AddedToken(\"[control_617]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t620: AddedToken(\"[control_618]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t621: AddedToken(\"[control_619]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t622: AddedToken(\"[control_620]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t623: AddedToken(\"[control_621]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t624: AddedToken(\"[control_622]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t625: AddedToken(\"[control_623]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t626: AddedToken(\"[control_624]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t627: AddedToken(\"[control_625]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t628: AddedToken(\"[control_626]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t629: AddedToken(\"[control_627]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t630: AddedToken(\"[control_628]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t631: AddedToken(\"[control_629]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t632: AddedToken(\"[control_630]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t633: AddedToken(\"[control_631]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t634: AddedToken(\"[control_632]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t635: AddedToken(\"[control_633]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t636: AddedToken(\"[control_634]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t637: AddedToken(\"[control_635]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t638: AddedToken(\"[control_636]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t639: AddedToken(\"[control_637]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t640: AddedToken(\"[control_638]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t641: AddedToken(\"[control_639]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t642: AddedToken(\"[control_640]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t643: AddedToken(\"[control_641]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t644: AddedToken(\"[control_642]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t645: AddedToken(\"[control_643]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t646: AddedToken(\"[control_644]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t647: AddedToken(\"[control_645]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t648: AddedToken(\"[control_646]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t649: AddedToken(\"[control_647]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t650: AddedToken(\"[control_648]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t651: AddedToken(\"[control_649]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t652: AddedToken(\"[control_650]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t653: AddedToken(\"[control_651]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t654: AddedToken(\"[control_652]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t655: AddedToken(\"[control_653]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t656: AddedToken(\"[control_654]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t657: AddedToken(\"[control_655]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t658: AddedToken(\"[control_656]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t659: AddedToken(\"[control_657]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t660: AddedToken(\"[control_658]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t661: AddedToken(\"[control_659]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t662: AddedToken(\"[control_660]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t663: AddedToken(\"[control_661]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t664: AddedToken(\"[control_662]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t665: AddedToken(\"[control_663]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t666: AddedToken(\"[control_664]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t667: AddedToken(\"[control_665]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t668: AddedToken(\"[control_666]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t669: AddedToken(\"[control_667]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t670: AddedToken(\"[control_668]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t671: AddedToken(\"[control_669]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t672: AddedToken(\"[control_670]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t673: AddedToken(\"[control_671]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t674: AddedToken(\"[control_672]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t675: AddedToken(\"[control_673]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t676: AddedToken(\"[control_674]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t677: AddedToken(\"[control_675]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t678: AddedToken(\"[control_676]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t679: AddedToken(\"[control_677]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t680: AddedToken(\"[control_678]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t681: AddedToken(\"[control_679]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t682: AddedToken(\"[control_680]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t683: AddedToken(\"[control_681]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t684: AddedToken(\"[control_682]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t685: AddedToken(\"[control_683]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t686: AddedToken(\"[control_684]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t687: AddedToken(\"[control_685]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t688: AddedToken(\"[control_686]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t689: AddedToken(\"[control_687]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t690: AddedToken(\"[control_688]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t691: AddedToken(\"[control_689]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t692: AddedToken(\"[control_690]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t693: AddedToken(\"[control_691]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t694: AddedToken(\"[control_692]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t695: AddedToken(\"[control_693]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t696: AddedToken(\"[control_694]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t697: AddedToken(\"[control_695]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t698: AddedToken(\"[control_696]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t699: AddedToken(\"[control_697]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t700: AddedToken(\"[control_698]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t701: AddedToken(\"[control_699]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t702: AddedToken(\"[control_700]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t703: AddedToken(\"[control_701]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t704: AddedToken(\"[control_702]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t705: AddedToken(\"[control_703]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t706: AddedToken(\"[control_704]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t707: AddedToken(\"[control_705]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t708: AddedToken(\"[control_706]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t709: AddedToken(\"[control_707]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t710: AddedToken(\"[control_708]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t711: AddedToken(\"[control_709]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t712: AddedToken(\"[control_710]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t713: AddedToken(\"[control_711]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t714: AddedToken(\"[control_712]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t715: AddedToken(\"[control_713]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t716: AddedToken(\"[control_714]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t717: AddedToken(\"[control_715]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t718: AddedToken(\"[control_716]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t719: AddedToken(\"[control_717]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t720: AddedToken(\"[control_718]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t721: AddedToken(\"[control_719]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t722: AddedToken(\"[control_720]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t723: AddedToken(\"[control_721]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t724: AddedToken(\"[control_722]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t725: AddedToken(\"[control_723]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t726: AddedToken(\"[control_724]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t727: AddedToken(\"[control_725]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t728: AddedToken(\"[control_726]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t729: AddedToken(\"[control_727]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t730: AddedToken(\"[control_728]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t731: AddedToken(\"[control_729]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t732: AddedToken(\"[control_730]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t733: AddedToken(\"[control_731]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t734: AddedToken(\"[control_732]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t735: AddedToken(\"[control_733]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t736: AddedToken(\"[control_734]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t737: AddedToken(\"[control_735]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t738: AddedToken(\"[control_736]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t739: AddedToken(\"[control_737]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t740: AddedToken(\"[control_738]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t741: AddedToken(\"[control_739]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t742: AddedToken(\"[control_740]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t743: AddedToken(\"[control_741]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t744: AddedToken(\"[control_742]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t745: AddedToken(\"[control_743]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t746: AddedToken(\"[control_744]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t747: AddedToken(\"[control_745]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t748: AddedToken(\"[control_746]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t749: AddedToken(\"[control_747]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t750: AddedToken(\"[control_748]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t751: AddedToken(\"[control_749]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t752: AddedToken(\"[control_750]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t753: AddedToken(\"[control_751]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t754: AddedToken(\"[control_752]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t755: AddedToken(\"[control_753]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t756: AddedToken(\"[control_754]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t757: AddedToken(\"[control_755]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t758: AddedToken(\"[control_756]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t759: AddedToken(\"[control_757]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t760: AddedToken(\"[control_758]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t761: AddedToken(\"[control_759]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t762: AddedToken(\"[control_760]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t763: AddedToken(\"[control_761]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t764: AddedToken(\"[control_762]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t765: AddedToken(\"[control_763]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t766: AddedToken(\"[control_764]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t767: AddedToken(\"[control_765]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t768: AddedToken(\"[control_766]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t769: AddedToken(\"[control_767]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t770: AddedToken(\"[control_768]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save to awq quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5047bca64b09457bb0eccdb54cebd24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "AWQ: 100%|██████████| 32/32 [17:05<00:00, 32.05s/it]\n",
      "Note that `shard_checkpoint` is deprecated and will be removed in v4.44. We recommend you using split_torch_state_dict_into_shards from huggingface_hub library\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('merged_model-AWQ/tokenizer_config.json',\n",
       " 'merged_model-AWQ/special_tokens_map.json',\n",
       " 'merged_model-AWQ/tokenizer.model',\n",
       " 'merged_model-AWQ/added_tokens.json',\n",
       " 'merged_model-AWQ/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = AutoAWQForCausalLM.from_pretrained( 'unload', device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained( 'unload' )\n",
    "\n",
    "quant_path = 'merged_model-AWQ'\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 6, \"version\": \"GEMM\" }\n",
    "\n",
    "\n",
    "model.quantize( tokenizer, quant_config = quant_config, export_compatible=True )\n",
    "model.save_quantized( quant_path )\n",
    "tokenizer.save_pretrained( quant_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 32/32 [00:05<00:00,  6.30it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:00<00:00, 110.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "quant_path = 'merged_model-AWQ'\n",
    "\n",
    "awq_model = AutoAWQForCausalLM.from_quantized(quant_path, fuse_layers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706cfd5101354c7684efa565324db1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'MistralAWQForCausalLM' object has no attribute 'merge_and_unload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m base_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-v0.3\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16, device_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mawq_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_and_unload\u001b[49m(safe_merge\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, progressbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MistralAWQForCausalLM' object has no attribute 'merge_and_unload'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", torch_dtype=torch.bfloat16, device_map={\"\": \"cpu\"})\n",
    "\n",
    "\n",
    "model = awq_model.merge_and_unload(safe_merge=True, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "AWQ kernels could not be loaded. Please install them from https://github.com/casper-hansen/AutoAWQ_kernels",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mawq_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.15\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/awq/models/base.py:113\u001b[0m, in \u001b[0;36mBaseAWQForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A generate function that mimics the HF generate function.\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/generation/utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1907\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1908\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1909\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1911\u001b[0m     )\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1929\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m     )\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/generation/utils.py:2651\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2648\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2651\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2659\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1200\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1197\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1200\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1214\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/awq/modules/fused/model.py:127\u001b[0m, in \u001b[0;36mLlamaLikeModel.forward\u001b[0;34m(self, input_ids, attn_bias, attention_mask, is_causal, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    122\u001b[0m     h, mask \u001b[38;5;241m=\u001b[39m fused_utils\u001b[38;5;241m.\u001b[39mprepare_correct_devices(\n\u001b[1;32m    123\u001b[0m         layer,\n\u001b[1;32m    124\u001b[0m         h,\n\u001b[1;32m    125\u001b[0m         mask,\n\u001b[1;32m    126\u001b[0m     )\n\u001b[0;32m--> 127\u001b[0m     h, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(h)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    133\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mh,\n\u001b[1;32m    134\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    135\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39m(),\n\u001b[1;32m    136\u001b[0m     attentions\u001b[38;5;241m=\u001b[39m(),\n\u001b[1;32m    137\u001b[0m )\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/awq/modules/fused/block.py:122\u001b[0m, in \u001b[0;36mLlamaLikeBlock.forward\u001b[0;34m(self, hidden_states, past_key_value, attn_bias, attention_mask, is_causal)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    116\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m ):\n\u001b[0;32m--> 122\u001b[0m     norm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     attn_output, _, past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    124\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mnorm_out,\n\u001b[1;32m    125\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    126\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    129\u001b[0m     h \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(attn_output\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m+\u001b[39m attn_output\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/awq/modules/fused/norm.py:19\u001b[0m, in \u001b[0;36mFasterTransformerRMSNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m AWQ_INSTALLED, (\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAWQ kernels could not be loaded. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install them from https://github.com/casper-hansen/AutoAWQ_kernels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty_like(x)\n\u001b[1;32m     25\u001b[0m     awq_ext\u001b[38;5;241m.\u001b[39mlayernorm_forward_cuda(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n",
      "\u001b[0;31mAssertionError\u001b[0m: AWQ kernels could not be loaded. Please install them from https://github.com/casper-hansen/AutoAWQ_kernels"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = [{\"page_content\": \"\\n\\u00a7 4 Sprachkenntnisse\\n(1) 1Neben den allgemeinen Zugangsvoraussetzungen (\\u00a7 59 LHG) sind f\\u00fcr die in \\u00a7 1 Abs. 1\\nS. 1 genannten Studieng\\u00e4nge deutsche Sprach kenntnisse nachzuweisen. 2Diese k\\u00f6nnen\\ndurch eine deutsche Hochschulzugangsberechtigung (u. a. erfolgreich abgeschlossenes\\ngrundst\\u00e4ndiges Hochschulstudium) nachgewiesen werden. 3Ferner kann der\\nSprachnachweis durch die Vorlage eines der folgenden Dokumente erbracht werden:\\n1. Feststellungspr\\u00fcfung f\\u00fcr ein Bachelorstudium durch Vorlage der Zugangsberechtigung\\ndes Studienkollegs an der Hochschule Konstanz,\\n2. Test Deutsch als Fremdsprache (TestDaF), sofern im Durchschnitt mindestens die\\nStufe TDN 4 erreicht wurde,   Seite 5 von 43 3. Deutsche Sprachpr\\u00fcfung f\\u00fcr den Hochschulzugang (DSH), sofern die DSH mit\\nmindestens der Stufe DSH -2 abgeschlossen wurde,\\n4. \\u201eTelc Deutsch C1 Hochschule\\u201c\\noder eine \\u00e4quivalente Sprachpr\\u00fcfung gem\\u00e4\\u00df der Rahmenordnung \\u00fcber Deutsche\\nSprachpr\\u00fcfungen f\\u00fcr das Studium an deutschen Hochschulen (RO -DT). 4Auf den Nachweis\\neiner deutschen Sprachpr\\u00fcfung kann bei Bewerber innen und Bewerbern im besonders\\nbegr\\u00fcndeten Einzelfall verzichtet werden, insbesondere wenn sie die deutsche\\nStaatsangeh\\u00f6rigkeit besitzen.\\n(2) 1Sprachnachweise f\\u00fcr den gew\\u00e4hl ten Studiengang, die durch die Bewerberin oder den\\nBewerber bis zum Bewerbungsschluss nicht vorgelegt werden k\\u00f6nnen, k\\u00f6nnen bis zum\\nVorlesungsbeginn des Semesters gem\\u00e4\\u00df Terminplan der Hochschule Konstanz, f\\u00fcr das  der\\nAntrag auf Zulassung gestellt wurde, nachgereicht werden. 2Die Zulassung erfolgt in diesem\\nFall gem \\u00e4\\u00df \\u00a7 6  Abs. 5  unter Vorbehalt .\\n(3) 1F\\u00fcr Zeitstudierende gelten die Regelungen in \\u00a7 10 Zulassungs - und\\nImmatrikulationsordnung (ZIO) der Hochschule Konstanz.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"\\n\\u00a7 21b Mechatronik (MME) Berufsbegleitendes Studium\\n(1) Studiengangspezifische Zugangsvoraussetzungen gem\\u00e4\\u00df \\u00a7 5  Abs. 1\\nZugangsvoraussetzungen f\\u00fcr den Masterstudiengang Mechatronik sind:\\n1. Ein mit der Note 2,9 oder besser abgeschlossenes grundst\\u00e4ndiges Hochschulstudium\\ngem\\u00e4\\u00df \\u00a7 5 Abs. 1 Nr. 1 in einem Studiengang der Fachrichtungen Systemtechnik,\\nMaschinenbau, Elektrotechnik, Fahrzeugtechnik, Mechatronik, Feinwerktechnik oder einer\\nverwandten Fachrichtung.\\n2. Englischkenntnisse, \\u00e4quivalent z u Niveau- Stufe B1 des Europ\\u00e4ischen Referenzrahmens\\nf\\u00fcr das Lernen und Lehren von Fremdsprachen. Als \\u00e4quivalent zu einem Zertifikat \\u00fcber die\\nNiveau -Stufe B1 gelten insbesondere folgende Nachweise:\\nI. das Schulabschlusszeugnis, aus dem der Besuch des Englischunterrichts bis zum\\nErreichen des mittleren Bildungsabschlusses (10. Klasse) bzw. bis zum Erreichen\\nder Fachhochschulreife hervorgeht oder\\nII. ein Notenspiegel, aus dem die bestandene Pr\\u00fcfungsleistung \\u00fcber eine\\nLehrveranstaltung im Rahmen des grundst\\u00e4ndigen Studiums hervorgeht, die die\\nenglische Sprache zum Inhalt hatte oder\\nIII. eine Bescheinigung \\u00fcber den mindestens sechsmonatigen Aufenthalt an einer Schule, Hochschule oder anderen Bildungsinstitution mit Englisch als\\nUnterrichtssprache oder\\nIV. eine Bescheinigung \\u00fcber den Aufenthalt im englischsprachigen Ausland, der einen Zeitraum von mindestens sechs Monaten bzw. einem Studiensemester umfasst.\\nDie Vorlage anderer geeigneter Nachweise ist m\\u00f6glich.\\n(2) Auswahlkriterien nach \\u00a7 9 Abs. 2\\n1. Ergebnis eines Auswahlgespr\\u00e4chs\\nNicht zutreffend.\\n2. Leistungen, die mit der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Studiums nach Abs. 1\\ni. V. m. \\u00a7 5 Abs. 1 Nr. 1 nachgewiesen sind\\nDie Durchschnittsnote der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums nach\\nAbs. 1 bildet die Teilnote 1 als Basis zur Bestimmung der Auswahlnote.  Abweichend von Satz\\n1 bildet in den F\\u00e4llen des \\u00a7 3 Abs. 2 Nr. 1 Satz 2 die Durchschnittsnote nach \\u00a7 3 Abs. 2 Nr. 1 Satz 3 die Teilnote 1. Bei ausl\\u00e4ndischen Bildungsnachweisen ist die Durchschnittsnote nach\\ndeutsc her Deutung als Teilnote 1 zu ber\\u00fccksichtigen.\\nZus\\u00e4tzlich werden die Einzelnoten folgender F\\u00e4cher der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums, die \\u00fcber die Eignung f\\u00fcr den gew\\u00e4hlten Studiengang\\nbesonderen Aufschluss geben, f\\u00fcr die Auswahl herangezogen:\\n- Technische Mechanik (Dynamik),\\n- Elektrotechnik,\\n- Messtechnik,\\n- Regelungstechnik,\\n- Elektrische Antriebe.\\nDabei wird eine Note zwischen 1,0 und 1,7 in einem der o. g. F\\u00e4cher jeweils mit dem Wert 0,1\\nbewertet. Die kumulierte Gesamtzahl bildet die Teil note 2.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"\\n\\u00a7 21a Mechatronik (MME) Vollzeitstudium\\n(1) Studiengangspezifische Zugangsvoraussetzungen gem\\u00e4\\u00df \\u00a7 5  Abs. 1\\nZugangsvoraussetzungen f\\u00fcr den Masterstudiengang Mechatronik sind:\\n1. Ein mit der Note 2,9 oder besser abgeschlossenes grundst\\u00e4ndiges Hochschulstudium\\ngem\\u00e4\\u00df \\u00a7 5 Abs. 1 Nr. 1 in einem Studiengang der Fachrichtungen Maschinenbau,\\nElektrotechnik, Fahrzeugtechnik, Mechatronik, Feinwerktechnik oder einer verwandten\\nFachrichtung.\\n2. Englischkenntnisse, \\u00e4quivalent zu Niveau- Stufe B1 des Europ\\u00e4ischen Referenzrahmens\\nf\\u00fcr das Lernen und Lehren von Fremdsprachen. Als \\u00e4quivalent zu einem Zertifikat \\u00fcber die\\nNiveau -Stufe B1 gelten insbesondere folgende Nachweise:\\nI. das Schulabschlusszeugnis, aus dem der Besuch des Englischunterrichts bis zum\\nErreichen des mittleren Bildungsabschlusses (10. Klass e) bzw. bis zum Erreichen\\nder Fachhochschulreife hervorgeht oder\\nII. ein Notenspiegel, aus dem die bestandene Pr\\u00fcfungsleistung \\u00fcber eine\\nLehrveranstaltung im Rahmen des grundst\\u00e4ndigen Studiums hervorgeht, die die\\nenglische Sprache zum Inhalt hatte oder\\nIII. eine Bescheinigung \\u00fcber den mindestens sechsmonatigen Aufenthalt an einer Schule, Hochschule oder anderen Bildungsinstitution mit Englisch als\\nUnterrichtssprache oder\\nIV. eine Bescheinigung \\u00fcber den Aufenthalt im englischsprachigen Ausland, der einen Zeitraum von mindestens sechs Monaten bzw. einem Studiensemester umfasst.\\nDie Vorlage anderer geeigneter Nachweise ist m\\u00f6glich.\\n(2) Auswahlkriterien nach \\u00a7 9 Abs. 2\\n1. Ergebnis eines Auswahlgespr\\u00e4chs\\nNicht zutreffend.\\n2. Leistungen, die mit der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Studiums nach Abs. 1\\ni. V. m. \\u00a7 5 Abs. 1 Nr. 1 nachgewiesen sind\\nDie Durchschnittsnote der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums nach\\nAbs. 1 bildet die Teilnote 1 als Basis zur Bestimmung der Auswahlnote. Abweichend von Satz\\n1 bildet in den F\\u00e4llen des \\u00a7 3 Abs. 2 Nr. 1 Satz 2 die Durchschnittsnote nach \\u00a7 3 Abs. 2 Nr. 1\\nSatz 3 die Teilnote 1. Bei ausl\\u00e4ndischen Bildungsnachweisen ist die Durchschnittsnote nach\\ndeutscher Deutung als Teilnote 1 zu ber\\u00fccksichtigen.\\nZus\\u00e4tzlich werden die Einzelnoten folgender F\\u00e4cher der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums, die \\u00fcber die Eignung f\\u00fcr den gew\\u00e4hlten Studiengang\\nbesonderen Aufschluss geben, f\\u00fcr die Auswahl herangezogen:\\n- Technische Mechanik (Dynamik),\\n- Elektrotechnik,\\n- Messt echnik,\\n- Regelungstechnik,\\n- Elektrische Antriebe.\\nDabei wird eine Note zwischen 1,0 und 1,7 in einem der o. g. F\\u00e4cher jeweils mit dem Wert 0,1\\nbewertet. Die kumulierte Gesamtzahl bildet die Teilnote 2.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"\\n\\u00a7 9 Zugangs - und Auswahlkriterien in den Masterstudieng\\u00e4ngen\\n(1)  1Im Besonderen Teil (\\u00a7\\u00a7 12 -26) dieser Satzung k\\u00f6nnen ein oder mehrere der in Absatz 2\\ngenannten Auswahlkriterien als weitere Zugangskriterien festgelegt werden. 2N\\u00e4heres\\nregelt der B esondere Teil f\\u00fcr den jeweiligen Studiengang (\\u00a7\\u00a7 12 -26).\\n(2)  1F\\u00fcr die Bildung der Ranglisten f\\u00fcr das erste Fachsemester in den Masterstudieng\\u00e4ngen\\nwird, neben dem Ergebnis des fachlich einschl\\u00e4gigen Hochschulabschlusses oder des\\ngleichwertigen Abschlusses,  mindestens eines der folgenden  Auswahlkriterien\\nber\\u00fccksichtigt:\\n1. Leistungen, die in dem Studium erbracht wurden, das Voraussetzung f\\u00fcr den Zugang\\nzu dem Masterstudiengang ist ,   Seite 8 von 43 2. Englischkenntnisse , n\\u00e4heres regelt der Besondere Teil f\\u00fcr den jeweiligen Studiengang\\n(\\u00a7\\u00a7 12 -26),\\n3. Berufst\\u00e4tigkeit und Qualifikationen:\\na) Art einer abgeschlossenen Berufsausbildung oder einer Berufst\\u00e4tigkeit in einem\\nanerkannten Ausbildungsberuf  oder eine andere einschl\\u00e4gige Berufst\\u00e4tigkeit , die \\u00fcber\\ndie fachspezifische Eignung Auskunft gibt, jeweils  einzeln und in Kombination, und\\nb) Qualifikation en, die \\u00fcber die fachspezifische Leistung Auskunft geben, jeweils einzeln\\noder in Kombination,\\n4. das Ergebnis eines fachspezifischen Studieneignungstests ,\\n5. das Ergebnis des Auswahlgespr\\u00e4chs/anderen m\\u00fcndlichen Verfahrens  gem\\u00e4\\u00df \\u00a7 9a ,\\n6. ein Motivationsschreiben,\\n7. eine schriftliche Abhandlung (Essay).\\n2N\\u00e4heres sowie die Gewichtung regelt der B esondere Teil f\\u00fcr den jeweiligen Studiengang (\\u00a7\\u00a7\\n12-26).\\n(2) 1Die Auswahl f\\u00fcr h\\u00f6here Fachsemester erfolgt gem\\u00e4\\u00df \\u00a7 7 HZG i. V. m. \\u00a7 32 HZVO.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}]\n",
    "context = [entry['page_content'] for entry in data]\n",
    "eval_prompt = create_eval_prompt(\"\"\"Welche Dokumente kÃ¶nnen als Nachweis fÃ¼r deutsche Sprachkenntnisse akzeptiert werden?\"\"\", str(context))\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "awq_model.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        print(tokenizer.decode(awq_model.generate(**model_input, max_new_tokens=500, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(query: str, context: str):\n",
    "    system_prompt = \"You are a smart helpful assistant for the HTWG Konstanz. Answer the following question based only on the provided context. It is mandatory to answer in GERMAN:\\n\\n\"\n",
    "    return f\"[INST]{system_prompt}Context: {context}\\n\\nQuestion: {query}[/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Cache only has 0 layers, attempted to access layer with index 0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.15\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/peft/peft_model.py:1638\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1637\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1638\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1640\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/generation/utils.py:1990\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1982\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1983\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1984\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1985\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1986\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1987\u001b[0m     )\n\u001b[1;32m   1989\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1990\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1991\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1999\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2002\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2004\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2005\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2006\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m     )\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/generation/utils.py:2933\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2930\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2932\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2933\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2936\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/unsloth/models/mistral.py:211\u001b[0m, in \u001b[0;36mMistralForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    220\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    221\u001b[0m         causal_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    230\u001b[0m     )\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/unsloth/models/llama.py:795\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    793\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtorch_dtype)\n\u001b[1;32m    794\u001b[0m bsz, q_len, hd \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 795\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bsz \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    797\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[1;32m    798\u001b[0m         attention_mask,\n\u001b[1;32m    799\u001b[0m         (bsz, q_len),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m         sliding_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_window\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    803\u001b[0m     )\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/cache_utils.py:314\u001b[0m, in \u001b[0;36mDynamicCache.__getitem__\u001b[0;34m(self, layer_idx)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache[layer_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx])\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCache only has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, attempted to access layer with index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Cache only has 0 layers, attempted to access layer with index 0'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = [{\"page_content\": \"\\n\\u00a7 4 Sprachkenntnisse\\n(1) 1Neben den allgemeinen Zugangsvoraussetzungen (\\u00a7 59 LHG) sind f\\u00fcr die in \\u00a7 1 Abs. 1\\nS. 1 genannten Studieng\\u00e4nge deutsche Sprach kenntnisse nachzuweisen. 2Diese k\\u00f6nnen\\ndurch eine deutsche Hochschulzugangsberechtigung (u. a. erfolgreich abgeschlossenes\\ngrundst\\u00e4ndiges Hochschulstudium) nachgewiesen werden. 3Ferner kann der\\nSprachnachweis durch die Vorlage eines der folgenden Dokumente erbracht werden:\\n1. Feststellungspr\\u00fcfung f\\u00fcr ein Bachelorstudium durch Vorlage der Zugangsberechtigung\\ndes Studienkollegs an der Hochschule Konstanz,\\n2. Test Deutsch als Fremdsprache (TestDaF), sofern im Durchschnitt mindestens die\\nStufe TDN 4 erreicht wurde,   Seite 5 von 43 3. Deutsche Sprachpr\\u00fcfung f\\u00fcr den Hochschulzugang (DSH), sofern die DSH mit\\nmindestens der Stufe DSH -2 abgeschlossen wurde,\\n4. \\u201eTelc Deutsch C1 Hochschule\\u201c\\noder eine \\u00e4quivalente Sprachpr\\u00fcfung gem\\u00e4\\u00df der Rahmenordnung \\u00fcber Deutsche\\nSprachpr\\u00fcfungen f\\u00fcr das Studium an deutschen Hochschulen (RO -DT). 4Auf den Nachweis\\neiner deutschen Sprachpr\\u00fcfung kann bei Bewerber innen und Bewerbern im besonders\\nbegr\\u00fcndeten Einzelfall verzichtet werden, insbesondere wenn sie die deutsche\\nStaatsangeh\\u00f6rigkeit besitzen.\\n(2) 1Sprachnachweise f\\u00fcr den gew\\u00e4hl ten Studiengang, die durch die Bewerberin oder den\\nBewerber bis zum Bewerbungsschluss nicht vorgelegt werden k\\u00f6nnen, k\\u00f6nnen bis zum\\nVorlesungsbeginn des Semesters gem\\u00e4\\u00df Terminplan der Hochschule Konstanz, f\\u00fcr das  der\\nAntrag auf Zulassung gestellt wurde, nachgereicht werden. 2Die Zulassung erfolgt in diesem\\nFall gem \\u00e4\\u00df \\u00a7 6  Abs. 5  unter Vorbehalt .\\n(3) 1F\\u00fcr Zeitstudierende gelten die Regelungen in \\u00a7 10 Zulassungs - und\\nImmatrikulationsordnung (ZIO) der Hochschule Konstanz.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"\\n\\u00a7 21b Mechatronik (MME) Berufsbegleitendes Studium\\n(1) Studiengangspezifische Zugangsvoraussetzungen gem\\u00e4\\u00df \\u00a7 5  Abs. 1\\nZugangsvoraussetzungen f\\u00fcr den Masterstudiengang Mechatronik sind:\\n1. Ein mit der Note 2,9 oder besser abgeschlossenes grundst\\u00e4ndiges Hochschulstudium\\ngem\\u00e4\\u00df \\u00a7 5 Abs. 1 Nr. 1 in einem Studiengang der Fachrichtungen Systemtechnik,\\nMaschinenbau, Elektrotechnik, Fahrzeugtechnik, Mechatronik, Feinwerktechnik oder einer\\nverwandten Fachrichtung.\\n2. Englischkenntnisse, \\u00e4quivalent z u Niveau- Stufe B1 des Europ\\u00e4ischen Referenzrahmens\\nf\\u00fcr das Lernen und Lehren von Fremdsprachen. Als \\u00e4quivalent zu einem Zertifikat \\u00fcber die\\nNiveau -Stufe B1 gelten insbesondere folgende Nachweise:\\nI. das Schulabschlusszeugnis, aus dem der Besuch des Englischunterrichts bis zum\\nErreichen des mittleren Bildungsabschlusses (10. Klasse) bzw. bis zum Erreichen\\nder Fachhochschulreife hervorgeht oder\\nII. ein Notenspiegel, aus dem die bestandene Pr\\u00fcfungsleistung \\u00fcber eine\\nLehrveranstaltung im Rahmen des grundst\\u00e4ndigen Studiums hervorgeht, die die\\nenglische Sprache zum Inhalt hatte oder\\nIII. eine Bescheinigung \\u00fcber den mindestens sechsmonatigen Aufenthalt an einer Schule, Hochschule oder anderen Bildungsinstitution mit Englisch als\\nUnterrichtssprache oder\\nIV. eine Bescheinigung \\u00fcber den Aufenthalt im englischsprachigen Ausland, der einen Zeitraum von mindestens sechs Monaten bzw. einem Studiensemester umfasst.\\nDie Vorlage anderer geeigneter Nachweise ist m\\u00f6glich.\\n(2) Auswahlkriterien nach \\u00a7 9 Abs. 2\\n1. Ergebnis eines Auswahlgespr\\u00e4chs\\nNicht zutreffend.\\n2. Leistungen, die mit der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Studiums nach Abs. 1\\ni. V. m. \\u00a7 5 Abs. 1 Nr. 1 nachgewiesen sind\\nDie Durchschnittsnote der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums nach\\nAbs. 1 bildet die Teilnote 1 als Basis zur Bestimmung der Auswahlnote.  Abweichend von Satz\\n1 bildet in den F\\u00e4llen des \\u00a7 3 Abs. 2 Nr. 1 Satz 2 die Durchschnittsnote nach \\u00a7 3 Abs. 2 Nr. 1 Satz 3 die Teilnote 1. Bei ausl\\u00e4ndischen Bildungsnachweisen ist die Durchschnittsnote nach\\ndeutsc her Deutung als Teilnote 1 zu ber\\u00fccksichtigen.\\nZus\\u00e4tzlich werden die Einzelnoten folgender F\\u00e4cher der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums, die \\u00fcber die Eignung f\\u00fcr den gew\\u00e4hlten Studiengang\\nbesonderen Aufschluss geben, f\\u00fcr die Auswahl herangezogen:\\n- Technische Mechanik (Dynamik),\\n- Elektrotechnik,\\n- Messtechnik,\\n- Regelungstechnik,\\n- Elektrische Antriebe.\\nDabei wird eine Note zwischen 1,0 und 1,7 in einem der o. g. F\\u00e4cher jeweils mit dem Wert 0,1\\nbewertet. Die kumulierte Gesamtzahl bildet die Teil note 2.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"\\n\\u00a7 21a Mechatronik (MME) Vollzeitstudium\\n(1) Studiengangspezifische Zugangsvoraussetzungen gem\\u00e4\\u00df \\u00a7 5  Abs. 1\\nZugangsvoraussetzungen f\\u00fcr den Masterstudiengang Mechatronik sind:\\n1. Ein mit der Note 2,9 oder besser abgeschlossenes grundst\\u00e4ndiges Hochschulstudium\\ngem\\u00e4\\u00df \\u00a7 5 Abs. 1 Nr. 1 in einem Studiengang der Fachrichtungen Maschinenbau,\\nElektrotechnik, Fahrzeugtechnik, Mechatronik, Feinwerktechnik oder einer verwandten\\nFachrichtung.\\n2. Englischkenntnisse, \\u00e4quivalent zu Niveau- Stufe B1 des Europ\\u00e4ischen Referenzrahmens\\nf\\u00fcr das Lernen und Lehren von Fremdsprachen. Als \\u00e4quivalent zu einem Zertifikat \\u00fcber die\\nNiveau -Stufe B1 gelten insbesondere folgende Nachweise:\\nI. das Schulabschlusszeugnis, aus dem der Besuch des Englischunterrichts bis zum\\nErreichen des mittleren Bildungsabschlusses (10. Klass e) bzw. bis zum Erreichen\\nder Fachhochschulreife hervorgeht oder\\nII. ein Notenspiegel, aus dem die bestandene Pr\\u00fcfungsleistung \\u00fcber eine\\nLehrveranstaltung im Rahmen des grundst\\u00e4ndigen Studiums hervorgeht, die die\\nenglische Sprache zum Inhalt hatte oder\\nIII. eine Bescheinigung \\u00fcber den mindestens sechsmonatigen Aufenthalt an einer Schule, Hochschule oder anderen Bildungsinstitution mit Englisch als\\nUnterrichtssprache oder\\nIV. eine Bescheinigung \\u00fcber den Aufenthalt im englischsprachigen Ausland, der einen Zeitraum von mindestens sechs Monaten bzw. einem Studiensemester umfasst.\\nDie Vorlage anderer geeigneter Nachweise ist m\\u00f6glich.\\n(2) Auswahlkriterien nach \\u00a7 9 Abs. 2\\n1. Ergebnis eines Auswahlgespr\\u00e4chs\\nNicht zutreffend.\\n2. Leistungen, die mit der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Studiums nach Abs. 1\\ni. V. m. \\u00a7 5 Abs. 1 Nr. 1 nachgewiesen sind\\nDie Durchschnittsnote der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums nach\\nAbs. 1 bildet die Teilnote 1 als Basis zur Bestimmung der Auswahlnote. Abweichend von Satz\\n1 bildet in den F\\u00e4llen des \\u00a7 3 Abs. 2 Nr. 1 Satz 2 die Durchschnittsnote nach \\u00a7 3 Abs. 2 Nr. 1\\nSatz 3 die Teilnote 1. Bei ausl\\u00e4ndischen Bildungsnachweisen ist die Durchschnittsnote nach\\ndeutscher Deutung als Teilnote 1 zu ber\\u00fccksichtigen.\\nZus\\u00e4tzlich werden die Einzelnoten folgender F\\u00e4cher der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums, die \\u00fcber die Eignung f\\u00fcr den gew\\u00e4hlten Studiengang\\nbesonderen Aufschluss geben, f\\u00fcr die Auswahl herangezogen:\\n- Technische Mechanik (Dynamik),\\n- Elektrotechnik,\\n- Messt echnik,\\n- Regelungstechnik,\\n- Elektrische Antriebe.\\nDabei wird eine Note zwischen 1,0 und 1,7 in einem der o. g. F\\u00e4cher jeweils mit dem Wert 0,1\\nbewertet. Die kumulierte Gesamtzahl bildet die Teilnote 2.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"\\n\\u00a7 9 Zugangs - und Auswahlkriterien in den Masterstudieng\\u00e4ngen\\n(1)  1Im Besonderen Teil (\\u00a7\\u00a7 12 -26) dieser Satzung k\\u00f6nnen ein oder mehrere der in Absatz 2\\ngenannten Auswahlkriterien als weitere Zugangskriterien festgelegt werden. 2N\\u00e4heres\\nregelt der B esondere Teil f\\u00fcr den jeweiligen Studiengang (\\u00a7\\u00a7 12 -26).\\n(2)  1F\\u00fcr die Bildung der Ranglisten f\\u00fcr das erste Fachsemester in den Masterstudieng\\u00e4ngen\\nwird, neben dem Ergebnis des fachlich einschl\\u00e4gigen Hochschulabschlusses oder des\\ngleichwertigen Abschlusses,  mindestens eines der folgenden  Auswahlkriterien\\nber\\u00fccksichtigt:\\n1. Leistungen, die in dem Studium erbracht wurden, das Voraussetzung f\\u00fcr den Zugang\\nzu dem Masterstudiengang ist ,   Seite 8 von 43 2. Englischkenntnisse , n\\u00e4heres regelt der Besondere Teil f\\u00fcr den jeweiligen Studiengang\\n(\\u00a7\\u00a7 12 -26),\\n3. Berufst\\u00e4tigkeit und Qualifikationen:\\na) Art einer abgeschlossenen Berufsausbildung oder einer Berufst\\u00e4tigkeit in einem\\nanerkannten Ausbildungsberuf  oder eine andere einschl\\u00e4gige Berufst\\u00e4tigkeit , die \\u00fcber\\ndie fachspezifische Eignung Auskunft gibt, jeweils  einzeln und in Kombination, und\\nb) Qualifikation en, die \\u00fcber die fachspezifische Leistung Auskunft geben, jeweils einzeln\\noder in Kombination,\\n4. das Ergebnis eines fachspezifischen Studieneignungstests ,\\n5. das Ergebnis des Auswahlgespr\\u00e4chs/anderen m\\u00fcndlichen Verfahrens  gem\\u00e4\\u00df \\u00a7 9a ,\\n6. ein Motivationsschreiben,\\n7. eine schriftliche Abhandlung (Essay).\\n2N\\u00e4heres sowie die Gewichtung regelt der B esondere Teil f\\u00fcr den jeweiligen Studiengang (\\u00a7\\u00a7\\n12-26).\\n(2) 1Die Auswahl f\\u00fcr h\\u00f6here Fachsemester erfolgt gem\\u00e4\\u00df \\u00a7 7 HZG i. V. m. \\u00a7 32 HZVO.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}]\n",
    "context = [entry['page_content'] for entry in data]\n",
    "eval_prompt = create_eval_prompt(\"\"\"Welche Dokumente kÃ¶nnen als Nachweis fÃ¼r deutsche Sprachkenntnisse akzeptiert werden?\"\"\", str(context))\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        print(tokenizer.decode(model.generate(**model_input, max_new_tokens=500, repetition_penalty=1.15)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o convert.py https://github.com/ggerganov/llama.cpp/raw/master/convert.py\n",
    "!curl -L -o requirements.txt https://github.com/ggerganov/llama.cpp/raw/master/requirements.txt\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Convert the 7B model to ggml FP16 format\n",
    "!python3 convert.py model\n",
    "\n",
    "## Code below is optional - uncomment (remove leftmost '# ') to use\n",
    "\n",
    "# # [Optional] for models using BPE tokenizers\n",
    "# !python convert.py model --vocabtype bpe\n",
    "\n",
    "# # [Optional] quantize the model to 4-bits (using q4_0 method)\n",
    "# !./quantize ./model/ggml-model-f16.gguf ./model/ggml-model-q4_0.gguf q4_0\n",
    "\n",
    "# # [Optional] update the gguf filetype to current if older version is unsupported by another application\n",
    "# !./quantize ./model/ggml-model-q4_0.gguf ./model/ggml-model-q4_0-v2.gguf COPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIA UNSLOTH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unsloth provides less ressource intensive and faster finetuning + conversion\n",
    "- see https://github.com/unslothai/unsloth for installation instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_bf16_supported()\n",
    "torch.cuda.get_device_capability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Mistral patching. Transformers = 4.44.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA TITAN RTX. Max memory: 23.643 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device does not support bfloat16. Will change to float16.\n",
      "Unsloth: Will load checkpoints/raft-v2/checkpoint-250 as a legacy tokenizer.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "#\"checkpoints/raft-completion-only/checkpoint-350\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"checkpoints/raft-v2/checkpoint-250\", load_in_4bit = True, dtype=torch.bfloat16)\n",
    "#model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='checkpoints/raft-v2/checkpoint-250', vocab_size=32768, model_max_length=32768, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[/INST]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[TOOL_CALLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"[AVAILABLE_TOOLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"[/AVAILABLE_TOOLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"[TOOL_RESULTS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"[/TOOL_RESULTS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"[control_8]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t11: AddedToken(\"[control_9]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t12: AddedToken(\"[control_10]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t13: AddedToken(\"[control_11]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t14: AddedToken(\"[control_12]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t15: AddedToken(\"[control_13]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t16: AddedToken(\"[control_14]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t17: AddedToken(\"[control_15]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t18: AddedToken(\"[control_16]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t19: AddedToken(\"[control_17]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20: AddedToken(\"[control_18]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21: AddedToken(\"[control_19]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t22: AddedToken(\"[control_20]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t23: AddedToken(\"[control_21]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t24: AddedToken(\"[control_22]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t25: AddedToken(\"[control_23]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t26: AddedToken(\"[control_24]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t27: AddedToken(\"[control_25]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t28: AddedToken(\"[control_26]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t29: AddedToken(\"[control_27]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30: AddedToken(\"[control_28]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t31: AddedToken(\"[control_29]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32: AddedToken(\"[control_30]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t33: AddedToken(\"[control_31]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t34: AddedToken(\"[control_32]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t35: AddedToken(\"[control_33]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t36: AddedToken(\"[control_34]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t37: AddedToken(\"[control_35]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t38: AddedToken(\"[control_36]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t39: AddedToken(\"[control_37]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t40: AddedToken(\"[control_38]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t41: AddedToken(\"[control_39]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t42: AddedToken(\"[control_40]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t43: AddedToken(\"[control_41]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t44: AddedToken(\"[control_42]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t45: AddedToken(\"[control_43]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t46: AddedToken(\"[control_44]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t47: AddedToken(\"[control_45]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t48: AddedToken(\"[control_46]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t49: AddedToken(\"[control_47]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50: AddedToken(\"[control_48]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51: AddedToken(\"[control_49]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t52: AddedToken(\"[control_50]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t53: AddedToken(\"[control_51]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t54: AddedToken(\"[control_52]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t55: AddedToken(\"[control_53]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t56: AddedToken(\"[control_54]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57: AddedToken(\"[control_55]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t58: AddedToken(\"[control_56]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59: AddedToken(\"[control_57]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t60: AddedToken(\"[control_58]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t61: AddedToken(\"[control_59]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t62: AddedToken(\"[control_60]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t63: AddedToken(\"[control_61]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t64: AddedToken(\"[control_62]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t65: AddedToken(\"[control_63]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t66: AddedToken(\"[control_64]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t67: AddedToken(\"[control_65]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t68: AddedToken(\"[control_66]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t69: AddedToken(\"[control_67]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t70: AddedToken(\"[control_68]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t71: AddedToken(\"[control_69]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t72: AddedToken(\"[control_70]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t73: AddedToken(\"[control_71]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t74: AddedToken(\"[control_72]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t75: AddedToken(\"[control_73]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t76: AddedToken(\"[control_74]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t77: AddedToken(\"[control_75]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t78: AddedToken(\"[control_76]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t79: AddedToken(\"[control_77]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t80: AddedToken(\"[control_78]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t81: AddedToken(\"[control_79]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t82: AddedToken(\"[control_80]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t83: AddedToken(\"[control_81]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t84: AddedToken(\"[control_82]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t85: AddedToken(\"[control_83]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t86: AddedToken(\"[control_84]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t87: AddedToken(\"[control_85]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t88: AddedToken(\"[control_86]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t89: AddedToken(\"[control_87]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t90: AddedToken(\"[control_88]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t91: AddedToken(\"[control_89]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92: AddedToken(\"[control_90]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t93: AddedToken(\"[control_91]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t94: AddedToken(\"[control_92]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t95: AddedToken(\"[control_93]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t96: AddedToken(\"[control_94]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t97: AddedToken(\"[control_95]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t98: AddedToken(\"[control_96]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t99: AddedToken(\"[control_97]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[control_98]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[control_99]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[control_100]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[control_101]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t104: AddedToken(\"[control_102]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t105: AddedToken(\"[control_103]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t106: AddedToken(\"[control_104]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t107: AddedToken(\"[control_105]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t108: AddedToken(\"[control_106]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t109: AddedToken(\"[control_107]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t110: AddedToken(\"[control_108]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t111: AddedToken(\"[control_109]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t112: AddedToken(\"[control_110]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t113: AddedToken(\"[control_111]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t114: AddedToken(\"[control_112]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t115: AddedToken(\"[control_113]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t116: AddedToken(\"[control_114]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t117: AddedToken(\"[control_115]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t118: AddedToken(\"[control_116]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t119: AddedToken(\"[control_117]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t120: AddedToken(\"[control_118]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t121: AddedToken(\"[control_119]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t122: AddedToken(\"[control_120]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t123: AddedToken(\"[control_121]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t124: AddedToken(\"[control_122]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t125: AddedToken(\"[control_123]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t126: AddedToken(\"[control_124]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t127: AddedToken(\"[control_125]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128: AddedToken(\"[control_126]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t129: AddedToken(\"[control_127]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t130: AddedToken(\"[control_128]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t131: AddedToken(\"[control_129]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t132: AddedToken(\"[control_130]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t133: AddedToken(\"[control_131]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t134: AddedToken(\"[control_132]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t135: AddedToken(\"[control_133]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t136: AddedToken(\"[control_134]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t137: AddedToken(\"[control_135]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t138: AddedToken(\"[control_136]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t139: AddedToken(\"[control_137]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t140: AddedToken(\"[control_138]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t141: AddedToken(\"[control_139]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t142: AddedToken(\"[control_140]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t143: AddedToken(\"[control_141]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t144: AddedToken(\"[control_142]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t145: AddedToken(\"[control_143]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t146: AddedToken(\"[control_144]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t147: AddedToken(\"[control_145]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t148: AddedToken(\"[control_146]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t149: AddedToken(\"[control_147]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t150: AddedToken(\"[control_148]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151: AddedToken(\"[control_149]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t152: AddedToken(\"[control_150]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t153: AddedToken(\"[control_151]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t154: AddedToken(\"[control_152]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t155: AddedToken(\"[control_153]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t156: AddedToken(\"[control_154]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t157: AddedToken(\"[control_155]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t158: AddedToken(\"[control_156]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t159: AddedToken(\"[control_157]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t160: AddedToken(\"[control_158]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t161: AddedToken(\"[control_159]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t162: AddedToken(\"[control_160]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t163: AddedToken(\"[control_161]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t164: AddedToken(\"[control_162]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t165: AddedToken(\"[control_163]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t166: AddedToken(\"[control_164]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t167: AddedToken(\"[control_165]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t168: AddedToken(\"[control_166]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t169: AddedToken(\"[control_167]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t170: AddedToken(\"[control_168]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t171: AddedToken(\"[control_169]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t172: AddedToken(\"[control_170]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t173: AddedToken(\"[control_171]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t174: AddedToken(\"[control_172]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t175: AddedToken(\"[control_173]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t176: AddedToken(\"[control_174]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t177: AddedToken(\"[control_175]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t178: AddedToken(\"[control_176]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t179: AddedToken(\"[control_177]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t180: AddedToken(\"[control_178]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t181: AddedToken(\"[control_179]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t182: AddedToken(\"[control_180]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t183: AddedToken(\"[control_181]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t184: AddedToken(\"[control_182]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t185: AddedToken(\"[control_183]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t186: AddedToken(\"[control_184]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t187: AddedToken(\"[control_185]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t188: AddedToken(\"[control_186]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t189: AddedToken(\"[control_187]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t190: AddedToken(\"[control_188]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t191: AddedToken(\"[control_189]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t192: AddedToken(\"[control_190]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t193: AddedToken(\"[control_191]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t194: AddedToken(\"[control_192]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t195: AddedToken(\"[control_193]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t196: AddedToken(\"[control_194]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t197: AddedToken(\"[control_195]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t198: AddedToken(\"[control_196]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t199: AddedToken(\"[control_197]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200: AddedToken(\"[control_198]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t201: AddedToken(\"[control_199]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t202: AddedToken(\"[control_200]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t203: AddedToken(\"[control_201]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t204: AddedToken(\"[control_202]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t205: AddedToken(\"[control_203]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t206: AddedToken(\"[control_204]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t207: AddedToken(\"[control_205]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t208: AddedToken(\"[control_206]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t209: AddedToken(\"[control_207]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t210: AddedToken(\"[control_208]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t211: AddedToken(\"[control_209]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t212: AddedToken(\"[control_210]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t213: AddedToken(\"[control_211]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t214: AddedToken(\"[control_212]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t215: AddedToken(\"[control_213]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t216: AddedToken(\"[control_214]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t217: AddedToken(\"[control_215]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t218: AddedToken(\"[control_216]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t219: AddedToken(\"[control_217]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t220: AddedToken(\"[control_218]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t221: AddedToken(\"[control_219]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t222: AddedToken(\"[control_220]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t223: AddedToken(\"[control_221]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t224: AddedToken(\"[control_222]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t225: AddedToken(\"[control_223]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t226: AddedToken(\"[control_224]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t227: AddedToken(\"[control_225]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t228: AddedToken(\"[control_226]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t229: AddedToken(\"[control_227]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t230: AddedToken(\"[control_228]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t231: AddedToken(\"[control_229]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t232: AddedToken(\"[control_230]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t233: AddedToken(\"[control_231]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t234: AddedToken(\"[control_232]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t235: AddedToken(\"[control_233]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t236: AddedToken(\"[control_234]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t237: AddedToken(\"[control_235]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t238: AddedToken(\"[control_236]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t239: AddedToken(\"[control_237]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t240: AddedToken(\"[control_238]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t241: AddedToken(\"[control_239]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t242: AddedToken(\"[control_240]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t243: AddedToken(\"[control_241]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t244: AddedToken(\"[control_242]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t245: AddedToken(\"[control_243]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t246: AddedToken(\"[control_244]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t247: AddedToken(\"[control_245]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t248: AddedToken(\"[control_246]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t249: AddedToken(\"[control_247]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t250: AddedToken(\"[control_248]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t251: AddedToken(\"[control_249]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t252: AddedToken(\"[control_250]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t253: AddedToken(\"[control_251]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t254: AddedToken(\"[control_252]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t255: AddedToken(\"[control_253]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256: AddedToken(\"[control_254]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t257: AddedToken(\"[control_255]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t258: AddedToken(\"[control_256]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t259: AddedToken(\"[control_257]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t260: AddedToken(\"[control_258]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t261: AddedToken(\"[control_259]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t262: AddedToken(\"[control_260]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t263: AddedToken(\"[control_261]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t264: AddedToken(\"[control_262]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t265: AddedToken(\"[control_263]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t266: AddedToken(\"[control_264]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t267: AddedToken(\"[control_265]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t268: AddedToken(\"[control_266]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t269: AddedToken(\"[control_267]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t270: AddedToken(\"[control_268]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t271: AddedToken(\"[control_269]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t272: AddedToken(\"[control_270]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t273: AddedToken(\"[control_271]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t274: AddedToken(\"[control_272]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t275: AddedToken(\"[control_273]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t276: AddedToken(\"[control_274]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t277: AddedToken(\"[control_275]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t278: AddedToken(\"[control_276]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t279: AddedToken(\"[control_277]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t280: AddedToken(\"[control_278]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t281: AddedToken(\"[control_279]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t282: AddedToken(\"[control_280]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t283: AddedToken(\"[control_281]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t284: AddedToken(\"[control_282]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t285: AddedToken(\"[control_283]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t286: AddedToken(\"[control_284]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t287: AddedToken(\"[control_285]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t288: AddedToken(\"[control_286]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t289: AddedToken(\"[control_287]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t290: AddedToken(\"[control_288]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t291: AddedToken(\"[control_289]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t292: AddedToken(\"[control_290]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t293: AddedToken(\"[control_291]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t294: AddedToken(\"[control_292]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t295: AddedToken(\"[control_293]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t296: AddedToken(\"[control_294]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t297: AddedToken(\"[control_295]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t298: AddedToken(\"[control_296]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t299: AddedToken(\"[control_297]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t300: AddedToken(\"[control_298]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t301: AddedToken(\"[control_299]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t302: AddedToken(\"[control_300]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t303: AddedToken(\"[control_301]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t304: AddedToken(\"[control_302]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t305: AddedToken(\"[control_303]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t306: AddedToken(\"[control_304]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t307: AddedToken(\"[control_305]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t308: AddedToken(\"[control_306]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t309: AddedToken(\"[control_307]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t310: AddedToken(\"[control_308]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t311: AddedToken(\"[control_309]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t312: AddedToken(\"[control_310]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t313: AddedToken(\"[control_311]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t314: AddedToken(\"[control_312]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t315: AddedToken(\"[control_313]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t316: AddedToken(\"[control_314]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t317: AddedToken(\"[control_315]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t318: AddedToken(\"[control_316]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t319: AddedToken(\"[control_317]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t320: AddedToken(\"[control_318]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t321: AddedToken(\"[control_319]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t322: AddedToken(\"[control_320]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t323: AddedToken(\"[control_321]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t324: AddedToken(\"[control_322]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t325: AddedToken(\"[control_323]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t326: AddedToken(\"[control_324]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t327: AddedToken(\"[control_325]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t328: AddedToken(\"[control_326]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t329: AddedToken(\"[control_327]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t330: AddedToken(\"[control_328]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t331: AddedToken(\"[control_329]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t332: AddedToken(\"[control_330]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t333: AddedToken(\"[control_331]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t334: AddedToken(\"[control_332]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t335: AddedToken(\"[control_333]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t336: AddedToken(\"[control_334]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t337: AddedToken(\"[control_335]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t338: AddedToken(\"[control_336]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t339: AddedToken(\"[control_337]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t340: AddedToken(\"[control_338]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t341: AddedToken(\"[control_339]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t342: AddedToken(\"[control_340]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t343: AddedToken(\"[control_341]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t344: AddedToken(\"[control_342]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t345: AddedToken(\"[control_343]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t346: AddedToken(\"[control_344]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t347: AddedToken(\"[control_345]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t348: AddedToken(\"[control_346]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t349: AddedToken(\"[control_347]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t350: AddedToken(\"[control_348]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t351: AddedToken(\"[control_349]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t352: AddedToken(\"[control_350]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t353: AddedToken(\"[control_351]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t354: AddedToken(\"[control_352]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t355: AddedToken(\"[control_353]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t356: AddedToken(\"[control_354]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t357: AddedToken(\"[control_355]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t358: AddedToken(\"[control_356]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t359: AddedToken(\"[control_357]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t360: AddedToken(\"[control_358]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t361: AddedToken(\"[control_359]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t362: AddedToken(\"[control_360]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t363: AddedToken(\"[control_361]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t364: AddedToken(\"[control_362]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t365: AddedToken(\"[control_363]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t366: AddedToken(\"[control_364]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t367: AddedToken(\"[control_365]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t368: AddedToken(\"[control_366]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t369: AddedToken(\"[control_367]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t370: AddedToken(\"[control_368]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t371: AddedToken(\"[control_369]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t372: AddedToken(\"[control_370]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t373: AddedToken(\"[control_371]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t374: AddedToken(\"[control_372]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t375: AddedToken(\"[control_373]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t376: AddedToken(\"[control_374]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t377: AddedToken(\"[control_375]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t378: AddedToken(\"[control_376]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t379: AddedToken(\"[control_377]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t380: AddedToken(\"[control_378]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t381: AddedToken(\"[control_379]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t382: AddedToken(\"[control_380]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t383: AddedToken(\"[control_381]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t384: AddedToken(\"[control_382]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t385: AddedToken(\"[control_383]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t386: AddedToken(\"[control_384]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t387: AddedToken(\"[control_385]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t388: AddedToken(\"[control_386]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t389: AddedToken(\"[control_387]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t390: AddedToken(\"[control_388]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t391: AddedToken(\"[control_389]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t392: AddedToken(\"[control_390]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t393: AddedToken(\"[control_391]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t394: AddedToken(\"[control_392]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t395: AddedToken(\"[control_393]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t396: AddedToken(\"[control_394]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t397: AddedToken(\"[control_395]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t398: AddedToken(\"[control_396]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t399: AddedToken(\"[control_397]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t400: AddedToken(\"[control_398]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t401: AddedToken(\"[control_399]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t402: AddedToken(\"[control_400]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t403: AddedToken(\"[control_401]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t404: AddedToken(\"[control_402]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t405: AddedToken(\"[control_403]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t406: AddedToken(\"[control_404]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t407: AddedToken(\"[control_405]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t408: AddedToken(\"[control_406]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t409: AddedToken(\"[control_407]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t410: AddedToken(\"[control_408]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t411: AddedToken(\"[control_409]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t412: AddedToken(\"[control_410]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t413: AddedToken(\"[control_411]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t414: AddedToken(\"[control_412]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t415: AddedToken(\"[control_413]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t416: AddedToken(\"[control_414]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t417: AddedToken(\"[control_415]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t418: AddedToken(\"[control_416]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t419: AddedToken(\"[control_417]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t420: AddedToken(\"[control_418]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t421: AddedToken(\"[control_419]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t422: AddedToken(\"[control_420]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t423: AddedToken(\"[control_421]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t424: AddedToken(\"[control_422]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t425: AddedToken(\"[control_423]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t426: AddedToken(\"[control_424]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t427: AddedToken(\"[control_425]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t428: AddedToken(\"[control_426]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t429: AddedToken(\"[control_427]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t430: AddedToken(\"[control_428]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t431: AddedToken(\"[control_429]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t432: AddedToken(\"[control_430]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t433: AddedToken(\"[control_431]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t434: AddedToken(\"[control_432]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t435: AddedToken(\"[control_433]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t436: AddedToken(\"[control_434]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t437: AddedToken(\"[control_435]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t438: AddedToken(\"[control_436]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t439: AddedToken(\"[control_437]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t440: AddedToken(\"[control_438]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t441: AddedToken(\"[control_439]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t442: AddedToken(\"[control_440]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t443: AddedToken(\"[control_441]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t444: AddedToken(\"[control_442]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t445: AddedToken(\"[control_443]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t446: AddedToken(\"[control_444]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t447: AddedToken(\"[control_445]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t448: AddedToken(\"[control_446]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t449: AddedToken(\"[control_447]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t450: AddedToken(\"[control_448]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t451: AddedToken(\"[control_449]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t452: AddedToken(\"[control_450]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t453: AddedToken(\"[control_451]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t454: AddedToken(\"[control_452]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t455: AddedToken(\"[control_453]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t456: AddedToken(\"[control_454]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t457: AddedToken(\"[control_455]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t458: AddedToken(\"[control_456]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t459: AddedToken(\"[control_457]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t460: AddedToken(\"[control_458]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t461: AddedToken(\"[control_459]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t462: AddedToken(\"[control_460]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t463: AddedToken(\"[control_461]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t464: AddedToken(\"[control_462]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t465: AddedToken(\"[control_463]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t466: AddedToken(\"[control_464]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t467: AddedToken(\"[control_465]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t468: AddedToken(\"[control_466]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t469: AddedToken(\"[control_467]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t470: AddedToken(\"[control_468]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t471: AddedToken(\"[control_469]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t472: AddedToken(\"[control_470]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t473: AddedToken(\"[control_471]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t474: AddedToken(\"[control_472]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t475: AddedToken(\"[control_473]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t476: AddedToken(\"[control_474]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t477: AddedToken(\"[control_475]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t478: AddedToken(\"[control_476]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t479: AddedToken(\"[control_477]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t480: AddedToken(\"[control_478]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t481: AddedToken(\"[control_479]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t482: AddedToken(\"[control_480]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t483: AddedToken(\"[control_481]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t484: AddedToken(\"[control_482]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t485: AddedToken(\"[control_483]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t486: AddedToken(\"[control_484]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t487: AddedToken(\"[control_485]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t488: AddedToken(\"[control_486]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t489: AddedToken(\"[control_487]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t490: AddedToken(\"[control_488]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t491: AddedToken(\"[control_489]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t492: AddedToken(\"[control_490]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t493: AddedToken(\"[control_491]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t494: AddedToken(\"[control_492]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t495: AddedToken(\"[control_493]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t496: AddedToken(\"[control_494]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t497: AddedToken(\"[control_495]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t498: AddedToken(\"[control_496]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t499: AddedToken(\"[control_497]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t500: AddedToken(\"[control_498]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t501: AddedToken(\"[control_499]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t502: AddedToken(\"[control_500]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t503: AddedToken(\"[control_501]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t504: AddedToken(\"[control_502]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t505: AddedToken(\"[control_503]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t506: AddedToken(\"[control_504]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t507: AddedToken(\"[control_505]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t508: AddedToken(\"[control_506]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t509: AddedToken(\"[control_507]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t510: AddedToken(\"[control_508]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t511: AddedToken(\"[control_509]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t512: AddedToken(\"[control_510]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t513: AddedToken(\"[control_511]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t514: AddedToken(\"[control_512]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t515: AddedToken(\"[control_513]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t516: AddedToken(\"[control_514]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t517: AddedToken(\"[control_515]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t518: AddedToken(\"[control_516]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t519: AddedToken(\"[control_517]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t520: AddedToken(\"[control_518]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t521: AddedToken(\"[control_519]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t522: AddedToken(\"[control_520]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t523: AddedToken(\"[control_521]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t524: AddedToken(\"[control_522]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t525: AddedToken(\"[control_523]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t526: AddedToken(\"[control_524]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t527: AddedToken(\"[control_525]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t528: AddedToken(\"[control_526]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t529: AddedToken(\"[control_527]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t530: AddedToken(\"[control_528]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t531: AddedToken(\"[control_529]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t532: AddedToken(\"[control_530]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t533: AddedToken(\"[control_531]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t534: AddedToken(\"[control_532]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t535: AddedToken(\"[control_533]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t536: AddedToken(\"[control_534]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t537: AddedToken(\"[control_535]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t538: AddedToken(\"[control_536]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t539: AddedToken(\"[control_537]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t540: AddedToken(\"[control_538]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t541: AddedToken(\"[control_539]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t542: AddedToken(\"[control_540]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t543: AddedToken(\"[control_541]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t544: AddedToken(\"[control_542]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t545: AddedToken(\"[control_543]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t546: AddedToken(\"[control_544]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t547: AddedToken(\"[control_545]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t548: AddedToken(\"[control_546]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t549: AddedToken(\"[control_547]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t550: AddedToken(\"[control_548]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t551: AddedToken(\"[control_549]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t552: AddedToken(\"[control_550]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t553: AddedToken(\"[control_551]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t554: AddedToken(\"[control_552]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t555: AddedToken(\"[control_553]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t556: AddedToken(\"[control_554]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t557: AddedToken(\"[control_555]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t558: AddedToken(\"[control_556]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t559: AddedToken(\"[control_557]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t560: AddedToken(\"[control_558]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t561: AddedToken(\"[control_559]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t562: AddedToken(\"[control_560]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t563: AddedToken(\"[control_561]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t564: AddedToken(\"[control_562]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t565: AddedToken(\"[control_563]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t566: AddedToken(\"[control_564]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t567: AddedToken(\"[control_565]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t568: AddedToken(\"[control_566]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t569: AddedToken(\"[control_567]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t570: AddedToken(\"[control_568]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t571: AddedToken(\"[control_569]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t572: AddedToken(\"[control_570]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t573: AddedToken(\"[control_571]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t574: AddedToken(\"[control_572]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t575: AddedToken(\"[control_573]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t576: AddedToken(\"[control_574]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t577: AddedToken(\"[control_575]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t578: AddedToken(\"[control_576]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t579: AddedToken(\"[control_577]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t580: AddedToken(\"[control_578]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t581: AddedToken(\"[control_579]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t582: AddedToken(\"[control_580]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t583: AddedToken(\"[control_581]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t584: AddedToken(\"[control_582]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t585: AddedToken(\"[control_583]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t586: AddedToken(\"[control_584]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t587: AddedToken(\"[control_585]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t588: AddedToken(\"[control_586]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t589: AddedToken(\"[control_587]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t590: AddedToken(\"[control_588]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t591: AddedToken(\"[control_589]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t592: AddedToken(\"[control_590]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t593: AddedToken(\"[control_591]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t594: AddedToken(\"[control_592]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t595: AddedToken(\"[control_593]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t596: AddedToken(\"[control_594]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t597: AddedToken(\"[control_595]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t598: AddedToken(\"[control_596]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t599: AddedToken(\"[control_597]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t600: AddedToken(\"[control_598]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t601: AddedToken(\"[control_599]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t602: AddedToken(\"[control_600]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t603: AddedToken(\"[control_601]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t604: AddedToken(\"[control_602]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t605: AddedToken(\"[control_603]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t606: AddedToken(\"[control_604]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t607: AddedToken(\"[control_605]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t608: AddedToken(\"[control_606]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t609: AddedToken(\"[control_607]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t610: AddedToken(\"[control_608]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t611: AddedToken(\"[control_609]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t612: AddedToken(\"[control_610]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t613: AddedToken(\"[control_611]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t614: AddedToken(\"[control_612]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t615: AddedToken(\"[control_613]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t616: AddedToken(\"[control_614]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t617: AddedToken(\"[control_615]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t618: AddedToken(\"[control_616]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t619: AddedToken(\"[control_617]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t620: AddedToken(\"[control_618]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t621: AddedToken(\"[control_619]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t622: AddedToken(\"[control_620]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t623: AddedToken(\"[control_621]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t624: AddedToken(\"[control_622]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t625: AddedToken(\"[control_623]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t626: AddedToken(\"[control_624]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t627: AddedToken(\"[control_625]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t628: AddedToken(\"[control_626]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t629: AddedToken(\"[control_627]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t630: AddedToken(\"[control_628]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t631: AddedToken(\"[control_629]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t632: AddedToken(\"[control_630]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t633: AddedToken(\"[control_631]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t634: AddedToken(\"[control_632]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t635: AddedToken(\"[control_633]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t636: AddedToken(\"[control_634]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t637: AddedToken(\"[control_635]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t638: AddedToken(\"[control_636]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t639: AddedToken(\"[control_637]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t640: AddedToken(\"[control_638]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t641: AddedToken(\"[control_639]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t642: AddedToken(\"[control_640]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t643: AddedToken(\"[control_641]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t644: AddedToken(\"[control_642]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t645: AddedToken(\"[control_643]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t646: AddedToken(\"[control_644]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t647: AddedToken(\"[control_645]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t648: AddedToken(\"[control_646]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t649: AddedToken(\"[control_647]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t650: AddedToken(\"[control_648]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t651: AddedToken(\"[control_649]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t652: AddedToken(\"[control_650]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t653: AddedToken(\"[control_651]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t654: AddedToken(\"[control_652]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t655: AddedToken(\"[control_653]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t656: AddedToken(\"[control_654]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t657: AddedToken(\"[control_655]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t658: AddedToken(\"[control_656]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t659: AddedToken(\"[control_657]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t660: AddedToken(\"[control_658]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t661: AddedToken(\"[control_659]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t662: AddedToken(\"[control_660]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t663: AddedToken(\"[control_661]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t664: AddedToken(\"[control_662]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t665: AddedToken(\"[control_663]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t666: AddedToken(\"[control_664]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t667: AddedToken(\"[control_665]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t668: AddedToken(\"[control_666]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t669: AddedToken(\"[control_667]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t670: AddedToken(\"[control_668]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t671: AddedToken(\"[control_669]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t672: AddedToken(\"[control_670]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t673: AddedToken(\"[control_671]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t674: AddedToken(\"[control_672]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t675: AddedToken(\"[control_673]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t676: AddedToken(\"[control_674]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t677: AddedToken(\"[control_675]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t678: AddedToken(\"[control_676]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t679: AddedToken(\"[control_677]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t680: AddedToken(\"[control_678]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t681: AddedToken(\"[control_679]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t682: AddedToken(\"[control_680]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t683: AddedToken(\"[control_681]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t684: AddedToken(\"[control_682]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t685: AddedToken(\"[control_683]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t686: AddedToken(\"[control_684]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t687: AddedToken(\"[control_685]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t688: AddedToken(\"[control_686]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t689: AddedToken(\"[control_687]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t690: AddedToken(\"[control_688]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t691: AddedToken(\"[control_689]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t692: AddedToken(\"[control_690]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t693: AddedToken(\"[control_691]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t694: AddedToken(\"[control_692]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t695: AddedToken(\"[control_693]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t696: AddedToken(\"[control_694]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t697: AddedToken(\"[control_695]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t698: AddedToken(\"[control_696]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t699: AddedToken(\"[control_697]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t700: AddedToken(\"[control_698]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t701: AddedToken(\"[control_699]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t702: AddedToken(\"[control_700]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t703: AddedToken(\"[control_701]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t704: AddedToken(\"[control_702]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t705: AddedToken(\"[control_703]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t706: AddedToken(\"[control_704]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t707: AddedToken(\"[control_705]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t708: AddedToken(\"[control_706]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t709: AddedToken(\"[control_707]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t710: AddedToken(\"[control_708]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t711: AddedToken(\"[control_709]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t712: AddedToken(\"[control_710]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t713: AddedToken(\"[control_711]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t714: AddedToken(\"[control_712]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t715: AddedToken(\"[control_713]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t716: AddedToken(\"[control_714]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t717: AddedToken(\"[control_715]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t718: AddedToken(\"[control_716]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t719: AddedToken(\"[control_717]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t720: AddedToken(\"[control_718]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t721: AddedToken(\"[control_719]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t722: AddedToken(\"[control_720]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t723: AddedToken(\"[control_721]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t724: AddedToken(\"[control_722]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t725: AddedToken(\"[control_723]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t726: AddedToken(\"[control_724]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t727: AddedToken(\"[control_725]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t728: AddedToken(\"[control_726]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t729: AddedToken(\"[control_727]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t730: AddedToken(\"[control_728]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t731: AddedToken(\"[control_729]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t732: AddedToken(\"[control_730]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t733: AddedToken(\"[control_731]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t734: AddedToken(\"[control_732]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t735: AddedToken(\"[control_733]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t736: AddedToken(\"[control_734]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t737: AddedToken(\"[control_735]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t738: AddedToken(\"[control_736]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t739: AddedToken(\"[control_737]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t740: AddedToken(\"[control_738]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t741: AddedToken(\"[control_739]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t742: AddedToken(\"[control_740]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t743: AddedToken(\"[control_741]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t744: AddedToken(\"[control_742]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t745: AddedToken(\"[control_743]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t746: AddedToken(\"[control_744]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t747: AddedToken(\"[control_745]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t748: AddedToken(\"[control_746]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t749: AddedToken(\"[control_747]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t750: AddedToken(\"[control_748]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t751: AddedToken(\"[control_749]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t752: AddedToken(\"[control_750]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t753: AddedToken(\"[control_751]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t754: AddedToken(\"[control_752]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t755: AddedToken(\"[control_753]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t756: AddedToken(\"[control_754]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t757: AddedToken(\"[control_755]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t758: AddedToken(\"[control_756]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t759: AddedToken(\"[control_757]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t760: AddedToken(\"[control_758]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t761: AddedToken(\"[control_759]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t762: AddedToken(\"[control_760]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t763: AddedToken(\"[control_761]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t764: AddedToken(\"[control_762]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t765: AddedToken(\"[control_763]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t766: AddedToken(\"[control_764]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t767: AddedToken(\"[control_765]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t768: AddedToken(\"[control_766]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t769: AddedToken(\"[control_767]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t770: AddedToken(\"[control_768]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float16'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.to_dict()[\"torch_dtype\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp#L19\n",
    "# From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\n",
    "ALLOWED_QUANTS = \\\n",
    "{\n",
    "    \"not_quantized\"  : \"Recommended. Fast conversion. Slow inference, big files.\",\n",
    "    \"fast_quantized\" : \"Recommended. Fast conversion. OK inference, OK file size.\",\n",
    "    \"quantized\"      : \"Recommended. Slow conversion. Fast inference, small files.\",\n",
    "    \"f32\"     : \"Not recommended. Retains 100% accuracy, but super slow and memory hungry.\",\n",
    "    \"f16\"     : \"Fastest conversion + retains 100% accuracy. Slow and memory hungry.\",\n",
    "    \"q8_0\"    : \"Fast conversion. High resource use, but generally acceptable.\",\n",
    "    \"q4_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\n",
    "    \"q5_k_m\"  : \"Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\n",
    "    \"q2_k\"    : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\n",
    "    \"q3_k_l\"  : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\n",
    "    \"q3_k_m\"  : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\n",
    "    \"q3_k_s\"  : \"Uses Q3_K for all tensors\",\n",
    "    \"q4_0\"    : \"Original quant method, 4-bit.\",\n",
    "    \"q4_1\"    : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\n",
    "    \"q4_k_s\"  : \"Uses Q4_K for all tensors\",\n",
    "    \"q4_k\"    : \"alias for q4_k_m\",\n",
    "    \"q5_k\"    : \"alias for q5_k_m\",\n",
    "    \"q5_0\"    : \"Higher accuracy, higher resource usage and slower inference.\",\n",
    "    \"q5_1\"    : \"Even higher accuracy, resource usage and slower inference.\",\n",
    "    \"q5_k_s\"  : \"Uses Q5_K for all tensors\",\n",
    "    \"q6_k\"    : \"Uses Q8_K for all tensors\",\n",
    "    \"iq2_xxs\" : \"2.06 bpw quantization\",\n",
    "    \"iq2_xs\"  : \"2.31 bpw quantization\",\n",
    "    \"iq3_xxs\" : \"3.06 bpw quantization\",\n",
    "    \"q3_k_xs\" : \"3-bit extra small quantization\",\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 178.35 out of 251.41 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 28/32 [00:00<00:00, 38.90it/s]We will save to Disk and not RAM now.\n",
      "100%|██████████| 32/32 [00:01<00:00, 31.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# if save_pretrained_gguf fails\n",
    "\n",
    "#model.save_pretrained_merged(\"model/mistral-rag-instruct\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 202.92 out of 251.39 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 35.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting mistral model. Can use fast conversion = True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting model at model/mistral-rag-instruct into f16 GGUF format.\n",
      "The output location will be ./model/mistral-rag-instruct/unsloth.F16.gguf\n",
      "This will take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: mistral-rag-instruct\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 32768}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {4096, 32768}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 2\n",
      "INFO:gguf.vocab:Setting special token type pad to 770\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content'] %}\n",
      "    {%- set loop_messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set loop_messages = messages %}\n",
      "{%- endif %}\n",
      "\n",
      "{{- bos_token }}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
      "        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n",
      "    {%- endif %}\n",
      "    {%- if message['role'] == 'user' %}\n",
      "        {%- if loop.last and system_message is defined %}\n",
      "            {{- '[INST] ' + system_message + '\\n\\n' + message['content'] + '[/INST]' }}\n",
      "        {%- else %}\n",
      "            {{- '[INST] ' + message['content'] + '[/INST]' }}\n",
      "        {%- endif %}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "        {{- ' ' + message['content'] + eos_token}}\n",
      "    {%- else %}\n",
      "        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:model/mistral-rag-instruct/unsloth.F16.gguf: n_tensors = 291, total_size = 14.5G\n",
      "Writing: 100%|██████████| 14.5G/14.5G [00:07<00:00, 2.00Gbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to model/mistral-rag-instruct/unsloth.F16.gguf\n",
      "Unsloth: Conversion completed! Output location: ./model/mistral-rag-instruct/unsloth.F16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
      "main: build = 3486 (0832de72)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing './model/mistral-rag-instruct/unsloth.F16.gguf' to './model/mistral-rag-instruct/unsloth.Q4_K_M.gguf' as Q4_K_M using 40 threads\n",
      "llama_model_loader: loaded meta data with 34 key-value pairs and 291 tensors from ./model/mistral-rag-instruct/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Mistral 7b v0.3 Bnb 4bit\n",
      "llama_model_loader: - kv   3:                            general.version str              = v0.3\n",
      "llama_model_loader: - kv   4:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   5:                           general.finetune str              = bnb-4bit\n",
      "llama_model_loader: - kv   6:                           general.basename str              = mistral\n",
      "llama_model_loader: - kv   7:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 32768\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.add_space_prefix bool             = true\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.scores arr[f32,32768]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,32768]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 770\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 32768,     1,     1], type =    f16, converting to q4_K .. size =   256.00 MiB ->    72.00 MiB\n",
      "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  20/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  21/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  22/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  23/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  24/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  25/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  26/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  29/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  30/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  31/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  32/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  33/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  34/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  35/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  38/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  39/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  40/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  41/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  42/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  43/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  44/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  47/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  48/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  49/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  50/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  51/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  52/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  53/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  56/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  57/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  58/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  59/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  60/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  61/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  62/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  65/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  66/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  67/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  68/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  69/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  70/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  71/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  74/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  75/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  76/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  77/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  78/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  79/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  80/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  83/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  84/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  85/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  86/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  87/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  88/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  89/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  92/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[  93/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  94/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  95/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  96/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  97/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  98/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 101/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 103/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 104/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 105/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 106/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 107/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 108/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 109/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 110/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 112/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 113/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 114/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 115/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 116/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 117/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 118/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 119/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 121/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 122/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 123/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 125/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 126/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 127/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 128/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 130/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 131/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 132/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 133/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 134/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 135/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 136/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 137/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 139/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 140/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 141/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 143/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 144/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 145/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 146/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 148/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 149/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 150/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 151/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 152/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 153/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 154/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 155/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 157/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 158/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 159/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 161/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 162/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 163/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 164/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 166/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 167/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 168/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 169/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 170/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 171/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 172/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 173/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 174/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 175/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 176/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 177/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 178/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 179/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 180/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 181/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 182/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 184/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 185/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 186/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 187/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 188/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 189/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 190/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 200/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 201/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 202/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 203/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 204/ 291]                        output.weight - [ 4096, 32768,     1,     1], type =    f16, converting to q6_K .. size =   256.00 MiB ->   105.00 MiB\n",
      "[ 205/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 206/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 207/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 208/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 212/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 213/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 214/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 215/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 216/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 217/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 218/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 219/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 221/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 222/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 223/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 224/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 225/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 226/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 227/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 228/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 230/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 231/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 232/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 233/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 234/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 235/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 236/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 237/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 239/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 240/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 241/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 242/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 243/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 244/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 245/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 246/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 248/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 249/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 250/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 251/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 252/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 253/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 254/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 255/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 257/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 258/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 259/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 260/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 261/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 262/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 263/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 264/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 266/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 267/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 268/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 269/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 270/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 271/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 272/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 273/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 275/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 276/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 277/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 278/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 279/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 280/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 281/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 282/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 284/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 285/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
      "[ 286/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 287/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 288/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 289/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 290/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 13825.02 MB\n",
      "llama_model_quantize_internal: quant size  =  4169.52 MB\n",
      "\n",
      "main: quantize time = 58289.58 ms\n",
      "main:    total time = 58289.58 ms\n",
      "Unsloth: Conversion completed! Output location: ./model/mistral-rag-instruct/unsloth.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "#model.save_pretrained_gguf(\"model/mistral-rag-instruct\", tokenizer, quantization_method = \"q8_0\") # Save to 8bit Q8_0\n",
    "model.save_pretrained_gguf(\"model/mistral-rag-instruct\", tokenizer, quantization_method = \"q4_k_m\") # Save to 4bit q4_k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1f1d891eef423ebd22eac0282e44f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 \n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id) #, add_bos_token=True, trust_remote_code=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"checkpoints/raft-v2/checkpoint-250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_prompt(query: str, context: str):\n",
    "    system_prompt = \"You are a smart helpful assistant for the HTWG Konstanz. Answer the following question based only on the provided context. It is mandatory to answer in GERMAN:\\n\\n\"\n",
    "    return f\"[INST]{system_prompt}Context: {context}\\n\\nQuestion: {query}[/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "merged_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/merged_model/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/file_download.py:1325\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/file_download.py:1823\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1822\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1823\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-66a791fc-39ce1a3e078e7ceb1d832ea8;a1973306-ad7d-48fc-9030-a423f15629c0)\n\nRepository Not Found for url: https://huggingface.co/merged_model/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 3\u001b[0m ft_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m eval_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:503\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m         adapter_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 503\u001b[0m maybe_adapter_path \u001b[38;5;241m=\u001b[39m \u001b[43mfind_adapter_config_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maybe_adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(maybe_adapter_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/utils/peft_utils.py:88\u001b[0m, in \u001b[0;36mfind_adapter_config_file\u001b[0;34m(model_id, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, _commit_hash)\u001b[0m\n\u001b[1;32m     86\u001b[0m         adapter_cached_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_id, ADAPTER_CONFIG_NAME)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     adapter_cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mADAPTER_CONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adapter_cached_filename\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/utils/hub.py:425\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: merged_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"merged_model\", local_files_only=True, device_map='auto'\n",
    ")\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m ft_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(eval_tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mft_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.15\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/generation/utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1907\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1908\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1909\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1911\u001b[0m     )\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1929\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m     )\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/generation/utils.py:2651\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2648\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2651\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2659\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1200\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1197\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1200\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1214\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:976\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    965\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    966\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    967\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m         cache_position,\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 976\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:732\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    731\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 732\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    735\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:171\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 164\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/accelerate/hooks.py:354\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    347\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    348\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    351\u001b[0m         ):\n\u001b[1;32m    352\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 354\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    364\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    365\u001b[0m )\n",
      "File \u001b[0;32m~/environments/finetuning/lib/python3.11/site-packages/accelerate/utils/modeling.py:405\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    403\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 405\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = [{\"page_content\": \"\\n\\u00a7 4 Sprachkenntnisse\\n(1) 1Neben den allgemeinen Zugangsvoraussetzungen (\\u00a7 59 LHG) sind f\\u00fcr die in \\u00a7 1 Abs. 1\\nS. 1 genannten Studieng\\u00e4nge deutsche Sprach kenntnisse nachzuweisen. 2Diese k\\u00f6nnen\\ndurch eine deutsche Hochschulzugangsberechtigung (u. a. erfolgreich abgeschlossenes\\ngrundst\\u00e4ndiges Hochschulstudium) nachgewiesen werden. 3Ferner kann der\\nSprachnachweis durch die Vorlage eines der folgenden Dokumente erbracht werden:\\n1. Feststellungspr\\u00fcfung f\\u00fcr ein Bachelorstudium durch Vorlage der Zugangsberechtigung\\ndes Studienkollegs an der Hochschule Konstanz,\\n2. Test Deutsch als Fremdsprache (TestDaF), sofern im Durchschnitt mindestens die\\nStufe TDN 4 erreicht wurde,   Seite 5 von 43 3. Deutsche Sprachpr\\u00fcfung f\\u00fcr den Hochschulzugang (DSH), sofern die DSH mit\\nmindestens der Stufe DSH -2 abgeschlossen wurde,\\n4. \\u201eTelc Deutsch C1 Hochschule\\u201c\\noder eine \\u00e4quivalente Sprachpr\\u00fcfung gem\\u00e4\\u00df der Rahmenordnung \\u00fcber Deutsche\\nSprachpr\\u00fcfungen f\\u00fcr das Studium an deutschen Hochschulen (RO -DT). 4Auf den Nachweis\\neiner deutschen Sprachpr\\u00fcfung kann bei Bewerber innen und Bewerbern im besonders\\nbegr\\u00fcndeten Einzelfall verzichtet werden, insbesondere wenn sie die deutsche\\nStaatsangeh\\u00f6rigkeit besitzen.\\n(2) 1Sprachnachweise f\\u00fcr den gew\\u00e4hl ten Studiengang, die durch die Bewerberin oder den\\nBewerber bis zum Bewerbungsschluss nicht vorgelegt werden k\\u00f6nnen, k\\u00f6nnen bis zum\\nVorlesungsbeginn des Semesters gem\\u00e4\\u00df Terminplan der Hochschule Konstanz, f\\u00fcr das  der\\nAntrag auf Zulassung gestellt wurde, nachgereicht werden. 2Die Zulassung erfolgt in diesem\\nFall gem \\u00e4\\u00df \\u00a7 6  Abs. 5  unter Vorbehalt .\\n(3) 1F\\u00fcr Zeitstudierende gelten die Regelungen in \\u00a7 10 Zulassungs - und\\nImmatrikulationsordnung (ZIO) der Hochschule Konstanz.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"\\n\\u00a7 21b Mechatronik (MME) Berufsbegleitendes Studium\\n(1) Studiengangspezifische Zugangsvoraussetzungen gem\\u00e4\\u00df \\u00a7 5  Abs. 1\\nZugangsvoraussetzungen f\\u00fcr den Masterstudiengang Mechatronik sind:\\n1. Ein mit der Note 2,9 oder besser abgeschlossenes grundst\\u00e4ndiges Hochschulstudium\\ngem\\u00e4\\u00df \\u00a7 5 Abs. 1 Nr. 1 in einem Studiengang der Fachrichtungen Systemtechnik,\\nMaschinenbau, Elektrotechnik, Fahrzeugtechnik, Mechatronik, Feinwerktechnik oder einer\\nverwandten Fachrichtung.\\n2. Englischkenntnisse, \\u00e4quivalent z u Niveau- Stufe B1 des Europ\\u00e4ischen Referenzrahmens\\nf\\u00fcr das Lernen und Lehren von Fremdsprachen. Als \\u00e4quivalent zu einem Zertifikat \\u00fcber die\\nNiveau -Stufe B1 gelten insbesondere folgende Nachweise:\\nI. das Schulabschlusszeugnis, aus dem der Besuch des Englischunterrichts bis zum\\nErreichen des mittleren Bildungsabschlusses (10. Klasse) bzw. bis zum Erreichen\\nder Fachhochschulreife hervorgeht oder\\nII. ein Notenspiegel, aus dem die bestandene Pr\\u00fcfungsleistung \\u00fcber eine\\nLehrveranstaltung im Rahmen des grundst\\u00e4ndigen Studiums hervorgeht, die die\\nenglische Sprache zum Inhalt hatte oder\\nIII. eine Bescheinigung \\u00fcber den mindestens sechsmonatigen Aufenthalt an einer Schule, Hochschule oder anderen Bildungsinstitution mit Englisch als\\nUnterrichtssprache oder\\nIV. eine Bescheinigung \\u00fcber den Aufenthalt im englischsprachigen Ausland, der einen Zeitraum von mindestens sechs Monaten bzw. einem Studiensemester umfasst.\\nDie Vorlage anderer geeigneter Nachweise ist m\\u00f6glich.\\n(2) Auswahlkriterien nach \\u00a7 9 Abs. 2\\n1. Ergebnis eines Auswahlgespr\\u00e4chs\\nNicht zutreffend.\\n2. Leistungen, die mit der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Studiums nach Abs. 1\\ni. V. m. \\u00a7 5 Abs. 1 Nr. 1 nachgewiesen sind\\nDie Durchschnittsnote der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums nach\\nAbs. 1 bildet die Teilnote 1 als Basis zur Bestimmung der Auswahlnote.  Abweichend von Satz\\n1 bildet in den F\\u00e4llen des \\u00a7 3 Abs. 2 Nr. 1 Satz 2 die Durchschnittsnote nach \\u00a7 3 Abs. 2 Nr. 1 Satz 3 die Teilnote 1. Bei ausl\\u00e4ndischen Bildungsnachweisen ist die Durchschnittsnote nach\\ndeutsc her Deutung als Teilnote 1 zu ber\\u00fccksichtigen.\\nZus\\u00e4tzlich werden die Einzelnoten folgender F\\u00e4cher der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums, die \\u00fcber die Eignung f\\u00fcr den gew\\u00e4hlten Studiengang\\nbesonderen Aufschluss geben, f\\u00fcr die Auswahl herangezogen:\\n- Technische Mechanik (Dynamik),\\n- Elektrotechnik,\\n- Messtechnik,\\n- Regelungstechnik,\\n- Elektrische Antriebe.\\nDabei wird eine Note zwischen 1,0 und 1,7 in einem der o. g. F\\u00e4cher jeweils mit dem Wert 0,1\\nbewertet. Die kumulierte Gesamtzahl bildet die Teil note 2.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"\\n\\u00a7 21a Mechatronik (MME) Vollzeitstudium\\n(1) Studiengangspezifische Zugangsvoraussetzungen gem\\u00e4\\u00df \\u00a7 5  Abs. 1\\nZugangsvoraussetzungen f\\u00fcr den Masterstudiengang Mechatronik sind:\\n1. Ein mit der Note 2,9 oder besser abgeschlossenes grundst\\u00e4ndiges Hochschulstudium\\ngem\\u00e4\\u00df \\u00a7 5 Abs. 1 Nr. 1 in einem Studiengang der Fachrichtungen Maschinenbau,\\nElektrotechnik, Fahrzeugtechnik, Mechatronik, Feinwerktechnik oder einer verwandten\\nFachrichtung.\\n2. Englischkenntnisse, \\u00e4quivalent zu Niveau- Stufe B1 des Europ\\u00e4ischen Referenzrahmens\\nf\\u00fcr das Lernen und Lehren von Fremdsprachen. Als \\u00e4quivalent zu einem Zertifikat \\u00fcber die\\nNiveau -Stufe B1 gelten insbesondere folgende Nachweise:\\nI. das Schulabschlusszeugnis, aus dem der Besuch des Englischunterrichts bis zum\\nErreichen des mittleren Bildungsabschlusses (10. Klass e) bzw. bis zum Erreichen\\nder Fachhochschulreife hervorgeht oder\\nII. ein Notenspiegel, aus dem die bestandene Pr\\u00fcfungsleistung \\u00fcber eine\\nLehrveranstaltung im Rahmen des grundst\\u00e4ndigen Studiums hervorgeht, die die\\nenglische Sprache zum Inhalt hatte oder\\nIII. eine Bescheinigung \\u00fcber den mindestens sechsmonatigen Aufenthalt an einer Schule, Hochschule oder anderen Bildungsinstitution mit Englisch als\\nUnterrichtssprache oder\\nIV. eine Bescheinigung \\u00fcber den Aufenthalt im englischsprachigen Ausland, der einen Zeitraum von mindestens sechs Monaten bzw. einem Studiensemester umfasst.\\nDie Vorlage anderer geeigneter Nachweise ist m\\u00f6glich.\\n(2) Auswahlkriterien nach \\u00a7 9 Abs. 2\\n1. Ergebnis eines Auswahlgespr\\u00e4chs\\nNicht zutreffend.\\n2. Leistungen, die mit der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Studiums nach Abs. 1\\ni. V. m. \\u00a7 5 Abs. 1 Nr. 1 nachgewiesen sind\\nDie Durchschnittsnote der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums nach\\nAbs. 1 bildet die Teilnote 1 als Basis zur Bestimmung der Auswahlnote. Abweichend von Satz\\n1 bildet in den F\\u00e4llen des \\u00a7 3 Abs. 2 Nr. 1 Satz 2 die Durchschnittsnote nach \\u00a7 3 Abs. 2 Nr. 1\\nSatz 3 die Teilnote 1. Bei ausl\\u00e4ndischen Bildungsnachweisen ist die Durchschnittsnote nach\\ndeutscher Deutung als Teilnote 1 zu ber\\u00fccksichtigen.\\nZus\\u00e4tzlich werden die Einzelnoten folgender F\\u00e4cher der Abschlusspr\\u00fcfung des grundst\\u00e4ndigen Hochschulstudiums, die \\u00fcber die Eignung f\\u00fcr den gew\\u00e4hlten Studiengang\\nbesonderen Aufschluss geben, f\\u00fcr die Auswahl herangezogen:\\n- Technische Mechanik (Dynamik),\\n- Elektrotechnik,\\n- Messt echnik,\\n- Regelungstechnik,\\n- Elektrische Antriebe.\\nDabei wird eine Note zwischen 1,0 und 1,7 in einem der o. g. F\\u00e4cher jeweils mit dem Wert 0,1\\nbewertet. Die kumulierte Gesamtzahl bildet die Teilnote 2.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"\\n\\u00a7 9 Zugangs - und Auswahlkriterien in den Masterstudieng\\u00e4ngen\\n(1)  1Im Besonderen Teil (\\u00a7\\u00a7 12 -26) dieser Satzung k\\u00f6nnen ein oder mehrere der in Absatz 2\\ngenannten Auswahlkriterien als weitere Zugangskriterien festgelegt werden. 2N\\u00e4heres\\nregelt der B esondere Teil f\\u00fcr den jeweiligen Studiengang (\\u00a7\\u00a7 12 -26).\\n(2)  1F\\u00fcr die Bildung der Ranglisten f\\u00fcr das erste Fachsemester in den Masterstudieng\\u00e4ngen\\nwird, neben dem Ergebnis des fachlich einschl\\u00e4gigen Hochschulabschlusses oder des\\ngleichwertigen Abschlusses,  mindestens eines der folgenden  Auswahlkriterien\\nber\\u00fccksichtigt:\\n1. Leistungen, die in dem Studium erbracht wurden, das Voraussetzung f\\u00fcr den Zugang\\nzu dem Masterstudiengang ist ,   Seite 8 von 43 2. Englischkenntnisse , n\\u00e4heres regelt der Besondere Teil f\\u00fcr den jeweiligen Studiengang\\n(\\u00a7\\u00a7 12 -26),\\n3. Berufst\\u00e4tigkeit und Qualifikationen:\\na) Art einer abgeschlossenen Berufsausbildung oder einer Berufst\\u00e4tigkeit in einem\\nanerkannten Ausbildungsberuf  oder eine andere einschl\\u00e4gige Berufst\\u00e4tigkeit , die \\u00fcber\\ndie fachspezifische Eignung Auskunft gibt, jeweils  einzeln und in Kombination, und\\nb) Qualifikation en, die \\u00fcber die fachspezifische Leistung Auskunft geben, jeweils einzeln\\noder in Kombination,\\n4. das Ergebnis eines fachspezifischen Studieneignungstests ,\\n5. das Ergebnis des Auswahlgespr\\u00e4chs/anderen m\\u00fcndlichen Verfahrens  gem\\u00e4\\u00df \\u00a7 9a ,\\n6. ein Motivationsschreiben,\\n7. eine schriftliche Abhandlung (Essay).\\n2N\\u00e4heres sowie die Gewichtung regelt der B esondere Teil f\\u00fcr den jeweiligen Studiengang (\\u00a7\\u00a7\\n12-26).\\n(2) 1Die Auswahl f\\u00fcr h\\u00f6here Fachsemester erfolgt gem\\u00e4\\u00df \\u00a7 7 HZG i. V. m. \\u00a7 32 HZVO.\", \"metadata\": {\"file_path\": \"/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf\"}, \"type\": \"Document\"}]\n",
    "context = [entry['page_content'] for entry in data]\n",
    "eval_prompt = create_eval_prompt(\"\"\"Welche Dokumente kÃ¶nnen als Nachweis fÃ¼r deutsche Sprachkenntnisse akzeptiert werden?\"\"\", str(context))\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=500, repetition_penalty=1.15)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpllmws23/environments/finetuning/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = ft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merged_adapters/tokenizer_config.json',\n",
       " 'merged_adapters/special_tokens_map.json',\n",
       " 'merged_adapters/tokenizer.model',\n",
       " 'merged_adapters/added_tokens.json',\n",
       " 'merged_adapters/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"merged_adapters\")\n",
    "#eval_tokenizer.pad_token = eval_tokenizer.unk_token\n",
    "eval_tokenizer.save_pretrained(\"merged_adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858bd148dc264e539d59729382a68ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to Hugging Face Hub: mistral-rag-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df3fbef27a34adba5804c6f3a2fb810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.41G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15bee98584748a5a5a03a36d2ab93ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffa393fd44349d7a0c5ed787b105946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/BenjaminBruenau/mistral-rag-instruct/commit/5d46b37ba2657fafa0abbc1cd5f56f3d1ab39a69', commit_message='Upload tokenizer', commit_description='', oid='5d46b37ba2657fafa0abbc1cd5f56f3d1ab39a69', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_id = \"mistral-rag-instruct\"\n",
    "#model.save_pretrained(f\"{hub_id}\")\n",
    "#eval_tokenizer.save_pretrained(f\"{hub_id}\")\n",
    "\n",
    "print(f\"Uploading to Hugging Face Hub: {hub_id}\")\n",
    "model.push_to_hub(f\"{hub_id}\")\n",
    "eval_tokenizer.push_to_hub(f\"{hub_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Finetuned Model to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529a887cec894a3d91521eca81c1b565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea392fc6d3dd484db08b8686039b417b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "# checkpoints/raft/checkpoint-425\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"unload\", torch_dtype=torch.bfloat16, device_map={\"\": \"cpu\"})\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to Hugging Face Hub: mistral-rag-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f47dde17a7f4ed9898cc50470d1e15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6081bf1e943d45c7b475b8a24a87d917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f617eee55e411fb93a3980ddeae488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39adb4678f094ad9aa37707430269827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/BenjaminBruenau/mistral-rag-instruct/commit/92ad810150eb971fe514b237d5773fa920e631c0', commit_message='Upload tokenizer', commit_description='', oid='92ad810150eb971fe514b237d5773fa920e631c0', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_id = \"mistral-rag-instruct\"\n",
    "#model.save_pretrained(f\"{hub_id}\")\n",
    "#eval_tokenizer.save_pretrained(f\"{hub_id}\")\n",
    "\n",
    "print(f\"Uploading to Hugging Face Hub: {hub_id}\")\n",
    "base_model.push_to_hub(f\"{hub_id}\")\n",
    "tokenizer.push_to_hub(f\"{hub_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge and Manual GGUF Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth unsuccessfully patched LoraLayer.update_layer. Please file a bug report.\n",
      "Luckily, your training run will still work in the meantime!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA TITAN RTX. Max memory: 23.643 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load checkpoints/raft/checkpoint-425 as a legacy tokenizer.\n",
      "unsloth/mistral-7b-v0.3-bnb-4bit does not have a padding token! Will use pad_token = [control_768].\n",
      "Unsloth 2024.6 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "#\"checkpoints/raft-completion-only/checkpoint-350\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"checkpoints/raft/checkpoint-425\", load_in_4bit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 179.01 out of 251.41 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 29/32 [00:00<00:00, 37.23it/s]We will save to Disk and not RAM now.\n",
      "100%|██████████| 32/32 [00:01<00:00, 29.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we tried to dequantize and merge the model by hand to see if that mitigates the conversion quality loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import peft\n",
    "import json\n",
    "import shutil\n",
    "from peft.utils import _get_submodules\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "def dequantize_model(model, to='./dequantized_model', dtype=torch.float16, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    'model': the peftmodel you loaded with qlora.\n",
    "    'tokenizer': the model's corresponding hf's tokenizer.\n",
    "    'to': directory to save the dequantized model\n",
    "    'dtype': dtype that the model was trained using\n",
    "    'device': device to load the model to\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    os.makedirs(to, exist_ok=True)\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, cls):\n",
    "                print(f\"Dequantizing `{name}`...\")\n",
    "                quant_state = copy.deepcopy(module.weight.quant_state)\n",
    "                quant_state.dtype = dtype\n",
    "\n",
    "                weights = dequantize_4bit(module.weight.data, quant_state=quant_state, quant_type=\"nf4\").to(dtype)\n",
    "\n",
    "                new_module = torch.nn.Linear(module.in_features, module.out_features, bias=None, dtype=dtype)\n",
    "                new_module.weight = torch.nn.Parameter(weights)\n",
    "                new_module.to(device=\"cpu\", dtype=dtype)\n",
    "\n",
    "                parent, target, target_name = _get_submodules(model, name)\n",
    "                setattr(parent, target_name, new_module)\n",
    "\n",
    "        # a hack, setting this to avoid hf's saving error because hf\n",
    "        # itself does not support saving a model that is registered to be loaded in 4bit.\n",
    "        model.is_loaded_in_4bit = False\n",
    "\n",
    "        print(\"Saving dequantized model...\")\n",
    "        model.save_pretrained(to)\n",
    "        #tokenizer.save_pretrained(to)\n",
    "        config_data = json.loads(open(os.path.join(to, 'config.json'), 'r').read())\n",
    "        config_data.pop(\"quantization_config\", None)\n",
    "        config_data.pop(\"pretraining_tp\", None)\n",
    "        with open(os.path.join(to, 'config.json'), 'w') as config:\n",
    "            config.write(json.dumps(config_data, indent=2))\n",
    "\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the model mistralai/Mistral-7B-v0.3 into memory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fa4dcee3a44ca6acfc23a209596da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32768, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
      ")\n",
      "Dequantizing `model.layers.0.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.0.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.0.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.0.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.0.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.0.mlp.up_proj`...\n",
      "Dequantizing `model.layers.0.mlp.down_proj`...\n",
      "Dequantizing `model.layers.1.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.1.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.1.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.1.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.1.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.1.mlp.up_proj`...\n",
      "Dequantizing `model.layers.1.mlp.down_proj`...\n",
      "Dequantizing `model.layers.2.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.2.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.2.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.2.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.2.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.2.mlp.up_proj`...\n",
      "Dequantizing `model.layers.2.mlp.down_proj`...\n",
      "Dequantizing `model.layers.3.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.3.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.3.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.3.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.3.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.3.mlp.up_proj`...\n",
      "Dequantizing `model.layers.3.mlp.down_proj`...\n",
      "Dequantizing `model.layers.4.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.4.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.4.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.4.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.4.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.4.mlp.up_proj`...\n",
      "Dequantizing `model.layers.4.mlp.down_proj`...\n",
      "Dequantizing `model.layers.5.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.5.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.5.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.5.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.5.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.5.mlp.up_proj`...\n",
      "Dequantizing `model.layers.5.mlp.down_proj`...\n",
      "Dequantizing `model.layers.6.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.6.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.6.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.6.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.6.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.6.mlp.up_proj`...\n",
      "Dequantizing `model.layers.6.mlp.down_proj`...\n",
      "Dequantizing `model.layers.7.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.7.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.7.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.7.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.7.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.7.mlp.up_proj`...\n",
      "Dequantizing `model.layers.7.mlp.down_proj`...\n",
      "Dequantizing `model.layers.8.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.8.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.8.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.8.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.8.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.8.mlp.up_proj`...\n",
      "Dequantizing `model.layers.8.mlp.down_proj`...\n",
      "Dequantizing `model.layers.9.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.9.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.9.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.9.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.9.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.9.mlp.up_proj`...\n",
      "Dequantizing `model.layers.9.mlp.down_proj`...\n",
      "Dequantizing `model.layers.10.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.10.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.10.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.10.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.10.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.10.mlp.up_proj`...\n",
      "Dequantizing `model.layers.10.mlp.down_proj`...\n",
      "Dequantizing `model.layers.11.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.11.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.11.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.11.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.11.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.11.mlp.up_proj`...\n",
      "Dequantizing `model.layers.11.mlp.down_proj`...\n",
      "Dequantizing `model.layers.12.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.12.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.12.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.12.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.12.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.12.mlp.up_proj`...\n",
      "Dequantizing `model.layers.12.mlp.down_proj`...\n",
      "Dequantizing `model.layers.13.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.13.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.13.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.13.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.13.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.13.mlp.up_proj`...\n",
      "Dequantizing `model.layers.13.mlp.down_proj`...\n",
      "Dequantizing `model.layers.14.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.14.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.14.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.14.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.14.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.14.mlp.up_proj`...\n",
      "Dequantizing `model.layers.14.mlp.down_proj`...\n",
      "Dequantizing `model.layers.15.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.15.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.15.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.15.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.15.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.15.mlp.up_proj`...\n",
      "Dequantizing `model.layers.15.mlp.down_proj`...\n",
      "Dequantizing `model.layers.16.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.16.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.16.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.16.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.16.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.16.mlp.up_proj`...\n",
      "Dequantizing `model.layers.16.mlp.down_proj`...\n",
      "Dequantizing `model.layers.17.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.17.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.17.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.17.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.17.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.17.mlp.up_proj`...\n",
      "Dequantizing `model.layers.17.mlp.down_proj`...\n",
      "Dequantizing `model.layers.18.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.18.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.18.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.18.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.18.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.18.mlp.up_proj`...\n",
      "Dequantizing `model.layers.18.mlp.down_proj`...\n",
      "Dequantizing `model.layers.19.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.19.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.19.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.19.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.19.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.19.mlp.up_proj`...\n",
      "Dequantizing `model.layers.19.mlp.down_proj`...\n",
      "Dequantizing `model.layers.20.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.20.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.20.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.20.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.20.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.20.mlp.up_proj`...\n",
      "Dequantizing `model.layers.20.mlp.down_proj`...\n",
      "Dequantizing `model.layers.21.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.21.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.21.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.21.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.21.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.21.mlp.up_proj`...\n",
      "Dequantizing `model.layers.21.mlp.down_proj`...\n",
      "Dequantizing `model.layers.22.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.22.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.22.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.22.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.22.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.22.mlp.up_proj`...\n",
      "Dequantizing `model.layers.22.mlp.down_proj`...\n",
      "Dequantizing `model.layers.23.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.23.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.23.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.23.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.23.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.23.mlp.up_proj`...\n",
      "Dequantizing `model.layers.23.mlp.down_proj`...\n",
      "Dequantizing `model.layers.24.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.24.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.24.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.24.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.24.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.24.mlp.up_proj`...\n",
      "Dequantizing `model.layers.24.mlp.down_proj`...\n",
      "Dequantizing `model.layers.25.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.25.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.25.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.25.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.25.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.25.mlp.up_proj`...\n",
      "Dequantizing `model.layers.25.mlp.down_proj`...\n",
      "Dequantizing `model.layers.26.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.26.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.26.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.26.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.26.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.26.mlp.up_proj`...\n",
      "Dequantizing `model.layers.26.mlp.down_proj`...\n",
      "Dequantizing `model.layers.27.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.27.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.27.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.27.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.27.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.27.mlp.up_proj`...\n",
      "Dequantizing `model.layers.27.mlp.down_proj`...\n",
      "Dequantizing `model.layers.28.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.28.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.28.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.28.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.28.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.28.mlp.up_proj`...\n",
      "Dequantizing `model.layers.28.mlp.down_proj`...\n",
      "Dequantizing `model.layers.29.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.29.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.29.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.29.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.29.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.29.mlp.up_proj`...\n",
      "Dequantizing `model.layers.29.mlp.down_proj`...\n",
      "Dequantizing `model.layers.30.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.30.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.30.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.30.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.30.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.30.mlp.up_proj`...\n",
      "Dequantizing `model.layers.30.mlp.down_proj`...\n",
      "Dequantizing `model.layers.31.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.31.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.31.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.31.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.31.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.31.mlp.up_proj`...\n",
      "Dequantizing `model.layers.31.mlp.down_proj`...\n",
      "Saving dequantized model...\n",
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32768, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
      ")\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32768, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32768, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32768, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "        (lora_magnitude_vector): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32768, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
      ")\n",
      "Successfully loaded the model mistralai/Mistral-7B-v0.3 into memory\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "adapter = \"checkpoints/raft/checkpoint-425\"\n",
    "\n",
    "quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#model = dequantize_model(model, to='./dqz_model/', dtype=torch.bfloat16)\n",
    "'''\n",
    "model =  AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map='cuda'\n",
    ")\n",
    "print(model)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(f\"Successfully loaded the model {model_name} into memory\")\n",
    "model.save_pretrained(\"test\", safe_serialization=True)\n",
    "\n",
    "\n",
    "'''\n",
    "try:\n",
    "    print(f\"Starting to load the model {model_name} into memory\")\n",
    "\n",
    "    model =  AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map={\"\": 0}\n",
    "    )\n",
    "    print(model)\n",
    "    model = dequantize_model(model, to='./dqz_model/', dtype=torch.bfloat16)\n",
    "    print(model)\n",
    "    model = PeftModel.from_pretrained(model, adapter)\n",
    "    print(model)\n",
    "    model = model.merge_and_unload()\n",
    "    print(model)\n",
    "\n",
    "    print(f\"Successfully loaded the model {model_name} into memory\")\n",
    "    model.save_pretrained(\"test\", safe_serialization=True)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Delete the model object if it exists\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "\n",
    "    # Clear the GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Run the garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Model, GPU cache, and garbage have been cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17762"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free VRAM\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
