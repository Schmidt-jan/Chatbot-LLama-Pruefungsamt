{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3fb3da6-ae6c-4fda-98ea-70af7e595f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-cpp-python, langchain, llamaindex, chromadb, sentence_transformers, pypdf\n",
    "#CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1935d70-7edd-4b83-8991-f56c1b36e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
    "\n",
    "from llama_index.llms import LlamaCPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd198d4b-e935-4501-b7eb-508bc28bdc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
    "\n",
    "#\"You can only answer questions about the provided context. If you know the answer but it is not based in the provided context, don't provide the answer, just state the answer is not in the context provided.\"\n",
    "\n",
    "#\"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9662cf6-131a-4620-8721-6a385e8610b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA TITAN RTX, compute capability 7.5, VMM: yes\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   70.42 MiB\n",
      "llm_load_tensors: VRAM used           = 4095.05 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 278.56 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 275.37 MiB\n",
      "llama_new_context_with_model: total VRAM used: 4370.43 MiB (model: 4095.05 MiB, context: 275.37 MiB)\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "\n",
    "llm = LlamaCPP(\n",
    "                    model_path=str(\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\"),\n",
    "                    temperature=0,\n",
    "                    max_new_tokens=512,\n",
    "                    system_prompt=system_prompt,\n",
    "                    context_window=3900, #context_window=8192, # Mistral7B has an 8K context-window\n",
    "                    generate_kwargs={},\n",
    "                    # All to GPU\n",
    "                    model_kwargs={\"n_gpu_layers\": -1},\n",
    "                    verbose=True,\n",
    "                    #messages_to_prompt=messages_to_prompt,\n",
    "                    #completion_to_prompt=completion_to_prompt\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eae3f4cb-2481-4d6a-bc03-af0a085d9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index import ServiceContext, set_global_service_context\n",
    "\n",
    "# Will download Embedding Model from Huggingfaces\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "# chunk documents into tokens with embedding model, and use llm to generate responses\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024, # number of tokens per chunk -> bigger chunk = bigger source snippets\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d4b730-75a8-4c63-b004-15ae4043bf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndb = chromadb.PersistentClient(path=\"./chroma_db\")\\nchroma_collection = db.get_or_create_collection(\"pruefungsamt\")\\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\\n\\nif not os.path.exists(\\'./chroma_db/\\'):\\n    documents = SimpleDirectoryReader(\"./main_data/\").load_data()\\n    index = VectorStoreIndex.from_documents(\\n        documents,\\n        storage_context=storage_context,\\n        service_context=service_context\\n    )\\n    \\nindex = VectorStoreIndex.from_vector_store(\\n    vector_store,\\n    service_context=service_context,\\n)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "import chromadb\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "import os\n",
    "\n",
    "documents = SimpleDirectoryReader(\"main_data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "\n",
    "'''\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"pruefungsamt\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "if not os.path.exists('./chroma_db/'):\n",
    "    documents = SimpleDirectoryReader(\"./main_data/\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        service_context=service_context\n",
    "    )\n",
    "    \n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    service_context=service_context,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85adf75-abb6-4e19-ad48-162ccc0f37a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The admission requirements for the Master's program in Informatics include:\n",
      "1. A completed undergraduate degree with a minimum grade of 2.4 in one of the following fields: Informatics, Business Information Technology, Automotive Information Technology, or a related field. For a related field, at least 60 ECTS points must be earned in Informatics-related subjects.\n",
      "2. A choice of one of the three study directions: Autonomous Systems (MSI-AS), IT Management (MSI-ITM), or Software Engineering (MSI-SE).\n",
      "3. Successful participation in a selection interview, which is evaluated based on motivation for the chosen field of study, communication skills, technical understanding, and personal impression. The evaluation criteria are rated on a scale from 1 to 5, with intermediate values allowed by rounding up or down. A candidate must receive at least a 4.0 in all categories to be successful in the interview. The average of these ratings forms the first part of the selection decision.\n",
      "4. The average grade of the final exams from the undergraduate degree (as per Section 3, Abs. 2, Nr. 1, Satz 2 or 3). For foreign educational certificates, the German grading system is used for evaluation.\n",
      "5. Passing a specialized study aptitude test is not required."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3594.98 ms\n",
      "llama_print_timings:      sample time =     113.93 ms /   292 runs   (    0.39 ms per token,  2562.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26253.01 ms /  2067 tokens (   12.70 ms per token,    78.73 tokens per second)\n",
      "llama_print_timings:        eval time =   29378.76 ms /   291 runs   (  100.96 ms per token,     9.91 tokens per second)\n",
      "llama_print_timings:       total time =   56685.73 ms\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(streaming=True)\n",
    "response = query_engine.query(\"Was sind Zulassungsvorraussetzungen für den Master Informatik?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d991bf89-e0ec-4cdb-87ee-a46e06d01b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2: |119_ZuSMa_Senat_18012022.pdf| Hochschule Konstanz  \n",
      "Technik, Wirtschaft und Gestaltung  \n",
      " \n",
      " \n",
      "Seite 21 von 43 § 17 Informatik (MSI)  \n",
      "(1) Studiengangspezifische Zugangsvoraussetzungen gemäß § 5  Abs. 1  \n",
      "Zugangsvoraussetzung für den Masterstudiengang Informatik ist ein mit mindestens der Note \n",
      "2,4 abgeschlossenes Hochschulstudium der Informatik, Wirtsc haftsinformatik, \n",
      "Automobilinformationstechnik, Gesundheitsinformatik oder einer verwandten Fachrichtung. Bei einer verwandten Fachrichtung müssen mindestens 60 ECTS -Punkte in Fächern der \n",
      "Informatik nachgewiesen werden. Mit dem Antrag auf Zulassung zum Stud ium muss eine der \n",
      "drei Studienrichtungen verbindlich gewählt werden: Autonome Systeme (MSI -AS), IT -\n",
      "Management (MSI -ITM) oder Software- Engineering (MSI -SE). Auf die einzelnen \n",
      "Studienrichtungen entfallen in der Regel jeweils ein Drittel der zur Verfügung stehenden Studienplätze.  \n",
      "(2) Auswahlkriterien nach § 9 Abs. 2  \n",
      "1. Ergebnis eines Auswahlgesprächs  \n",
      "Die Bewertung der Auswahlgespräche nach § 9a Abs. 3 erfolgt nach folgenden Kriterien:  \n",
      "I. Grad der Motivation für den gewählten Studiengang und sich typischerweise daran \n",
      "anschließenden Berufstätigkeit, Interesse an der gewählten Studienrichtung und \n",
      "Reflexion der beruflichen Zielsetzung in Wissenschaft und Praxis  \n",
      "II. Kommunikative Kompetenzen,  \n",
      "III. Technisches Verständnis und \n",
      "IV. Persönlicher Eindruck, insbesondere Sc hlüssigkeit der Argumentation, \n",
      "Selbstdarstellung und Ausdrucksweise sowie Stressresistenz.  \n",
      "Für jede Ziffer erfolgt eine Bewertung auf einer Notenskala 1,0; 2,0; 3,0; 4,0; 5,0. Zur differenzierten Bewertung sind Zwischenwerte durch Erniedrigen oder Erhöhen der Notenziffer \n",
      "um 0,3 zulässig. Dabei sind die Noten 0,7; 4,3; 4,7 und 5,3 ausgeschlossen. Aus den Noten der Mitglieder der Auswahlkommission bzw. der Gesprächskommissionsmitglieder nach § 8 \n",
      "und § 9a Abs. 4 wird der arithmetische Mittelwert gebildet. Dabei wird nur die erste \n",
      "Dezimalstelle hinter dem Komma berücksichtigt; alle weiteren Stellen werden ohne Rundung \n",
      "gestrichen. Ein Bewerber bzw. eine Bewerberin hat erfolgreich an dem Auswahlgespräch \n",
      "teilgenommen, wenn alle Kriterien mindestens mit der Note 4,0  bewertet wurden. Das \n",
      "arithmetische Mittel der Noten für die Kriterien bildet die Teilnote 1 für die Auswahlentscheidung.  \n",
      "2. Leistungen , die mit der Abschlussprüfung des grundständigen Studiums nach Abs. 1 \n",
      "i. V. m. § 5 Abs. 1 Nr. 1 nachgewiesen sind \n",
      "Die Durchschnittsnote der Abschlussprüfung nach Abs. 1 bildet die Teilnote 2 für die \n",
      "Auswahlentscheidung. Abweichend von Satz 1 bildet in den Fällen des § 3 Abs. 2 Nr. 1 Satz \n",
      "2 die Durchschnittsnote nach § 3 Abs. 2 Nr. 1 Satz 3 die Teilnote 2. Bei ausländi schen \n",
      "Bildungsnachweisen ist die Durchschnittsnote nach deutscher Deutung zu berücksichtigen.  \n",
      "3. Ergebnis eines fachspezifischen Studienfähigkeitstests  \n",
      "Nicht zutreffend.   \n",
      "4.\n",
      "{'page_label': '21', 'file_name': '119_ZuSMa_Senat_18012022.pdf', 'file_path': 'main_data/119_ZuSMa_Senat_18012022.pdf', 'file_type': 'application/pdf', 'file_size': 355889, 'creation_date': '2023-11-20', 'last_modified_date': '2023-11-20', 'last_accessed_date': '2024-02-14'}\n",
      "2/2: |SPO_MSI_SPONr5_Senat_10122019.pdf| SPO Nr. 5 | Version nach Amtsblatt Nr. 96 | Senat 10. Dezember 2019  \n",
      "Seite 1 von 8 § 37 \n",
      "Studiengang \n",
      "Informatik (MSI) \n",
      "(1) Studiengangsprofil \n",
      "Der Masterstudiengang Informatik ist ein stärker anw endungsorientierter, konsekutiver Studiengang in \n",
      "Vollzeit. Ziele des Studiums sind sowohl die Verm ittlung vertiefender theoretischer als auch \n",
      "anwendungsbezogener Informatikkenntnisse. Nebe n der Problemlösungs- und Methodenkompetenz \n",
      "werden Schlüsselqualifikationen gefördert. \n",
      "(2) Studienaufbau \n",
      "Das Studium umfasst drei Semester und kann im Sommer- oder im Wintersemester begonnen werden. \n",
      "Lehrveranstaltungen von Pflichtmodulen (PM), die in  beliebiger Reihenfolge belegt werden können, \n",
      "können im Jahresrhythmus angeboten werden. Die Wahl pflichtmodule (WPM) werden in der Regel im \n",
      "Jahresrhythmus angeboten. Im dritten Semest er werden die Masterarbeit und die mündliche \n",
      "Masterprüfung durchgeführt. \n",
      "(3) Vertiefungs- bzw.  Studienrichtungen  \n",
      "Mit dem Antrag auf Zulassung zum Studium muss eine der drei Vertiefungsrichtungen verbindlich \n",
      "gewählt werden: \n",
      "(1) Autonome Systeme (MSI-AS), \n",
      "(2) IT-Management (MSI-ITM) oder \n",
      "(3) Software-Engineering (MSI-SE). \n",
      "Die Vertiefungsrichtung wird im Zeugnis ausgewiesen. \n",
      "(4) Studienumfang  \n",
      "Der Gesamtumfang des Studiums beträgt 90 ECTS-Punk te. Hiervon entfallen 60 ECTS-Punkte auf die \n",
      "erforderlichen Lehrveranstaltungen im Pflicht- und Wahlpflichtbereich, 30 ECTS Punkte auf \n",
      "Masterarbeit und -kolloquium (mündliche Masterprüfung). \n",
      "(5) Sonstige schriftliche oder praktische Arbeiten  \n",
      "Die Modul- und Modulteilprüfungen der Art SP (sonst ige schriftliche oder praktische Arbeiten gemäß § \n",
      "12 Abs. 1 Nr. 4 SPOMa) können folgendermaßen durchgeführt werden: \n",
      "AB = Ausarbeitung/Berichte  ",
      " \n",
      "LP = Labor-/Programmierarbeiten  ",
      " \n",
      "PR = Präsentation \n",
      "TE = Testate. \n",
      "Bei Modul- und Modulteilprüfungen der Art TE, LP, AB und PR legt die/der Prüfer/in gemäß § 15 Abs. \n",
      "2 SPOMa zu Beginn des Semesters die Prüfungsmoda litäten, insbesondere die Prüfungstermine fest. \n",
      "(6) Lehr- und Prüfungssprachen  \n",
      "Die Lehr- und Prüfungssprache ist in der Regel Deutsch. Lehrveranstaltungen können gemäß § 5 \n",
      "SPOMa ganz oder teilweise in englischer Sprache abgehalten werden. In diesem Fall kann die Prüfung auch in englischer Sprache durc hgeführt werden. Dies ist von der/vom Prüfer/in zu Beginn des \n",
      "Semesters bekannt zu geben. Die Masterarbeit ka nn in englischer Sprache verfasst werden.\n",
      "{'page_label': '1', 'file_name': 'SPO_MSI_SPONr5_Senat_10122019.pdf', 'file_path': 'main_data/SPO_MSI_SPONr5_Senat_10122019.pdf', 'file_type': 'application/pdf', 'file_size': 170767, 'creation_date': '2023-11-20', 'last_modified_date': '2023-11-20', 'last_accessed_date': '2024-02-14'}\n"
     ]
    }
   ],
   "source": [
    "for index, node in enumerate(response.source_nodes, start=1):\n",
    "    print(f\"{index}/{len(response.source_nodes)}: |{node.node.metadata['file_name']}| {node.node.get_text()}\")\n",
    "    print(node.node.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
