{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpllmws23/environments/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tpllmws23/environments/rag/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! chat_format is not default parameter.\n",
      "                chat_format was transferred to model_kwargs.\n",
      "                Please confirm that chat_format is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/tpllmws23/llms/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA TITAN RTX, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 4096\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
     ]
    }
   ],
   "source": [
    "from custom_rag_loader import DbSupportedEmbeddingModels, RagConfig, SupportedModels, load_llm_rag_model, DbSupportedChunkSizes, DbSupportedChunkOverlap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm, db = load_llm_rag_model(\n",
    "    RagConfig(\n",
    "        model=SupportedModels.Mistral, \n",
    "        n_ctx=4096,\n",
    "        db_embedding_model=DbSupportedEmbeddingModels.Paraphrase_multilingual_MiniLM_L12_v2,\n",
    "        db_chunk_overlap=DbSupportedChunkOverlap.Overlap_256,\n",
    "        db_chunk_size=DbSupportedChunkSizes.Chunk_4096,\n",
    "        version=\"v3\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# set_debug(True)\n",
    "\n",
    "template = \"\"\"Antworte auf die folgende Frage nur mit dem Wissen welches du aus dem bereitgestellten Kontext ziehen kannst. Wenn die Antwort nicht im Kontext enthalten ist, antworte mit \"Ich weiß es nicht\".\n",
    "{context}\n",
    "\n",
    "Frage: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    context =\"\"\n",
    "    for doc in docs:\n",
    "        context += \"Content: \\n\" + doc.page_content + \"\\n\"\n",
    "        context += \"Source: \\n\" + str(doc.metadata['file_path']) + \"\\n\\n\\n\"\n",
    "    return context\n",
    "\n",
    "def simple_rag_chain(question: str):\n",
    "    docs = db.search(question, 'similarity', k=5)\n",
    "    formatted_docs = format_docs(docs)\n",
    "\n",
    "    final_prompt = prompt.format(context= formatted_docs, question= question)\n",
    "    llm.invoke(final_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tpllmws23/Chatbot-LLama-Pruefungsamt/Chatbot-Jan/databases/v3/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2_4096_256_l2\n"
     ]
    }
   ],
   "source": [
    "from custom_rag_loader import RagConfig, SupportedModels, DbSupportedEmbeddingModels, DbSupportedChunkSizes, DbSupportedChunkOverlap\n",
    "\n",
    "config = RagConfig(\n",
    "        model=SupportedModels.Mistral,\n",
    "        db_embedding_model=DbSupportedEmbeddingModels.Paraphrase_multilingual_MiniLM_L12_v2,\n",
    "        db_chunk_overlap=DbSupportedChunkOverlap.Overlap_256,\n",
    "        db_chunk_size=DbSupportedChunkSizes.Chunk_4096,\n",
    "        version=\"v3\",\n",
    "        distance=\"l2\",\n",
    "    )\n",
    "\n",
    "print(config.db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Antwort: Die Sprachnachweise für den gewählten Studiengang können nachgereicht werden, wenn sie bis zum Bewerbungsschluss nicht vorgelegt werden konnten. Es ist zu beachten, dass die Hochschule Konstanz die Nachweisfrist gemäß § 5 Abs. 2 Satz 1 Alternative 4 verlängern kann, wenn es besonders begründete Einzelfälle gibt. In solchen Fällen kann die Sprachnachweise nachgereicht werden, auch wenn die reguläre Nachweisfrist abgelaufen ist. Es ist jedoch zu beachten, dass die Hochschule Konstanz in solchen Fällen die Verlängerung der Nachweisfrist und die Annahme des Spätnachweises auf Grundlage von § 5 Abs. 2 Satz 1 Alternative 4 verfügen kann. Es ist also zu empfehlen, dass sich Bewerber bei der Hochschule Konstanz für die Möglichkeiten und Bedingungen der Verlängerung der Nachweisfrist und der Annahme"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     947.29 ms\n",
      "llama_print_timings:      sample time =     102.40 ms /   256 runs   (    0.40 ms per token,  2500.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     958.17 ms /  2000 tokens (    0.48 ms per token,  2087.32 tokens per second)\n",
      "llama_print_timings:        eval time =    6802.83 ms /   255 runs   (   26.68 ms per token,    37.48 tokens per second)\n",
      "llama_print_timings:       total time =   18141.35 ms /  2255 tokens\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Bis wann müssen die Anträge auf Zulassung für das Sommersemester und das Wintersemester bei der Hochschule Konstanz eingereicht werden?\",\n",
    "    \"Wie viele Studierende sind an der Hochschule Konstanz eingeschrieben?\",\n",
    "    \"Bis zu welchem Zeitpunkt können Sprachnachweise für den gewählten Studiengang nachgereicht werden, wenn sie bis zum Bewerbungsschluss nicht vorgelegt werden können?\",\n",
    "    \"Welche Unterlagen müssen dem Antrag auf Zulassung beigefügt werden?\",\n",
    "]\n",
    "\n",
    "simple_rag_chain(questions[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpllmws23/environments/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tpllmws23/environments/rag/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! chat_format is not default parameter.\n",
      "                chat_format was transferred to model_kwargs.\n",
      "                Please confirm that chat_format is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/tpllmws23/llms/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "  Device 0: NVIDIA TITAN RTX, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 4096\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
     ]
    }
   ],
   "source": [
    "from custom_rag_loader import DbSupportedEmbeddingModels, RagConfig, SupportedModels, load_llm_rag_model, DbSupportedChunkSizes, DbSupportedChunkOverlap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm, db = load_llm_rag_model(\n",
    "    RagConfig(\n",
    "        model=SupportedModels.Mistral, \n",
    "        n_ctx=4096,\n",
    "        db_embedding_model=DbSupportedEmbeddingModels.Paraphrase_multilingual_MiniLM_L12_v2,\n",
    "        db_chunk_overlap=DbSupportedChunkOverlap.Overlap_256,\n",
    "        db_chunk_size=DbSupportedChunkSizes.Chunk_2048,\n",
    "        version=\"v3\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# set_debug(True)\n",
    "\n",
    "template = \"\"\"Antworte auf die folgende Frage nur mit dem Wissen welches du aus dem bereitgestellten Kontext ziehen kannst. Wenn die Antwort nicht im Kontext enthalten ist, antworte mit \"Ich weiß es nicht\".\n",
    "{context}\n",
    "\n",
    "Frage: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    context =\"\"\n",
    "    for doc in docs:\n",
    "        context += \"Content: \\n\" + doc.page_content + \"\\n\"\n",
    "        context += \"Source: \\n\" + str(doc.metadata['file_path']) + \"\\n\\n\\n\"\n",
    "    return context\n",
    "\n",
    "def simple_rag_chain(question: str):\n",
    "    docs = db.search(question, 'similarity', k=5)\n",
    "    formatted_docs = format_docs(docs)\n",
    "\n",
    "    final_prompt = prompt.format(context= formatted_docs, question= question)\n",
    "    llm.invoke(final_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpllmws23/environments/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from custom_rag_loader import RagConfig, SupportedModels, DbSupportedEmbeddingModels, DbSupportedChunkSizes, DbSupportedChunkOverlap, load_db\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "config = RagConfig(\n",
    "        model=SupportedModels.Mistral,\n",
    "        db_embedding_model=DbSupportedEmbeddingModels.Paraphrase_multilingual_MiniLM_L12_v2,\n",
    "        db_chunk_overlap=DbSupportedChunkOverlap.Overlap_256,\n",
    "        db_chunk_size=DbSupportedChunkSizes.Chunk_2048,\n",
    "        version=\"v3\",\n",
    "        distance=\"l2\",\n",
    "        use_raw_documents=True\n",
    "    )\n",
    "\n",
    "db = load_db(config)\n",
    "\n",
    "docs = db.similarity_search(\"Test\")\n",
    "i = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Antwort: Die Sprachnachweise für den gewählten Studiengang können nachgereicht werden, wenn sie bis zum Abschluss des Masterstudiums nicht vorgelegt werden konnten. Dies ist in der Regelung gemäß § 5 Abs. 2 geregelt.\n",
      "\n",
      "Quelle: /home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1729.53 ms\n",
      "llama_print_timings:      sample time =      48.97 ms /   125 runs   (    0.39 ms per token,  2552.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1726.44 ms /  3361 tokens (    0.51 ms per token,  1946.78 tokens per second)\n",
      "llama_print_timings:        eval time =    4453.88 ms /   124 runs   (   35.92 ms per token,    27.84 tokens per second)\n",
      "llama_print_timings:       total time =   15695.58 ms /  3485 tokens\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Bis wann müssen die Anträge auf Zulassung für das Sommersemester und das Wintersemester bei der Hochschule Konstanz eingereicht werden?\",\n",
    "    \"Wie viele Studierende sind an der Hochschule Konstanz eingeschrieben?\",\n",
    "    \"Bis zu welchem Zeitpunkt können Sprachnachweise für den gewählten Studiengang nachgereicht werden, wenn sie bis zum Bewerbungsschluss nicht vorgelegt werden können?\",\n",
    "    \"Welche Unterlagen müssen dem Antrag auf Zulassung beigefügt werden?\",\n",
    "]\n",
    "\n",
    "simple_rag_chain(questions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpllmws23/environments/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from custom_rag_loader import DbSupportedEmbeddingModels, RagConfig, SupportedModels, load_db, DbSupportedChunkSizes, DbSupportedChunkOverlap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "db = load_db(\n",
    "    RagConfig(\n",
    "        model=SupportedModels.Mistral, \n",
    "        n_ctx=4096,\n",
    "        db_embedding_model=DbSupportedEmbeddingModels.Paraphrase_multilingual_MiniLM_L12_v2,\n",
    "        db_chunk_overlap=DbSupportedChunkOverlap.Overlap_256,\n",
    "        db_chunk_size=DbSupportedChunkSizes.Chunk_2048,\n",
    "        version=\"v3\"\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='(1) 1Vom Vergabeverfahren ausgeschlossen ist, wer\\n1. den Antrag auf Zulassung mit den erforderlichen Unterlagen nicht form - oder\\nfristgemäß gestellt hat oder\\n2. die Zugangsvoraussetzungen gemäß B esonderem Teil (§ 12 - 26) nicht erfüllt.\\n2Ferner ist vom Vergabeverfahren für das erste Semester ausgeschlossen, wer in dem\\nbetreffenden Studiengang bereits an einer Hochschule im Geltungsbereich des\\nGrundgesetzes eingeschrieben ist.\\n(2) 1Liegen nach Prüfung des Zulassungsantrages keine Hinderungsgründe vor, werden nach\\nAbschluss des Auswahlverfahrens Zulassungs - und Ablehnungsbescheide erteilt. 2Sie werden\\nvon der Hochschule Konstanz postalisch versandt. 3Die Zulassungsbescheide enthalten eine\\nFrist zur Annahme des Studienplatzes.\\n(3) 1Wird die Frist zur Annahme des Studienplatzes nicht eingehalten und keine Nachfrist\\ngewährt, erlischt die Zulassung. 2In diesem Fall ist eine Immatrikulation nicht mehr möglich.\\n(4) 1Die Zulassung ist zu ver sagen, wenn die Bewerberin oder der Bewerber nach\\nDurchführung des Auswahlverfahrens aus Kapazitätsgründen keinen Studienplatz erhält oder\\nwenn sie oder er eine nach der Prüfungsordnung des abgebenden Studiengangs erforderliche\\nPrüfung im gleichen Studieng ang endgültig nicht bestanden hat. 2Zulassungsanträge nach', metadata={'file_path': '/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf'}), Document(page_content='Eine einschlägige Berufstätigkeit in der Praxis, die nach Abschluss des grundständigen\\nHochschulstudiums gemäß Abs. 1 für die Dauer von mindestens einem Jahr nachgewiesen\\nwird, und besondere Vorbildungen, insbesondere Nachweise über abgeleistete einschlägige\\nFort- und Weiterbildungsnachweise einer anerkannten Einrichtung werden bei der Auswahl\\nberücksichtigt. Dabei wird die Berufstätigkeit oder  besondere Vorbildungen, insbesondere\\nNachweise über abgeleistete einschlägige Fort - und Weiterbildungsnachweise einer\\nanerkannten Einrichtung gemäß folgender Skala bewertet:\\n1. besondere Vorbildung(en) um den Wert 0,1,\\n2. Berufstätigkeit ab einem Jahr um den We rt 0,1,\\n3. Berufstätigkeit ab zwei Jahren um den Wert 0,2 und\\n4. Berufstätigkeit ab drei Jahren um den Wert 0,3.\\nDer entsprechende Wert bzw. die kumulierte Gesamtzahl von 1 und 2 oder 1 und 3 oder 1 und\\n4 bildet die Teilnote 3.\\n(3) Kriterien für die Auswahl der Bewerber und Bewerberinnen zu dem Auswahlgespräch nach § 9a Abs. 1\\nNicht zutreffend.\\n(4) Erstellung einer Rangliste für die Auswahlentscheidung nach § 10\\nFür die Auswahlentscheidung wird unter den Bewerbern und Bewerberinnen eine Rangliste\\nnach einer Auswahlnote erstellt, die wie folgt ermittelt wird. Von der Teilnote 1 werden die\\nTeilnote 2 und Teilnote 3 abgezogen.\\n(5) Ausländerquote gemäß Anlage 8 HZVO\\nNicht zutreffend.', metadata={'file_path': '/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf'}), Document(page_content='(3) Kriterien für die Auswahl der Bewerber und Bewerberinnen zu dem\\nAuswahlgespräch nach § 9a Abs. 1\\nUnter den Bewerbern, die die Zugangsvoraussetzungen gemäß Abs. 1 erfüllen, findet zur\\nBegrenzung der Teilnehmerzahl an den Auswahlgesprächen eine Vorauswahl nach einer\\nRangliste statt. Diese Rangliste wird anhand der Teilnote 2 erstellt. Die Zahl der einzuladenden  Seite 36 von 43 rangbesten Bewerber beträgt höchstens das Dreifache der zur Verfügung stehenden\\nStudienplätze im Masterstudiengang Unternehmensführung.\\n(4) Erstellung einer Rangliste für die Auswahlentscheidung nach § 10\\nFür die Auswahlentscheidung wird unter den Bewerbern, die am Auswahlgespräch nach Abs.\\n2 Nr. 1 erfolg reich teilgenommen haben, eine Rangliste nach einer Auswahlnote erstellt, in\\nwelche die Teilnote 1 und die Teilnote 2 jeweils zu 50 vom Hundert eingehen.\\n(5) Ausländerquote gemäß Anlage 8 HZVO\\nNicht zutreffend.', metadata={'file_path': '/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf'}), Document(page_content='§ 5 Allgemeine Zugangsvoraussetzungen\\n(1) 1Zugangsvoraussetzungen für einen Masterstudiengang sind\\n1. Ein mit überdurchschnittlichem Erfolg abgeschlossenes grundständiges\\nHochschulstudium, für das eine Regelstudienzeit von mindestens drei Jahren\\nfestgesetzt ist, in einem der im Besonderen Teil für den jeweiligen Masterstudiengang\\nfestgelegten Studiengänge oder ein vergleichbarer Abschluss. Für den Abschluss des\\ngrundständigen Hochschulstudiums muss ein Umfang von 210 ECTS -Punkten\\nnachgewiesen werden. Die Zulassung ist ausgeschlossen, wenn das grundständige Hochschulstudium nicht mindestens mit der gemäß der im  besonderem Teil (§§ 12- 26)\\nfestgelegten Gesamtnote abgeschlossen wurde,\\n2. Nachweise, dass weitere Zugangskriterien nach § 9 i.  V. m. §§ 1 2-26 erfüllt sind,\\n3. die erfolgreiche Teilnahme an dem Auswahl verfahren gemäß § 9.\\n2Die Entscheidung über das Vorliegen der genannten Voraussetzungen trifft die\\nAuswahlkommission des  jeweiligen Masterstudiengangs (§ 8 ).\\n(2) 1Wird die Zulassung für einen dreisemestrigen Masterstudiengang beantragt und wird  das\\ngrundständige Studium abweichend von Abs. 1 Nr. 1 mit 180 ECTS -Punkten nachgewiesen,\\nerfolgt die Zulassung zum Studium unter Auflage gemäß § 6 Abs. 7.\\n(3) 1Bei der Anerkennung von akademischen Graden, die außerhalb der Bundesrepublik\\nDeutschland erworben wurden, sind die von der Kultusministerkonferenz und der\\nHochschulrektorenkonferenz gebilligten Äquivalenzvereinbarungen sowie die Vereinbarungen\\nim Rahmen von Hochschulpartnerschaften zu beachten.', metadata={'file_path': '/home/tpllmws23/Chatbot-LLama-Pruefungsamt/main_data_filtered/119_ZuSMa_Senat_18012022.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(db.similarity_search(\"Test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
